---
title: "DataLens SF Bay"
author: "Annika Nazanin Carlos"
# date: "`r {Sys.Date()}`"
# format:
#   html:
#     toc: true
#     code-fold: true
#     self-contained: true
#     fontsize: 10px
#     linestretch: 1
# echo: false
# output-file: "2022_09_20"
# params:
#   useAWStoReadWrite: FALSE
#   RERUNopenCleanAWSdataYN: FALSE
#   RERUNsubsetofTheVARSinsteadofAll: FALSE
#   wantSUBSETofTheVARSinsteadofAll: TRUE
#   RERUNsubsetofVerySmallVars: FALSE
#   hardcodeFileName: "none"
#   scen________: "___scenario____________________"
#   place: "SanFrancisco"
#   category_rh:    TRUE
#   lever_flsz:     TRUE
#   category_tr:  FALSE
#   lever_fq:     FALSE
#   year:
#     label: "Year"
#     value: 2018
#     input: slider
#     min: 2018
#     max: 2040
#     step: 1
#   subset________: "___ subset by ____________________"
#   HomeToMandatory: TRUE
#   MandatoryToHome: FALSE
#   MandatoryToMandatory: FALSE
#   yvars________: "___ y variables ____________________"
#   yvar_doorDoor:    TRUE
#   yvar_potInex:     TRUE
#   yvar_realInex:    TRUE
#   yvar_socInex:     TRUE
#   yvar_mode:        TRUE
#   yvar_carbon:      false
#   het________: "___heterogeneity variables_____"
#   hetvar_incXcar:   TRUE
#   hetvar_carOwn:    TRUE
#   hetvar_Inc10hiLo: TRUE
#   hetvar_mode:      TRUE
#   save_______: "___save graphs this time_____"
#   saveGraphsTF:     TRUE
# editor_options: 
#   chunk_output_type: inline
---

```{r}
print("Hi!")
```

This is common to all projects. Any time we change the title of a graph or a table, or the colors, or the variable names, etc, it should be in here And then this file should be run before ANY of the other ones.

All functions for graphs, regressions, etc, should be defined as FUNCTIONS, not in files, so that they are common across all projects.

A file for a specific project should only incude the YAML parameters and then Call functions And then not much else. \# Setup {#sec-setup}

<!-- ::: panel-tabset -->

## Scenario

Description of the setup

## Source Files

Parameters

```{r}
load(file = "TEMP_PARAMS.RData")
params100 <- TEMP_PARAMS
```

Define file pathways for data and output

```{r }
source(paste0(getwd(),"/00__global_file_directories.R"))
```

Open libraries and define setup functions

```{r }
source(paste0(getwd(),"/02__SetupLibrary.R")) # returns: 
```

Open description file, with the Titles and colors that are common across all projects

```{r }
#| include: FALSE 
t <- "03__MasterTitlesNamesColorsEtc.csv"
print(paste("opening csv description ", t))
description_df   <- read_csv(file =t)
```

Define other functions

```{r}
t <- "/50__GraphFunctions.qmd"
print(paste("opening source file ", t))
quarto::quarto_render(paste0(getwd(),t))
```

## Parameters

From YAML header, where all of the definitions should be

```{r }
#| include: FALSE 
# Print pretty table of parameters
parameters <- as.data.frame.list(params100) 
paramsT <- data.table::transpose(parameters, keep.names = "type_title"  )
# paramsT <- paramsT |> filter(V1 != FALSE)
descr <- full_join(paramsT,description_df) 
descr <- descr |> relocate("type", "title") |> filter(V1 != FALSE | type_title=="generic_plot")
```

Import titles etc from description file

```{r}
#| include: FALSE 
categoryTitleShort  <- as.character(filter(descr, type == "category")["title_short"])
leverTitleShort     <-  as.character(filter(descr, type == "lever")["title_short"])
categoryTitleLong  <- as.character(filter(descr, type == "category")["title"])
leverTitleLong     <-  as.character(filter(descr, type == "lever")["title"])
year     <-  as.character(filter(descr, type == "year")["title"])
place     <-  as.character(filter(descr, type == "place")["title"])
placeTitleShort <-  gsub(" ", "", as.character(filter(descr, type == "place")["title_short"]))
placeFilename     <-   placeTitleShort
saveGraphsTF <- params100$saveGraphsTF
category <- categoryTitleShort
lever <- leverTitleShort
leverLevels <- (as.character(filter(descr, type == "lever")["leverLevels"][1]))

print(glue("Scenario: {categoryTitleLong} ({categoryTitleShort}), 
           levers: {leverTitleLong} ({leverTitleShort})
           Levels: {leverLevels}"))
```

<!-- ::: -->

## Scenario: **`r categoryTitleLong`** -- **`r leverTitleLong`**

### `r year`, `r place`

<!-- ::: {#overv .panel-tabset} -->

## Scenario

Category: **`r categoryTitleLong`** (`r categoryTitleShort`), Levers : **`r leverTitleLong`** (`r leverTitleShort`)

```{r}
knitr::kable(
descr |> select("type", "title") |> 
  filter(type != "yvar" & type != "hetvar") 
)
```

**Outcomes (Y variables)**

```{r }
knitr::kable(
  descr |> select("type", "title", "variable") |> 
  filter(type == "yvar") 
)

listY <- descr |> filter(type == "yvar")
listY <- as.list(listY$variable) 
# glimpse(listY)
```

**Heterogeneity variables**

```{r}
knitr::kable(
  descr |> select("type", "title", "variable") |> 
  filter(type == "hetvar") 
)

listHet  <- descr |> filter(type == "hetvar")
listHet <- as.list(listHet$variable) 
# glimpse(listHet)
```

## parameters

```{r}
knitr::kable(
descr |> select("type", "title", "variable", "V1", "title_short") 
)
```

<!-- ## Permanent TITLE Changes -->

<!-- Permanently ChANGE Titles, and colors, in the descriptive excel table -->

<!-- Change -- for EVERY GRAPH this and other files, into the csv -->

```{r titles}

description_df <- description_df |>
  mutate(title =
# Only add ones that are new, the others will be saved
      case_when(
        variable == "Potential_INEXUS_in_dollar" ~ "Potential INEXUS",
        TRUE ~ title
      ))

# leverLevels <- (as.character(filter(descr, type == "lever")["leverLevels"][1]))

```

## YAML det

```{r}
# kable(params100, glimpse
glimpse(params100)
```

<!-- ::: -->

```{r .}
# knitr::knit_exit()
```

## Dataset

```{r}
print("Reading and writing from AWS? ")
print(params100$useAWStoReadWrite)
print("do we have to read it in totally from raw (rather than cleaned version)?")
print(params100$RERUNopenCleanAWSdataYN)
```

### List the files we want

First, decide if we want AWS or local. Then make a List of all files -- either from AWS or locally, then (put into a dataset):

```{r}
useAWStoReadWrite <-TRUE
```

```{r List_Files}
if(params100$useAWStoReadWrite==TRUE) {
    library(aws.s3)
    library(dbplyr) # to get from aws
    aws_prefix <- "deepDive/CleanData/SanFrancisco"
    Sys.setenv("AWS_DEFAULT_REGION"="us-east-2", TZ='GMT')
    awsDF <- get_bucket_df("beam-core-act", prefix = aws_prefix) # dataframe with all of the AWS files
    dataframe_of_files <- awsDF
} else if (params100$hardcodeFileName != "none") {
    dataframe_of_files <- as_tibble(params100$hardcodeFileName)
} else {
  dataframe_of_files <- as_tibble(list.files(path = data_dir_on_this_machine)) |> 
    rename(Key = value)
}
    # print(dataframe_of_files)
```

List of files for this city (either locally or on AWS), and the stacked ones, that are not the previous files

```{r __ofCity}
data_file_list_paths <- dataframe_of_files |> select(Key)
data_file_list_paths <- data_file_list_paths |> 
          filter(grepl(pattern = "*.tacked*.",x = Key, ignore.case = TRUE))
data_file_list_paths <- data_file_list_paths |> 
          filter(!grepl(pattern = "*.revious*.",x = Key, ignore.case = TRUE))
# print(data_file_list_paths)
data_file_list_paths <- data_file_list_paths |> 
          filter(grepl(pattern = paste0("*",placeTitleShort,"_","*."),
                       x = Key, ignore.case = TRUE))
# print(data_file_list_paths)
```

Of those, the smaller List of files for this specific scenario (defined in YAML) (either locally or on AWS) . Hopefully there will be one cleaned, one cleaned subset, and one raw: , and cleaned stacked if there is one: wantSUBSETofTheVARSinsteadofAll

```{r __ofScenario}

        data_file_list_paths <- data_file_list_paths |> 
          filter(grepl(pattern = paste0("*.",categoryTitleShort,
                                        "_",leverTitleShort),
                       x = Key, ignore.case = TRUE))
        # print(data_file_list_paths)
```

The list of cleaned, ready for analysis files

```{r __cleaned}
# not including the words old or previous
        data_file_list_analysis <- data_file_list_paths |> 
          filter( grepl(pattern = "*.raphclean*",x = Key, ignore.case = TRUE)) |> 
          filter( grepl(pattern = "*.ubset*",x = Key, ignore.case = TRUE)) |> 
          filter( grepl(pattern = "*.hort*",x = Key, ignore.case = TRUE)) |> 
          filter( grepl(pattern = "*.mand*",x = Key, ignore.case = TRUE)) |> 
          filter(!grepl(pattern = "*.old*",x = Key, ignore.case = TRUE)) |> 
          filter(!grepl(pattern = "*.previous*",x = Key, ignore.case = TRUE))  |> 
          filter( grepl(pattern = "*readyForAnalysis*",x = Key, ignore.case = TRUE))
         
        data_file_list_analysis <- data_file_list_analysis$Key
        print(data_file_list_analysis)
        
# print("cleaned,man)
```

## Open

Do we have to re-run the raw data? `r params100$RERUNopenCleanAWSdataYN` Do we have to re-run the subset data? `r params100$RERUNsubsetofTheVARSinsteadofAll`

```{r redo_raw_and_subset}
if(params100$RERUNopenCleanAWSdataYN==TRUE) {
        print("re-running raw, subset, createnewvars")
        rmarkdown::render("4__OpenRawAWSdata.qmd")
} else if(params100$RERUNsubsetofTheVARSinsteadofAll==TRUE) {
        print("re-running raw")
        rmarkdown::render("5__Subset_SelectVariables.qmd")
} else if(params100$RERUNsubsetofVerySmallVars==TRUE) {
        print("re-running new vars")
       rmarkdown::render("6__CreateNewVariables")
} else {
  print("using same dataset as last time")
}
```

```{r open_cleaned_subsetted}
#| collapse: true
data_file_list_analysis
print("opening")
if(params100$useAWStoReadWrite==TRUE) {
  df_temp <- aws.s3::s3read_using(read_rds,
                             object = data_file_list_analysis,
                             bucket = "beam-core-act")
                          print("Using data from AWS!")
} else {
  df_temp <- read_rds(file = paste0(data_dir_on_this_machine,
                             data_file_list_analysis) )
                          print("Using local data")
}
print("opened")
```

### .

## Describe scenario

```{r ListThe_scenarios}
# unique(df_temp$category)

# unique(df_temp$lever)
# unique(df_temp$lever_position)


# listleverLevels <- as.list(
#                       levels(as_factor(df_temp$lever_position)))
listleverLevels <- as.list(levels(df_temp$lever_positionFactor))
listleverLevels <- as.numeric(listleverLevels)
# as.character(listleverLevels)

print(listleverLevels)
knitr::kable(listleverLevels)

print(as.character(listleverLevels))
```

```{r}
library(Hmisc)
# testdf <- contents(df_temp,
#                    id = df_temp$IDMerged,
#                    maxlevels = 1 , prlevels=TRUE, levelType='table',
#                    nshow = TRUE)

# html(testdf)
# testdf
  contentsOfDataT <- html(contents(df_temp),  levelType='table')
  contentsOfData <- contents(df_temp)
  print(html(contents(df_temp),  levelType='table'))
  describe(df_temp$lever_positionFactor)
  # print(contentsOfDataT)
# lever_position

```

## Subset into Mandatory?

```{r}

print(listleverLevels)
leverlevelsStr <- paste(listleverLevels, collapse="  ")
print(leverlevelsStr)
print(descr |> filter(type == "yvar"))
print(glue(" Scenario: {categoryTitleLong} ({categoryTitleShort}), 
                lever: {leverTitleLong} ({leverTitleShort}),
         lever Levels: {leverlevelsStr}"))
```

#### FIX type_title to match whatever

## Modify Master output

```{r save_descr}
# description_df <- description_df |>
#   mutate(leverLevelListStr =
# # Only add ones that are new, the others will be saved
#       case_when(
#          type_title== "lever_flsz" ~ leverlevelsStr ,
#         TRUE ~ leverLevelListStr
#       ))
# write_excel_csv(description_df, file ="03__MasterTitlesNamesColorsEtc.csv")
```

```{r mand}
# knitr::kable(df_temp |> 
#                count(mandatoryCat))

```

## Summary Tables

```{r area}
df_summary <- df_temp |>
  group_by(lever_position) |>
  rename(`Lever Position` = lever_position) |>
  count(mode_5catPooled)
df_summary <- df_summary |>
  mutate(`Total Trips` = sum(n)) |>
  mutate(`Percent of Total Trips` = n /`Total Trips` ,
         n = NULL)
df_summary1 <- df_summary |>
  pivot_wider(names_from = mode_5catPooled, values_from = `Percent of Total Trips`)
print(df_summary1)
```

```{r}
# df_summary2 <- df_temp |> 
#   group_by(lever_position) |> 
#   rename(`Lever Position` = lever_position) |> 
#   count(mode_planned_5, mode_planned_at_baseline)
# df_summary2 <- df_summary2 |> 
#   mutate(`Total Trips` = sum(n)) |> 
#   mutate(`Percent of Total Trips` = n /`Total Trips` ,
#          n = NULL)
# print(df_summary2)
```

## Setup Graphs

-   for Graphs

<!-- ::: panel-tabset -->

### Temporary

```{r scale_limits}

# scaleNexusLow <- min(median_hilow(df_temp$Potential_INEXUS_in_dollar,
#                       conf.int = .75)[["ymin"]],
#                      median_hilow(df_temp$Realized_INEXUS_in_dollar,
#                       conf.int = .75)[["ymin"]],
#                      median_hilow(df_temp$Social_INEXUS,
#                       conf.int = .75)[["ymin"]])
# 
# scaleNexusHigh <- max(median_hilow(df_temp$Potential_INEXUS_in_dollar,
#                       conf.int = .9)[["ymax"]],
#                      median_hilow(df_temp$Realized_INEXUS_in_dollar,
#                       conf.int = .9)[["ymax"]],
#                      median_hilow(df_temp$Social_INEXUS,
#                       conf.int = .9)[["ymax"]])
# scaleTimeLow <- 0.01
# scaleCostLow <- 0
# scaleTimeHigh <- median_hilow(df_temp$duration_door_to_door,
#                       conf.int = .8)[["ymax"]]
#                      
# scaleCostHigh <- -.5*scaleNexusLow 
# 
# descr <- descr |>
#   mutate(scaleLow = case_when(
#       str_detect(variable, "(?i)NEXU") ~ scaleNexusLow,
#       str_detect(variable, "(?i)dur") ~ scaleTimeLow,
#       str_detect(variable, "(?i)cost|price") ~ scaleCostLow,
#     TRUE ~ scaleLow   )) |> 
#   mutate(scaleHigh = case_when(
#       str_detect(variable, "(?i)NEXU") ~ scaleNexusHigh,
#       str_detect(variable, "(?i)dur") ~ scaleTimeHigh,
#       str_detect(variable, "(?i)cost|price") ~ scaleCostHigh,
#     TRUE ~ scaleHigh   ))
```

```{r Theme}
# theme_set(theme_light(base_size = 11)) # make ALL font based on this
# theme_update(
#   plot.title =
#             element_text(
#               face = "bold",
#               margin = margin(0, 0, -100, 0),
#               # size = 26,
#               # family = "KyivType Sans",
#               vjust = 0,
#               color = "grey25"
#         ))
# theme_update(
#   legend.justification = "center",
#              legend.position = "bottom",
#              legend.title = element_blank(),
#              # legend.text  = element_text(face = "green"),
#              # +
#              #   scale_color_vibrant() +
#              #   scale_fill_vibrant()
#              # Top-right position)
#              legend.direction = "vertical", # Elements within a guide are placed one on top of other in the same column
#              legend.box = "horizontal" # Different guides-legends are stacked horizontally
#              )
# theme_update(# Light background color
#     # plot.background = element_rect(fill = "#F5F4EF", color = NA),
#     # plot.margin = margin(20, 30, 20, 30),
#     # Customize the title. Note the new font family and its larger size.
#     plot.caption = element_text(size = 11),
#     # Remove titles for x and y axes.
#     # axis.title = element_blank(),
#     # Specify color for the tick labels along both axes 
#     axis.text = element_text(color = "grey40"),
#     # Specify face and color for the text on top of each panel/facet
#     strip.text = element_text(face = "bold", color = "grey20")
# )
```

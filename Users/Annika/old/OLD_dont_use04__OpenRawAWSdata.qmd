---
title: "Open data file, preliminary cleaning"
author: "atb"
format: html
---

```{r}
placeTitleShort <- "sf"
categoryTitleShort <- "rh"
leverTitleShort <- "price_fleetsz_0p125"
year <- "2018"
# leverTitleShort <- "price_fleetsz_0p25"
# leverTitleShort <- "price_fleetsz_0p5"
# leverTitleShort <- "price_fleetsz_2p25"
```

# TO DO: change this to save_object?

```{r}
library(readr)
library(dplyr)
library(forcats)
library(glue)
```

```{r}
source(paste0            (getwd(),    "/00__global_file_directories.R"))
```

## TODO change to csv_fread data.table::fread()

```{r}
        library(aws.s3)
        library(dbplyr) # to get from aws
        aws_prefix <- "deepDive/CleanData/SanFrancisco"
        Sys.setenv("AWS_DEFAULT_REGION"="us-east-2", TZ='GMT')
        awsDF <- get_bucket_df("beam-core-act", prefix = aws_prefix) # dataframe with all of the AWS files
        dataframe_of_files <- awsDF
```

```{r}
#List of files for this city (either locally or on AWS), and the stacked ones, that are not the previous files

    data_file_list_paths <- dataframe_of_files |> select(Key)
    data_file_list_paths <- data_file_list_paths |> 
              filter(grepl(pattern = "*.tacked*.",x = Key, ignore.case = TRUE))
    data_file_list_paths <- data_file_list_paths |> 
              filter(!grepl(pattern = "*.revious*.",x = Key, ignore.case = TRUE))
    # print(data_file_list_paths)
    data_file_list_paths <- data_file_list_paths |> 
              filter(grepl(pattern = paste0("*",placeTitleShort,"_","*."),
                           x = Key, ignore.case = TRUE))
    print(data_file_list_paths)
    
# Of those, the smaller List of files for this specific scenario (defined in YAML) (either locally or on AWS) . Hopefully there will be one cleaned, one cleaned subset, and one raw: , and cleaned stacked if there is one: wantSUBSETofTheVARSinsteadofAll
      data_file_list_paths <- data_file_list_paths |> 
          filter(grepl(pattern = paste0("*.",categoryTitleShort,
                                        "_",leverTitleShort),
                       x = Key, ignore.case = TRUE))
        print(data_file_list_paths)
```

```{r}
# Of those, the smaller List of files for this specific scenario (defined in YAML) (either locally or on AWS) . Hopefully there will be one cleaned, one cleaned subset, and one raw: , and cleaned stacked if there is one: wantSUBSETofTheVARSinsteadofAll
      data_file_list_paths <- data_file_list_paths |> 
          filter(grepl(pattern = paste0("*.",categoryTitleShort,
                                        "_",leverTitleShort),
                       x = Key, ignore.case = TRUE))
        print(data_file_list_paths)
```

```{r}
# not including the words old or previous
        data_file_list_tmp <- data_file_list_paths |> 
          filter(!grepl(pattern = "*.old*",x = Key, ignore.case = TRUE)) |>
          filter(!grepl(pattern = "*.previous*",x = Key, ignore.case = TRUE)) 
        data_file_list_tmp <- data_file_list_tmp$Key
        print(data_file_list_tmp)
```

```{r}
df_temp <- aws.s3::s3read_using(read_csv,
                             object = data_file_list_tmp,
                             bucket = "beam-core-act")
                          print("Re-cleaning data from AWS!")
                          
```

## TODO change to either

1.  csv read from csv csv_fread data.table::fread()
2.  fst::read_fst, but that ONLY reads a lookup table not the actual data? So not faster if you have to do something with it
3.  arrow::read_feather write to feather? write_feather arrow::read_feather,
4.  maybe data.table::fread ONLY reads writes csvs but does it fast

\# arrow::read_feather \# fst::read_fst

5.  OR qs::qread(file_qs), qs::qsave(test_df, file_qs),? Testing: https://github.com/tomaztk/Benchmarking-file-formats-for-cloud#using-r

6.  for EC2 -- feather -- https://community.rstudio.com/t/reading-feather-files-from-s3-from-ec2-instance-on-connect/38683/13

7.  Or FROM HADLEY AND JENNY: https://vroom.r-lib.org/ OH THIS MIGHT BE REALLY GOOD -- for AWS specificalky:

8.  Arrow feather https://ursalabs.org/arrow-r-nightly/articles/dataset.html arrow::arrow_with_s3() https://ursalabs.org/arrow-r-nightly/articles/dataset.html https://ursalabs.org/blog/2021-r-benchmarks-part-1/ library(arrow, warn.conflicts = FALSE) library(dplyr, warn.conflicts = FALSE) ds \<- open_dataset("nyc-taxi", partitioning = c("year", "month")) https://ursalabs.org/blog/2019-10-columnar-perf/ \## TODO save to aws?

OKAY OBVIOUSLY THIS IS THE BEST: HADLEY AND EC2: https://vroom.r-lib.org/articles/benchmarks.html

https://vroom.r-lib.org/articles/vroom.html#reading-remote-files

Read from EC2

https://vroom.r-lib.org/articles/vroom.html#reading-remote-files

https://github.com/tidyverse/vroom/tree/main/inst/bench

https://github.com/tidyverse/vroom/tree/main/inst/bench

VROOM_SHOW_PROGRESS

OH USE THIS REALLY

https://usethis.r-lib.org/articles/git-credentials.html

```{r save_rds_temp0}
# | eval: false
# | echo: false
names(df_temp)

    vroom::vroom_write(df_temp, file = paste0(data_dir_on_this_machine,
                   glue("{placeTitleShort}_{year}_",
                        "stacked_",
                        "{categoryTitleShort}_{leverTitleShort}",
                        "_0")))
```

# Make factors for everything

```{r}
df_temp <-   df_temp |> 
  mutate(across(where(is.character), as_factor))
```

# Save as \_1

, using vroom, which is basically readr

```{r}
vroom::vroom_write(df_temp, file = paste0(data_dir_on_this_machine,
                   glue("{placeTitleShort}_{year}_",
                        "stacked_",
                        "{categoryTitleShort}_{leverTitleShort}_",
                        "GraphClean",
                        "_1",   ""  )),
                progress = TRUE)
# readr::write_rds        ".rds")))
```

# list of variables, before subsetting:

```{r}
list_of_variables_before_subset <- as_tibble(names(df_temp))
readr::write_csv(list_of_variables_before_subset, 
                 file = paste0(data_dir_on_this_machine,
                               figures_folder,
                   glue("list_of_vars_",
                        "{categoryTitleShort}_{leverTitleShort}_",
                        "as_of",
                        Sys.Date(),
                        ".csv")))
```

Do next steps: subset, and then clean and create new vars

```{r}
rmarkdown::render("5__Subset_SelectVariables.qmd")
        rmarkdown::render("6__CreateNewVariables")
```

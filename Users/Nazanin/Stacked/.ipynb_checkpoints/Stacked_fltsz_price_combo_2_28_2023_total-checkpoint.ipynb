{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb40665e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\", color_codes=True)\n",
    "sns.set(font_scale=1.35, style=\"ticks\") #set styling preferences\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import math\n",
    "from math import pi\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.cluster.vq import kmeans2,vq, whiten\n",
    "import geopandas as gpd\n",
    "import h5py\n",
    "import boto.s3\n",
    "import glob\n",
    "import boto3\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07b419b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Shared-Work\\\\Data\\\\Ridehail_Fleetsize_Price_Scenarios.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read the CSV file into a DataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mShared-Work\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mRidehail_Fleetsize_Price_Scenarios.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Get the paths from the DataFrame\u001b[39;00m\n\u001b[0;32m      4\u001b[0m paths \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m11\u001b[39m:\u001b[38;5;241m17\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BEAM-CORE\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BEAM-CORE\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BEAM-CORE\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BEAM-CORE\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BEAM-CORE\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BEAM-CORE\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Shared-Work\\\\Data\\\\Ridehail_Fleetsize_Price_Scenarios.csv'"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('C:\\Shared-Work\\Data\\Ridehail_Fleetsize_Price_Scenarios.csv')\n",
    "# Get the paths from the DataFrame\n",
    "paths = df['Outputs'][11:17].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff8e6ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://storage.googleapis.com/beam-core-outputs/wheelchair-feb2023/sfbay-10pop-1fleet/inexus/sfbay_baseline_default-1.0_2021__20230319.csv.gz',\n",
       " 'https://storage.googleapis.com/beam-core-outputs/wheelchair-feb2023/sfbay-10pop-5fleet/inexus/sfbay_baseline_default-1.0_2021__20230211.csv.gz',\n",
       " 'https://storage.googleapis.com/beam-core-outputs/wheelchair-feb2023/sfbay-10pop-10fleet/inexus/sfbay_baseline_default-1.0_2021__20230208.csv.gz',\n",
       " 'https://storage.googleapis.com/beam-core-outputs/wheelchair-feb2023/sfbay-10pop-50fleet/inexus/sfbay_baseline_default-1.0_2021__20230211.csv.gz',\n",
       " 'https://storage.googleapis.com/beam-core-outputs/wheelchair-feb2023/sfbay-10pop-100fleet/inexus/sfbay_baseline_default-1.0_2021__20230211.csv.gz']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2549553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the S3 paths in the dataframe\n",
    "prefixs = []\n",
    "for path in paths:\n",
    "    prefix = path.replace('https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#', '')\n",
    "    prefix = prefix + 'inexus/'\n",
    "    prefixs.append(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a2f80f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "652a8a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 141 ms\n",
      "Wall time: 508 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "bucket_name = 'beam-outputs'\n",
    "\n",
    "# read all files into a list of dataframes\n",
    "key_list = []\n",
    "\n",
    "# navigate the folder and read the CSV files\n",
    "for prefix in prefixs:\n",
    "    # Use the S3 client to list all objects in the bucket and prefix\n",
    "    objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    # extract the keys of the CSV files from the object list\n",
    "    keys = [obj[\"Key\"] for obj in objects[\"Contents\"] if '_2019_' in obj['Key'] and obj[\"Key\"].endswith(\".csv.gz\")]\n",
    "    keys = ''.join(keys)\n",
    "    key_list.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a382430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pilates-outputs/sfbay_2fleets_47price_100fleet_30pct_20230226/inexus/sfbay_5_fleets_scenario_fleet_size-1000_2019__20230226.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_47price_100fleet_30pct_20230226/inexus/sfbay_rh_mixedprice5_rh_fltsz-100_2019__20230226.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_47price_164fleet_30pct_20230226/inexus/sfbay_5_fleets_scenario_fleet_size-1000_2019__20230226.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_47price_200fleet_30pct_20230226/inexus/sfbay_5_fleets_scenario_fleet_size-200_2019__20230225.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_47price_400fleet_30pct_20230226/inexus/sfbay_5_fleets_scenario_fleet_size-400_2019__20230225.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_47price_1000fleet_30pct_20230226/inexus/sfbay_5_fleets_scenario_fleet_size-1000_2019__20230226.csv.gz']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59919cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_use = ['IDMerged', 'tripIndex', 'actStartTime', 'actEndTime','duration_travelling', 'cost_BEAM', 'actStartType', \n",
    "               'actEndType', 'duration_walking', 'duration_in_privateCar', 'duration_on_bike', 'duration_in_ridehail', \n",
    "              'distance_travelling', 'duration_in_transit', 'distance_walking','distance_bike','distance_ridehail', \n",
    "              'distance_privateCar', 'distance_transit', 'mode_choice_planned_BEAM','mode_choice_actual_BEAM',\n",
    "              'vehicleIds', 'distance_mode_choice', 'replanning_status', 'reason', 'fuel_marginal','BlockGroupStart',\n",
    "              'startX', 'startY', 'bgid_start', 'tractid_start', 'juris_name_start', 'county_name_start', 'mpo_start', \n",
    "               'BlockGroupEnd', 'endX', 'endY', 'bgid_end', 'tractid_end', 'juris_name_end', 'county_name_end', 'mpo_end', \n",
    "               'emission_marginal', 'duration_door_to_door', 'waitTime_no_replanning', 'waitTime_replanning', 'actPurpose', \n",
    "               'mode_choice_actual_6', 'mode_choice_actual_5', 'mode_choice_actual_4', 'trip_mode_AS_trips', 'logsum_trip_Potential_INEXUS',\n",
    "               'age', 'income', 'hh_cars', 'TAZ_x', 'origin_x', 'destination_x', 'TAZ_y', 'home_taz', 'auto_ownership', 'home_is_urban', 'home_is_rural', 'DRIVEALONEFREE',\n",
    "               'DRIVEALONEPAY', 'SHARED2FREE', 'SHARED2PAY', 'SHARED3FREE', 'SHARED3PAY', 'WALK', 'BIKE', 'WALK_LOC', 'WALK_LRF', \n",
    "               'WALK_EXP', 'WALK_HVY', 'WALK_COM', 'DRIVE_LOC', 'DRIVE_LRF', 'DRIVE_EXP', 'DRIVE_HVY', 'DRIVE_COM', 'TAXI',\n",
    "               'TNC_SINGLE', 'TNC_SHARED', 'income_quartiles', 'income_deciles' ] # Specify the columns to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3d2ee41",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# define the filenames for each row in the DataFrame\n",
    "filenames = []\n",
    "\n",
    "for key in key_list:\n",
    "    # split the key based on the `/` separator and take the last element\n",
    "    filename = key.split('/')[-1].rstrip('.csv.gz')\n",
    "    #filename = filename.split(\"_2019\")[0]  # remove anything after the year 2019\n",
    "    filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fca42f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sfbay_5_fleets_scenario_fleet_size-1000_2019__20230226',\n",
       " 'sfbay_rh_mixedprice5_rh_fltsz-100_2019__20230226',\n",
       " 'sfbay_5_fleets_scenario_fleet_size-1000_2019__20230226',\n",
       " 'sfbay_5_fleets_scenario_fleet_size-200_2019__20230225',\n",
       " 'sfbay_5_fleets_scenario_fleet_size-400_2019__20230225',\n",
       " 'sfbay_5_fleets_scenario_fleet_size-1000_2019__20230226']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb18208",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_values = {}\n",
    "    \n",
    "for i, row in df[23:].reset_index(drop=True).iterrows():\n",
    "    # your code here.iterrows():\n",
    "    # extract the custom values for the current row\n",
    "    custom_value_1 = row['lever_position_price']\n",
    "    custom_value_2 = row['lever_position_fltsz']\n",
    "    custom_value_3 = row['lever_n_fleets']\n",
    "    custom_value_4 = row['fleetsize_uber']\n",
    "    custom_value_5 = row['fleetsize_lyft']\n",
    "    custom_value_6 = row['fleetsize_cruise']\n",
    "    custom_value_7 = row['fleetsize_flywheel']\n",
    "    custom_value_8 = row['fleetsize_waymo']\n",
    "    # convert the \"lever_position_price\" value from percent to decimal\n",
    "    custom_value_1_decimal = float(custom_value_1.strip('%')) / 100\n",
    "    custom_value_2_decimal = float(custom_value_2.strip('%')) / 100\n",
    "    \n",
    "    # get the filename for the current row\n",
    "    filename = filenames[i]  \n",
    "    \n",
    "    # check if the filename already exists in the dictionary\n",
    "    if filename not in custom_values:\n",
    "        # create a new dictionary entry for the filename if it doesn't exist\n",
    "        custom_values[filename] = []\n",
    "        \n",
    "    custom_value_dict = {'lever_position_price': custom_value_1_decimal, 'lever_position_fltsz': custom_value_2_decimal, \n",
    "                         'lever_n_fleets': custom_value_3, 'fleetsize_uber': custom_value_4, 'fleetsize_lyft': custom_value_5,\n",
    "                         'fleetsize_cruise': custom_value_6,'fleetsize_flywheel': custom_value_7,'fleetsize_waymo': custom_value_8}\n",
    "    \n",
    "    # append the custom values to the existing list for the filename\n",
    "    custom_values[filename].append(custom_value_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa94ab29",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (27,234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (27,234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (27,234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (27,234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (27,234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2h 5min 18s\n",
      "Wall time: 2h 5min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfs = []\n",
    "\n",
    "for key in key_list:\n",
    "    obj = s3.get_object(Bucket=\"beam-outputs\", Key=key)\n",
    "    sf_files = pd.read_csv(obj['Body'], compression = 'gzip', usecols = cols_to_use)\n",
    "    \n",
    "    filename = key.split('/')[-1].rstrip('.csv.gz')\n",
    "    #filename = filename.split(\"_2019\")[0]  # remove anything after the year 2019\n",
    "    \n",
    "    for custom_value_dict in custom_values[filename]:\n",
    "        sf_files = sf_files.assign(**custom_value_dict)\n",
    "    sf_files['year'] = 2018\n",
    "    #append the dataframe to the list of dataframes\n",
    "    dfs.append(sf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4f555b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all columns and rows\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0494b26f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def generate_summary_table(df):\n",
    "    df['socialCarbonCost'] = df['emission_marginal']*185\n",
    "    df['incomeInThousands'] = df['income']/1000\n",
    "    df = df[df['incomeInThousands'].notna()]\n",
    "    person_income = pd.pivot_table(df, index=['IDMerged'], aggfunc={'incomeInThousands': lambda x: ', '.join(set(x.dropna().astype(str)))}).reset_index() \n",
    "    person_income['incomeInThousands'] = person_income['incomeInThousands'].astype(float)\n",
    "   \n",
    "    # Add a column of income ranks\n",
    "    twenty_one_ranks = person_income['incomeInThousands'].quantile([0, 0.048, 0.095, 0.143, 0.191, 0.239, 0.287, 0.335, 0.383, 0.431, 0.479,\n",
    "                                                                0.527, 0.575, 0.623, 0.671, 0.719, 0.767, 0.815, 0.863, 0.911, 0.959,\n",
    "                                                                1]).tolist()\n",
    "    \n",
    "    # Add incomeInThousands twenty_one_ranks\n",
    "    conditions  = [(person_income['incomeInThousands'] >= twenty_one_ranks[0]) & (person_income['incomeInThousands'] < twenty_one_ranks[1]), \n",
    "               (person_income['incomeInThousands'] >= twenty_one_ranks[1]) & (person_income['incomeInThousands'] < twenty_one_ranks[2]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[2]) & (person_income['incomeInThousands'] < twenty_one_ranks[3]),\n",
    "               (person_income['incomeInThousands'] >= twenty_one_ranks[3]) & (person_income['incomeInThousands'] < twenty_one_ranks[4]), \n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[4]) & (person_income['incomeInThousands'] < twenty_one_ranks[5]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[5]) & (person_income['incomeInThousands'] < twenty_one_ranks[6]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[6]) & (person_income['incomeInThousands'] < twenty_one_ranks[7]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[7]) & (person_income['incomeInThousands'] < twenty_one_ranks[8]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[8]) & (person_income['incomeInThousands'] < twenty_one_ranks[9]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[9]) & (person_income['incomeInThousands'] <= twenty_one_ranks[10]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[10]) & (person_income['incomeInThousands'] <= twenty_one_ranks[11]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[11]) & (person_income['incomeInThousands'] <= twenty_one_ranks[12]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[12]) & (person_income['incomeInThousands'] <= twenty_one_ranks[13]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[13]) & (person_income['incomeInThousands'] <= twenty_one_ranks[14]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[14]) & (person_income['incomeInThousands'] <= twenty_one_ranks[15]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[15]) & (person_income['incomeInThousands'] <= twenty_one_ranks[16]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[16]) & (person_income['incomeInThousands'] <= twenty_one_ranks[17]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[17]) & (person_income['incomeInThousands'] <= twenty_one_ranks[18]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[18]) & (person_income['incomeInThousands'] <= twenty_one_ranks[19]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[19]) & (person_income['incomeInThousands'] <= twenty_one_ranks[20]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[20]) & (person_income['incomeInThousands'] <= twenty_one_ranks[21])]\n",
    "    choices = [0, 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "    \n",
    "    person_income['incRank'] = np.select(conditions, choices, default=None)\n",
    "    \n",
    "    df = pd.merge(left = df, right = person_income, how='left', on = ['IDMerged'], suffixes=('', '_drop'))\n",
    "    \n",
    "    df.drop([col for col in df.columns if 'drop' in col], axis=1, inplace=True)\n",
    "    \n",
    "    # Mean pivot table\n",
    "    mean_inc_table = pd.pivot_table(df,index=['incRank'], aggfunc={'incomeInThousands': np.mean,\n",
    "           'logsum_trip_Potential_INEXUS': np.mean,\n",
    "            'duration_travelling': np.mean,\n",
    "            'duration_door_to_door': np.mean,\n",
    "            'duration_walking': np.mean,\n",
    "            'duration_in_privateCar': np.mean,\n",
    "            'duration_on_bike': np.mean,\n",
    "            'duration_in_ridehail': np.mean,\n",
    "            'duration_in_transit': np.mean,\n",
    "            'waitTime_no_replanning': np.mean,\n",
    "            'waitTime_replanning': np.mean,\n",
    "            'distance_travelling': np.mean,\n",
    "            'distance_walking': np.mean,\n",
    "            'distance_bike': np.mean,\n",
    "            'distance_ridehail': np.mean,\n",
    "            'distance_privateCar': np.mean,\n",
    "            'distance_transit': np.mean,\n",
    "            'distance_mode_choice': np.mean,\n",
    "            'replanning_status': np.mean,\n",
    "            'fuel_marginal': np.mean,\n",
    "            'emission_marginal': np.mean,\n",
    "            'cost_BEAM': np.mean,\n",
    "            'socialCarbonCost':np.mean,\n",
    "           'lever_position_price':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "           'lever_position_fltsz':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "           'lever_n_fleets':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "            'fleetsize_uber':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "            'fleetsize_lyft':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "            'fleetsize_cruise':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "            'fleetsize_flywheel':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "            'fleetsize_waymo':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "           'IDMerged': 'nunique',\n",
    "           'tripIndex': 'nunique'}).reset_index()\n",
    "    # Median summary table\n",
    "    median_inc_table = pd.pivot_table(df, index=['incRank'], aggfunc={'incomeInThousands': np.median,\n",
    "           'logsum_trip_Potential_INEXUS': np.median,\n",
    "            'duration_travelling': np.median,\n",
    "            'duration_door_to_door': np.median,\n",
    "            'duration_walking':np.median,\n",
    "            'duration_in_privateCar': np.median,\n",
    "            'duration_on_bike': np.median,\n",
    "            'duration_in_ridehail': np.median,\n",
    "            'duration_in_transit': np.median,\n",
    "            'waitTime_no_replanning': np.median,\n",
    "            'waitTime_replanning': np.median,\n",
    "            'distance_travelling': np.median,\n",
    "            'distance_walking': np.median,\n",
    "            'distance_bike': np.median,\n",
    "            'distance_ridehail': np.median,\n",
    "            'distance_privateCar': np.median,\n",
    "            'distance_transit': np.median,\n",
    "            'distance_mode_choice': np.median,\n",
    "            'replanning_status': np.median,\n",
    "            'fuel_marginal': np.median,\n",
    "            'emission_marginal': np.median,\n",
    "            'cost_BEAM': np.median,\n",
    "            'socialCarbonCost':np.median\n",
    "           }).reset_index() \n",
    "   # Sum summary table\n",
    "    sum_inc_table = pd.pivot_table(df, index=['incRank'], aggfunc={'duration_travelling': np.sum,\n",
    "            'duration_door_to_door': np.sum,\n",
    "            'duration_walking': np.sum,\n",
    "            'duration_in_privateCar': np.sum,\n",
    "            'duration_on_bike': np.sum,\n",
    "            'duration_in_ridehail': np.sum,\n",
    "            'duration_in_transit': np.sum,\n",
    "            'waitTime_no_replanning': np.sum,\n",
    "            'waitTime_replanning': np.sum,\n",
    "            'distance_travelling': np.sum,\n",
    "            'distance_walking': np.sum,\n",
    "            'distance_bike': np.sum,\n",
    "            'distance_ridehail': np.sum,\n",
    "            'distance_privateCar': np.sum,\n",
    "            'distance_transit': np.sum,\n",
    "            'distance_mode_choice': np.sum,\n",
    "            'replanning_status': np.sum,\n",
    "            'fuel_marginal': np.sum,\n",
    "            'emission_marginal': np.sum,\n",
    "            'cost_BEAM': np.sum,\n",
    "            'socialCarbonCost':np.sum\n",
    "           }).reset_index() \n",
    "            \n",
    "    mode_counts = df.groupby(['incRank', 'mode_choice_actual_BEAM'])['mode_choice_actual_BEAM'].count().unstack().add_prefix('mode_').reset_index()\n",
    "    sum_inc_table = sum_inc_table.merge(mode_counts, on='incRank').assign(mode_ridehail_total = lambda x: x['mode_ride_hail'] + x['mode_ride_hail_pooled'])\n",
    "    sum_inc_table.columns = [col + '_sum' for col in sum_inc_table.columns]\n",
    "    sum_inc_table = sum_inc_table.rename(columns={'incRank_sum': 'incRank'})\n",
    "    mm_inc_table = pd.merge(mean_inc_table, median_inc_table, on='incRank', suffixes=('_mean', '_median'))\n",
    "    summary_inc_table = pd.merge(mm_inc_table, sum_inc_table, on='incRank', how='left')\n",
    "    summary_inc_table = summary_inc_table.rename(columns={'tripIndex': 'n_trips', \n",
    "                                                      'IDMerged': 'n_agents', \n",
    "                                                      'logsum_trip_Potential_INEXUS_median': 'Potential_INEXUS_median',\n",
    "                                                      'logsum_trip_Potential_INEXUS_mean': 'Potential_INEXUS_mean',\n",
    "                                                      'mode_ride_hail_sum': 'mode_ride_hail_solo_sum'})        \n",
    "    \n",
    "    summary_inc_table = summary_inc_table.iloc[:, :2].join(summary_inc_table.iloc[:, 2:].sort_index(axis=1)) \n",
    "    \n",
    "    # shift column 'person' to first position\n",
    "    third_column = summary_inc_table.pop('n_trips')\n",
    "    \n",
    "    # insert column using insert(position,column_name,first_column) function\n",
    "    summary_inc_table.insert(2, 'n_trips', third_column)\n",
    "    \n",
    "    # calculate median and mean wait times by mode and incRank\n",
    "    grouped_waitTime_no_replanning = df.groupby(['mode_choice_actual_BEAM', 'incRank'])['waitTime_no_replanning'].agg(['mean', 'median']).reset_index()\n",
    "    \n",
    "    # pivot the wait time statistics by incRank and mode\n",
    "    waittime_pivot = pd.pivot_table(grouped_waitTime_no_replanning, index=['incRank'], columns=['mode_choice_actual_BEAM'],\n",
    "    values=['median', 'mean'], aggfunc=np.sum)\n",
    "    \n",
    "    # flatten the multi-index column names and rename them with appropriate suffixes\n",
    "    waittime_pivot.columns = [f\"waitTime_no_replanning_{mode}_{agg}\" for (agg, mode) in waittime_pivot.columns.to_flat_index()]\n",
    "    \n",
    "    # merge the wait time statistics pivot table with the original pivot table\n",
    "    summary_inc_table = summary_inc_table.merge(waittime_pivot, on='incRank', how='left')\n",
    "    \n",
    "    # calculate median and mean wait times by mode and incRank\n",
    "    grouped_waitTime_replanning = df.groupby(['mode_choice_planned_BEAM', 'incRank'])['waitTime_replanning'].agg(['mean', 'median']).reset_index()\n",
    "    \n",
    "    # pivot the wait time statistics by incRank and mode\n",
    "    waittime_pivot = pd.pivot_table(grouped_waitTime_replanning, index=['incRank'], columns=['mode_choice_planned_BEAM'], values=['median', 'mean'], aggfunc=np.sum)\n",
    "    \n",
    "    # flatten the multi-index column names and rename them with appropriate suffixes\n",
    "    waittime_pivot.columns = [f\"waitTime_replanning_{mode}_{agg}\" for (agg, mode) in waittime_pivot.columns.to_flat_index()]\n",
    "    \n",
    "    # merge the wait time statistics pivot table with the original pivot table\n",
    "    summary_inc_table = summary_inc_table.merge(waittime_pivot, on='incRank', how='left')\n",
    "    return summary_inc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "537fef6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1h 28min 32s\n",
      "Wall time: 1h 28min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfs_summary = []\n",
    "for df in dfs:\n",
    "    df_summary = generate_summary_table(df)\n",
    "    dfs_summary.append(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d29ffc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 5.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sf_stacked = pd.concat(dfs_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e76e4c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_stacked.to_csv('s3://beam-core-act/deepDive/CleanData/SanFrancisco/Stacked/sf_2018_stacked_rh_fltsz_price_incRank_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de7a8b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 77.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stacked_rh_path = \"https://beam-core-act.s3.amazonaws.com/deepDive/CleanData/SanFrancisco/Stacked/\"\n",
    "sf_stacked_12 = pd.read_csv(stacked_rh_path + 'sf_2018_stacked_rh_fltsz_price_incRank_1_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3333244",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [sf_stacked_12, sf_stacked]\n",
    "sf_stacked123 = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4c16cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_stacked123.to_csv('s3://beam-core-act/deepDive/CleanData/SanFrancisco/Stacked/sf_2018_stacked_rh_fltsz_price_incRank_1_2_3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

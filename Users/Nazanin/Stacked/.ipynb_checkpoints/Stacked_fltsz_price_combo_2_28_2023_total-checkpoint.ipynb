{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb40665e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\", color_codes=True)\n",
    "sns.set(font_scale=1.35, style=\"ticks\") #set styling preferences\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import math\n",
    "from math import pi\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.cluster.vq import kmeans2,vq, whiten\n",
    "import geopandas as gpd\n",
    "import h5py\n",
    "import boto.s3\n",
    "import glob\n",
    "import boto3\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07b419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('C:/Users/nrezaei/Documents/Ridehail_Fleetsize_Price_Scenarios.csv') \n",
    "# Get the paths from the DataFrame\n",
    "paths = df['Outputs'][26:].tolist()  \n",
    "# Because there are some files with similar names, I ran the code multiple times using these chunks:\n",
    "#1-13, 13-16, 16-23, 23-24, 24-25, 25-26, 26:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff8e6ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_12_5price_200fleet_30pct_20230226/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_12_5price_400fleet_30pct_20230226/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_12_5price_1000fleet_30pct_20230226/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_2fleets_6_25_price_100fleet_30pct_20230223/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_6_25price_100fleet_30pct_20230218/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_6_25price_164fleet_30pct_20230218/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_6_25price_200fleet_30pct_20230223/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_6_25_price_400fleet_30pct_20230223/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_6_25_price_1000fleet_30pct_20230223/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_1fleet_mix_price_100fleet_30pct_20230223/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_2fleets_mix_price_100fleet_30pct_20230226/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_mixprice_100fleet_30pct_20230223/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_mix_price_164fleet_30pct_20230226/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_mixprice_200fleet_30pct_20230223/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_mixprice_400fleet_30pct_20230223/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_mixprice_1000fleet_30pct_20230223/']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2549553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the S3 paths in the dataframe\n",
    "prefixs = []\n",
    "for path in paths:\n",
    "    prefix = path.replace('https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#', '')\n",
    "    prefix = prefix + 'inexus/'\n",
    "    prefixs.append(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a2f80f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pilates-outputs/sfbay_5fleets_12_5price_200fleet_30pct_20230226/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_12_5price_400fleet_30pct_20230226/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_12_5price_1000fleet_30pct_20230226/inexus/',\n",
       " 'pilates-outputs/sfbay_2fleets_6_25_price_100fleet_30pct_20230223/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_6_25price_100fleet_30pct_20230218/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_6_25price_164fleet_30pct_20230218/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_6_25price_200fleet_30pct_20230223/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_6_25_price_400fleet_30pct_20230223/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_6_25_price_1000fleet_30pct_20230223/inexus/',\n",
       " 'pilates-outputs/sfbay_1fleet_mix_price_100fleet_30pct_20230223/inexus/',\n",
       " 'pilates-outputs/sfbay_2fleets_mix_price_100fleet_30pct_20230226/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_mixprice_100fleet_30pct_20230223/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_mix_price_164fleet_30pct_20230226/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_mixprice_200fleet_30pct_20230223/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_mixprice_400fleet_30pct_20230223/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_mixprice_1000fleet_30pct_20230223/inexus/']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "652a8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "bucket_name = 'beam-outputs'\n",
    "\n",
    "# read all files into a list of dataframes\n",
    "key_list = []\n",
    "\n",
    "# navigate the folder and read the CSV files\n",
    "for prefix in prefixs:\n",
    "    # Use the S3 client to list all objects in the bucket and prefix\n",
    "    objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    # extract the keys of the CSV files from the object list\n",
    "    keys = [obj[\"Key\"] for obj in objects[\"Contents\"] if '_2019_' in obj['Key'] and obj[\"Key\"].endswith(\".csv.gz\")]\n",
    "    keys = ''.join(keys)\n",
    "    key_list.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a382430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pilates-outputs/sfbay_5fleets_12_5price_200fleet_30pct_20230226/inexus/sfbay_mixedprice_fltsz-200_2019__20230225.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_12_5price_400fleet_30pct_20230226/inexus/sfbay_mixedprice_fleet_size-400_2019__20230225.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_12_5price_1000fleet_30pct_20230226/inexus/sfbay_5_fleets_scenario_fleet_size-1000_2019__20230226.csv.gz',\n",
       " 'pilates-outputs/sfbay_2fleets_6_25_price_100fleet_30pct_20230223/inexus/sfbay_rh_bae_price_rh_price-6.25_2019__20230223.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_6_25price_100fleet_30pct_20230218/inexus/sfbay_rh_price_0625_rh_price-6.25_2019__20230220.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_6_25price_164fleet_30pct_20230218/inexus/sfbay_rh_price_price-6.25_2019__20230220.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_6_25price_200fleet_30pct_20230223/inexus/sfbay_5_fleets_scenario_fleet_size-200_2019__20230223.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_6_25_price_400fleet_30pct_20230223/inexus/sfbay_5_fleets_scenario_fleet_size-400_2019__20230223.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_6_25_price_1000fleet_30pct_20230223/inexus/sfbay_5_fleets_scenario_fleet_size-1000_2019__20230224.csv.gz',\n",
       " 'pilates-outputs/sfbay_1fleet_mix_price_100fleet_30pct_20230223/inexus/sfbay_rh_1fleet_fleets-1_2019__20230224.csv.gz',\n",
       " 'pilates-outputs/sfbay_2fleets_mix_price_100fleet_30pct_20230226/inexus/sfbay_rh_bae_price_rh_price-6.25_2019__20230225.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_mixprice_100fleet_30pct_20230223/inexus/sfbay_rh_mixedprice5_rh_fltsz-100_2019__20230223.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_mix_price_164fleet_30pct_20230226/inexus/sfbay_rh_mixedprice_rhfltsz-164_2019__20230225.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_mixprice_200fleet_30pct_20230223/inexus/sfbay_mixedprice_fltsz-200_2019__20230223.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_mixprice_400fleet_30pct_20230223/inexus/sfbay_mixedprice_fleet_size-400_2019__20230223.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_mixprice_1000fleet_30pct_20230223/inexus/sfbay_mixed_price_fleetsz-1000_2019__20230224.csv.gz']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59919cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_use = ['IDMerged', 'tripIndex', 'actStartTime', 'actEndTime','duration_travelling', 'cost_BEAM', 'actStartType', \n",
    "               'actEndType', 'duration_walking', 'duration_in_privateCar', 'duration_on_bike', 'duration_in_ridehail', \n",
    "              'distance_travelling', 'duration_in_transit', 'distance_walking','distance_bike','distance_ridehail', \n",
    "              'distance_privateCar', 'distance_transit', 'mode_choice_planned_BEAM','mode_choice_actual_BEAM',\n",
    "              'vehicleIds', 'distance_mode_choice', 'replanning_status', 'reason', 'fuel_marginal','BlockGroupStart',\n",
    "              'startX', 'startY', 'bgid_start', 'tractid_start', 'juris_name_start', 'county_name_start', 'mpo_start', \n",
    "               'BlockGroupEnd', 'endX', 'endY', 'bgid_end', 'tractid_end', 'juris_name_end', 'county_name_end', 'mpo_end', \n",
    "               'emission_marginal', 'duration_door_to_door', 'waitTime_no_replanning', 'waitTime_replanning', 'actPurpose', \n",
    "               'mode_choice_actual_6', 'mode_choice_actual_5', 'mode_choice_actual_4', 'trip_mode_AS_trips', 'logsum_trip_Potential_INEXUS',\n",
    "               'age', 'income', 'hh_cars', 'TAZ_x', 'origin_x', 'destination_x', 'TAZ_y', 'home_taz', 'auto_ownership', 'home_is_urban', 'home_is_rural', 'DRIVEALONEFREE',\n",
    "               'DRIVEALONEPAY', 'SHARED2FREE', 'SHARED2PAY', 'SHARED3FREE', 'SHARED3PAY', 'WALK', 'BIKE', 'WALK_LOC', 'WALK_LRF', \n",
    "               'WALK_EXP', 'WALK_HVY', 'WALK_COM', 'DRIVE_LOC', 'DRIVE_LRF', 'DRIVE_EXP', 'DRIVE_HVY', 'DRIVE_COM', 'TAXI',\n",
    "               'TNC_SINGLE', 'TNC_SHARED', 'income_quartiles', 'income_deciles', 'value_of_time', 'primary_purpose_x',\n",
    "              ] # Specify the columns to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3d2ee41",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# define the filenames for each row in the DataFrame\n",
    "filenames = []\n",
    "\n",
    "for key in key_list:\n",
    "    # split the key based on the `/` separator and take the last element\n",
    "    filename = key.split('/')[-1].rstrip('.csv.gz')\n",
    "    #filename = filename.split(\"_2019\")[0]  # remove anything after the year 2019\n",
    "    filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fca42f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sfbay_mixedprice_fltsz-200_2019__20230225',\n",
       " 'sfbay_mixedprice_fleet_size-400_2019__20230225',\n",
       " 'sfbay_5_fleets_scenario_fleet_size-1000_2019__20230226',\n",
       " 'sfbay_rh_bae_price_rh_price-6.25_2019__20230223',\n",
       " 'sfbay_rh_price_0625_rh_price-6.25_2019__20230220',\n",
       " 'sfbay_rh_price_price-6.25_2019__20230220',\n",
       " 'sfbay_5_fleets_scenario_fleet_size-200_2019__20230223',\n",
       " 'sfbay_5_fleets_scenario_fleet_size-400_2019__20230223',\n",
       " 'sfbay_5_fleets_scenario_fleet_size-1000_2019__20230224',\n",
       " 'sfbay_rh_1fleet_fleets-1_2019__20230224',\n",
       " 'sfbay_rh_bae_price_rh_price-6.25_2019__20230225',\n",
       " 'sfbay_rh_mixedprice5_rh_fltsz-100_2019__20230223',\n",
       " 'sfbay_rh_mixedprice_rhfltsz-164_2019__20230225',\n",
       " 'sfbay_mixedprice_fltsz-200_2019__20230223',\n",
       " 'sfbay_mixedprice_fleet_size-400_2019__20230223',\n",
       " 'sfbay_mixed_price_fleetsz-1000_2019__20230224']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddb18208",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_values = {}\n",
    "    \n",
    "for i, row in df[26:].reset_index(drop=True).iterrows():\n",
    "    # your code here.iterrows():\n",
    "    # extract the custom values for the current row\n",
    "    custom_value_1 = row['lever_position_price']\n",
    "    custom_value_2 = row['lever_position_fltsz']\n",
    "    custom_value_3 = row['lever_n_fleets']\n",
    "    custom_value_4 = row['fleetsize_uber']\n",
    "    custom_value_5 = row['fleetsize_lyft']\n",
    "    custom_value_6 = row['fleetsize_cruise']\n",
    "    custom_value_7 = row['fleetsize_flywheel']\n",
    "    custom_value_8 = row['fleetsize_waymo']\n",
    "    # convert the \"lever_position_price\" value from percent to decimal\n",
    "    custom_value_1_decimal = float(custom_value_1.strip('%')) / 100\n",
    "    custom_value_2_decimal = float(custom_value_2.strip('%')) / 100\n",
    "    \n",
    "    # get the filename for the current row\n",
    "    filename = filenames[i]  \n",
    "    \n",
    "    # check if the filename already exists in the dictionary\n",
    "    if filename not in custom_values:\n",
    "        # create a new dictionary entry for the filename if it doesn't exist\n",
    "        custom_values[filename] = []\n",
    "        \n",
    "    custom_value_dict = {'lever_position_price': custom_value_1_decimal, 'lever_position_fltsz': custom_value_2_decimal, \n",
    "                         'lever_n_fleets': custom_value_3, 'fleetsize_uber': custom_value_4, 'fleetsize_lyft': custom_value_5,\n",
    "                         'fleetsize_cruise': custom_value_6,'fleetsize_flywheel': custom_value_7,'fleetsize_waymo': custom_value_8}\n",
    "    \n",
    "    # append the custom values to the existing list for the filename\n",
    "    custom_values[filename].append(custom_value_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa94ab29",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (27,234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (27,234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (27,234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2h 6min 20s\n",
      "Wall time: 2h 6min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfs = []\n",
    "\n",
    "for key in key_list:\n",
    "    obj = s3.get_object(Bucket=\"beam-outputs\", Key=key)\n",
    "    sf_files = pd.read_csv(obj['Body'], compression = 'gzip', usecols = cols_to_use)\n",
    "    \n",
    "    filename = key.split('/')[-1].rstrip('.csv.gz')\n",
    "    #filename = filename.split(\"_2019\")[0]  # remove anything after the year 2019\n",
    "    \n",
    "    for custom_value_dict in custom_values[filename]:\n",
    "        sf_files = sf_files.assign(**custom_value_dict)\n",
    "    sf_files['year'] = 2018\n",
    "    #append the dataframe to the list of dataframes\n",
    "    dfs.append(sf_files)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4f555b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all columns and rows\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0494b26f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def generate_summary_table(df):\n",
    "    df['socialCarbonCost'] = df['emission_marginal']*185\n",
    "    df['incomeInThousands'] = df['income']/1000\n",
    "    df = df[df['incomeInThousands'].notna()]\n",
    "    person_income = pd.pivot_table(df, index=['IDMerged'], aggfunc={'incomeInThousands': lambda x: ', '.join(set(x.dropna().astype(str)))}).reset_index() \n",
    "    person_income['incomeInThousands'] = person_income['incomeInThousands'].astype(float)\n",
    "\n",
    "    # Add a column of income ranks\n",
    "    twenty_one_ranks = person_income['incomeInThousands'].quantile([0, 0.048, 0.095, 0.143, 0.191, 0.239, 0.287, 0.335, 0.383, 0.431, 0.479,\n",
    "                                                                0.527, 0.575, 0.623, 0.671, 0.719, 0.767, 0.815, 0.863, 0.911, 0.959,\n",
    "                                                                1]).tolist()    \n",
    "    # Add incomeInThousands twenty_one_ranks\n",
    "    conditions  = [(person_income['incomeInThousands'] >= twenty_one_ranks[0]) & (person_income['incomeInThousands'] < twenty_one_ranks[1]), \n",
    "               (person_income['incomeInThousands'] >= twenty_one_ranks[1]) & (person_income['incomeInThousands'] < twenty_one_ranks[2]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[2]) & (person_income['incomeInThousands'] < twenty_one_ranks[3]),\n",
    "               (person_income['incomeInThousands'] >= twenty_one_ranks[3]) & (person_income['incomeInThousands'] < twenty_one_ranks[4]), \n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[4]) & (person_income['incomeInThousands'] < twenty_one_ranks[5]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[5]) & (person_income['incomeInThousands'] < twenty_one_ranks[6]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[6]) & (person_income['incomeInThousands'] < twenty_one_ranks[7]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[7]) & (person_income['incomeInThousands'] < twenty_one_ranks[8]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[8]) & (person_income['incomeInThousands'] < twenty_one_ranks[9]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[9]) & (person_income['incomeInThousands'] <= twenty_one_ranks[10]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[10]) & (person_income['incomeInThousands'] <= twenty_one_ranks[11]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[11]) & (person_income['incomeInThousands'] <= twenty_one_ranks[12]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[12]) & (person_income['incomeInThousands'] <= twenty_one_ranks[13]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[13]) & (person_income['incomeInThousands'] <= twenty_one_ranks[14]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[14]) & (person_income['incomeInThousands'] <= twenty_one_ranks[15]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[15]) & (person_income['incomeInThousands'] <= twenty_one_ranks[16]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[16]) & (person_income['incomeInThousands'] <= twenty_one_ranks[17]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[17]) & (person_income['incomeInThousands'] <= twenty_one_ranks[18]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[18]) & (person_income['incomeInThousands'] <= twenty_one_ranks[19]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[19]) & (person_income['incomeInThousands'] <= twenty_one_ranks[20]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[20]) & (person_income['incomeInThousands'] <= twenty_one_ranks[21])]\n",
    "    choices = [0, 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "    \n",
    "    person_income['incRank'] = np.select(conditions, choices, default=None)\n",
    "    \n",
    "    df = pd.merge(left = df, right = person_income, how='left', on = ['IDMerged'], suffixes=('', '_drop'))\n",
    "    \n",
    "    df.drop([col for col in df.columns if 'drop' in col], axis=1, inplace=True)\n",
    "    \n",
    "    # Calculate the Potential INEXUS in dollar\n",
    "    conditions1  = [(df['primary_purpose_x'] == 'work'),(df['primary_purpose_x'] == 'univ'),\n",
    "               (df['primary_purpose_x'] == 'school'),(df['primary_purpose_x'] == 'escort'),\n",
    "              (df['primary_purpose_x'] == 'shopping'),(df['primary_purpose_x'] == 'eatout'),\n",
    "              (df['primary_purpose_x'] == 'othmaint'),(df['primary_purpose_x'] == 'social'),\n",
    "              (df['primary_purpose_x'] == 'othdiscr'),(df['primary_purpose_x'] == 'atwork')]\n",
    "                                                              \n",
    "    choices1 = [-0.022, -0.0271, -0.0271, -0.0279, -0.0279, -0.0279, -0.0175, -0.0175, -0.0279, -0.0279]\n",
    "    \n",
    "    df['c_ivt'] = np.select(conditions1, choices1, default=np.nan)\n",
    "    \n",
    "    df['alpha'] = -0.6 *(df['c_ivt'])/(df['value_of_time'])\n",
    "    \n",
    "    df['Potential_INEXUS_in_dollar_2023'] = ((df['logsum_trip_Potential_INEXUS']/df['alpha'])/100)*1.75\n",
    "    \n",
    "    # Mean pivot table\n",
    "    mean_inc_table = pd.pivot_table(df,index=['incRank'], aggfunc={'incomeInThousands': np.mean,\n",
    "           'Potential_INEXUS_in_dollar_2023': np.mean,\n",
    "            'duration_travelling': np.mean,\n",
    "            'duration_door_to_door': np.mean,\n",
    "            'duration_walking': np.mean,\n",
    "            'duration_in_privateCar': np.mean,\n",
    "            'duration_on_bike': np.mean,\n",
    "            'duration_in_ridehail': np.mean,\n",
    "            'duration_in_transit': np.mean,\n",
    "            'waitTime_no_replanning': np.mean,\n",
    "            'waitTime_replanning': np.mean,\n",
    "            'distance_travelling': np.mean,\n",
    "            'distance_walking': np.mean,\n",
    "            'distance_bike': np.mean,\n",
    "            'distance_ridehail': np.mean,\n",
    "            'distance_privateCar': np.mean,\n",
    "            'distance_transit': np.mean,\n",
    "            'distance_mode_choice': np.mean,\n",
    "            'replanning_status': np.mean,\n",
    "            'fuel_marginal': np.mean,\n",
    "            'emission_marginal': np.mean,\n",
    "            'cost_BEAM': np.mean,\n",
    "            'socialCarbonCost':np.mean,\n",
    "           'lever_position_price':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "           'lever_position_fltsz':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "           'lever_n_fleets':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "            'fleetsize_uber':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "            'fleetsize_lyft':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "            'fleetsize_cruise':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "            'fleetsize_flywheel':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "            'fleetsize_waymo':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "           'IDMerged': 'nunique',\n",
    "           'tripIndex': 'nunique'}).reset_index()\n",
    "    # Median summary table\n",
    "    median_inc_table = pd.pivot_table(df, index=['incRank'], aggfunc={'incomeInThousands': np.median,\n",
    "           'Potential_INEXUS_in_dollar_2023': np.median,\n",
    "            'duration_travelling': np.median,\n",
    "            'duration_door_to_door': np.median,\n",
    "            'duration_walking':np.median,\n",
    "            'duration_in_privateCar': np.median,\n",
    "            'duration_on_bike': np.median,\n",
    "            'duration_in_ridehail': np.median,\n",
    "            'duration_in_transit': np.median,\n",
    "            'waitTime_no_replanning': np.median,\n",
    "            'waitTime_replanning': np.median,\n",
    "            'distance_travelling': np.median,\n",
    "            'distance_walking': np.median,\n",
    "            'distance_bike': np.median,\n",
    "            'distance_ridehail': np.median,\n",
    "            'distance_privateCar': np.median,\n",
    "            'distance_transit': np.median,\n",
    "            'distance_mode_choice': np.median,\n",
    "            'replanning_status': np.median,\n",
    "            'fuel_marginal': np.median,\n",
    "            'emission_marginal': np.median,\n",
    "            'cost_BEAM': np.median,\n",
    "            'socialCarbonCost':np.median\n",
    "           }).reset_index() \n",
    "   # Sum summary table\n",
    "    sum_inc_table = pd.pivot_table(df, index=['incRank'], aggfunc={'duration_travelling': np.sum,\n",
    "            'duration_door_to_door': np.sum,\n",
    "            'duration_walking': np.sum,\n",
    "            'duration_in_privateCar': np.sum,\n",
    "            'duration_on_bike': np.sum,\n",
    "            'duration_in_ridehail': np.sum,\n",
    "            'duration_in_transit': np.sum,\n",
    "            'waitTime_no_replanning': np.sum,\n",
    "            'waitTime_replanning': np.sum,\n",
    "            'distance_travelling': np.sum,\n",
    "            'distance_walking': np.sum,\n",
    "            'distance_bike': np.sum,\n",
    "            'distance_ridehail': np.sum,\n",
    "            'distance_privateCar': np.sum,\n",
    "            'distance_transit': np.sum,\n",
    "            'distance_mode_choice': np.sum,\n",
    "            'replanning_status': np.sum,\n",
    "            'fuel_marginal': np.sum,\n",
    "            'emission_marginal': np.sum,\n",
    "            'cost_BEAM': np.sum,\n",
    "            'socialCarbonCost':np.sum\n",
    "           }).reset_index() \n",
    "            \n",
    "    mode_counts = df.groupby(['incRank', 'mode_choice_actual_BEAM'])['mode_choice_actual_BEAM'].count().unstack().add_prefix('mode_').reset_index()\n",
    "    sum_inc_table = sum_inc_table.merge(mode_counts, on='incRank').assign(mode_ridehail_total = lambda x: x['mode_ride_hail'] + x['mode_ride_hail_pooled'])\n",
    "    sum_inc_table.columns = [col + '_sum' for col in sum_inc_table.columns]\n",
    "    sum_inc_table = sum_inc_table.rename(columns={'incRank_sum': 'incRank'})\n",
    "    mm_inc_table = pd.merge(mean_inc_table, median_inc_table, on='incRank', suffixes=('_mean', '_median'))\n",
    "    summary_inc_table = pd.merge(mm_inc_table, sum_inc_table, on='incRank', how='left')\n",
    "    summary_inc_table = summary_inc_table.rename(columns={'tripIndex': 'n_trips', \n",
    "                                                      'IDMerged': 'n_agents', \n",
    "                                                      'logsum_trip_Potential_INEXUS_median': 'Potential_INEXUS_median',\n",
    "                                                      'logsum_trip_Potential_INEXUS_mean': 'Potential_INEXUS_mean',\n",
    "                                                      'mode_ride_hail_sum': 'mode_ride_hail_solo_sum'})        \n",
    "    \n",
    "    summary_inc_table = summary_inc_table.iloc[:, :2].join(summary_inc_table.iloc[:, 2:].sort_index(axis=1)) \n",
    "    \n",
    "    # shift column 'person' to first position\n",
    "    third_column = summary_inc_table.pop('n_trips')\n",
    "    \n",
    "    # insert column using insert(position,column_name,first_column) function\n",
    "    summary_inc_table.insert(2, 'n_trips', third_column)\n",
    "    \n",
    "    # calculate median and mean wait times by mode and incRank\n",
    "    grouped_waitTime_no_replanning = df.groupby(['mode_choice_actual_BEAM', 'incRank'])['waitTime_no_replanning'].agg(['mean', 'median']).reset_index()\n",
    "    \n",
    "    # pivot the wait time statistics by incRank and mode\n",
    "    waittime_pivot = pd.pivot_table(grouped_waitTime_no_replanning, index=['incRank'], columns=['mode_choice_actual_BEAM'],\n",
    "    values=['median', 'mean'], aggfunc=np.sum)\n",
    "    \n",
    "    # flatten the multi-index column names and rename them with appropriate suffixes\n",
    "    waittime_pivot.columns = [f\"waitTime_no_replanning_{mode}_{agg}\" for (agg, mode) in waittime_pivot.columns.to_flat_index()]\n",
    "    \n",
    "    # merge the wait time statistics pivot table with the original pivot table\n",
    "    summary_inc_table = summary_inc_table.merge(waittime_pivot, on='incRank', how='left')\n",
    "    \n",
    "    # calculate median and mean wait times by mode and incRank\n",
    "    grouped_waitTime_replanning = df.groupby(['mode_choice_planned_BEAM', 'incRank'])['waitTime_replanning'].agg(['mean', 'median']).reset_index()\n",
    "    \n",
    "    # pivot the wait time statistics by incRank and mode\n",
    "    waittime_pivot = pd.pivot_table(grouped_waitTime_replanning, index=['incRank'], columns=['mode_choice_planned_BEAM'], values=['median', 'mean'], aggfunc=np.sum)\n",
    "    \n",
    "    # flatten the multi-index column names and rename them with appropriate suffixes\n",
    "    waittime_pivot.columns = [f\"waitTime_replanning_{mode}_{agg}\" for (agg, mode) in waittime_pivot.columns.to_flat_index()]\n",
    "    \n",
    "    # merge the wait time statistics pivot table with the original pivot table\n",
    "    summary_inc_table = summary_inc_table.merge(waittime_pivot, on='incRank', how='left')\n",
    "    return summary_inc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "537fef6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1h 16min 30s\n",
      "Wall time: 1h 16min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfs_summary = []\n",
    "for df in dfs:\n",
    "    df_summary = generate_summary_table(df)\n",
    "    dfs_summary.append(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d29ffc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 3.91 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sf_stacked = pd.concat(dfs_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84362529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the file locally\n",
    "sf_stacked.to_csv('C:/Users/nrezaei/Documents/sf_2018_stacked_rh_fltsz_price_incRank_26_.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4491952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 344 ms\n",
      "Wall time: 741 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Saving the file to Google Cloud\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "from google.cloud import storage\n",
    "\n",
    "# Export DataFrame to a gzip-compressed CSV file\n",
    "compressed_bytes = BytesIO()\n",
    "with gzip.GzipFile(fileobj=compressed_bytes, mode='w') as gz:\n",
    "    sf_stacked.to_csv(gz, index=False)\n",
    "compressed_bytes.seek(0)\n",
    "\n",
    "# Upload the gzip-compressed file to Google Cloud Storage\n",
    "storage_client = storage.Client.from_service_account_json('C:/Users/nrezaei/Documents/beam-core-a9ea929e82b1.json')\n",
    "bucket_name = 'beam-core-analysis'  # Replace with your Google Cloud Storage bucket name\n",
    "destination_blob_name = 'sf_2018_stacked_rh_fltsz_price_incRank_20230629.csv.gz'  # Replace with the desired destination file name in the bucket\n",
    "\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blob = bucket.blob(destination_blob_name)\n",
    "blob.upload_from_file(compressed_bytes, content_type='application/gzip')\n",
    "\n",
    "# Set the Content-Encoding metadata property\n",
    "blob.metadata = {'Content-Encoding': 'gzip'}\n",
    "blob.patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de7a8b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 77.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stacked_rh_path = \"https://beam-core-act.s3.amazonaws.com/deepDive/CleanData/SanFrancisco/Stacked/\"\n",
    "sf_stacked_12 = pd.read_csv(stacked_rh_path + 'sf_2018_stacked_rh_fltsz_price_incRank_1_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3333244",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [sf_stacked_12, sf_stacked]\n",
    "sf_stacked123 = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4c16cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_stacked123.to_csv('s3://beam-core-act/deepDive/CleanData/SanFrancisco/Stacked/sf_2018_stacked_rh_fltsz_price_incRank_1_2_3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb40665e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\", color_codes=True)\n",
    "sns.set(font_scale=1.35, style=\"ticks\") #set styling preferences\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import math\n",
    "from math import pi\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.cluster.vq import kmeans2,vq, whiten\n",
    "import geopandas as gpd\n",
    "import h5py\n",
    "import boto.s3\n",
    "import glob\n",
    "import boto3\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c07b419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('C:\\Shared-Work\\Data\\Ridehail_Fleetsize_Price_Scenarios.csv')\n",
    "# Get the paths from the DataFrame\n",
    "paths = df['Outputs'][11:17].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff8e6ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_2fleets_47price_100fleet_30pct_20230226/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_47price_100fleet_30pct_20230226/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_47price_164fleet_30pct_20230226/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_47price_200fleet_30pct_20230226/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_47price_400fleet_30pct_20230226/',\n",
       " 'https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#pilates-outputs/sfbay_5fleets_47price_1000fleet_30pct_20230226/']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2549553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the S3 paths in the dataframe\n",
    "prefixs = []\n",
    "for path in paths:\n",
    "    prefix = path.replace('https://s3.us-east-2.amazonaws.com/beam-outputs/index.html#', '')\n",
    "    prefix = prefix + 'inexus/'\n",
    "    prefixs.append(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a2f80f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pilates-outputs/sfbay_2fleets_47price_100fleet_30pct_20230226/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_47price_100fleet_30pct_20230226/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_47price_164fleet_30pct_20230226/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_47price_200fleet_30pct_20230226/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_47price_400fleet_30pct_20230226/inexus/',\n",
       " 'pilates-outputs/sfbay_5fleets_47price_1000fleet_30pct_20230226/inexus/']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "652a8a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 141 ms\n",
      "Wall time: 508 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "bucket_name = 'beam-outputs'\n",
    "\n",
    "# read all files into a list of dataframes\n",
    "key_list = []\n",
    "\n",
    "# navigate the folder and read the CSV files\n",
    "for prefix in prefixs:\n",
    "    # Use the S3 client to list all objects in the bucket and prefix\n",
    "    objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    # extract the keys of the CSV files from the object list\n",
    "    keys = [obj[\"Key\"] for obj in objects[\"Contents\"] if '_2019_' in obj['Key'] and obj[\"Key\"].endswith(\".csv.gz\")]\n",
    "    keys = ''.join(keys)\n",
    "    key_list.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a382430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pilates-outputs/sfbay_2fleets_47price_100fleet_30pct_20230226/inexus/sfbay_5_fleets_scenario_fleet_size-1000_2019__20230226.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_47price_100fleet_30pct_20230226/inexus/sfbay_rh_mixedprice5_rh_fltsz-100_2019__20230226.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_47price_164fleet_30pct_20230226/inexus/sfbay_5_fleets_scenario_fleet_size-1000_2019__20230226.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_47price_200fleet_30pct_20230226/inexus/sfbay_5_fleets_scenario_fleet_size-200_2019__20230225.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_47price_400fleet_30pct_20230226/inexus/sfbay_5_fleets_scenario_fleet_size-400_2019__20230225.csv.gz',\n",
       " 'pilates-outputs/sfbay_5fleets_47price_1000fleet_30pct_20230226/inexus/sfbay_5_fleets_scenario_fleet_size-1000_2019__20230226.csv.gz']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59919cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_use = ['IDMerged', 'tripIndex', 'actStartTime', 'actEndTime','duration_travelling', 'cost_BEAM', 'actStartType', \n",
    "               'actEndType', 'duration_walking', 'duration_in_privateCar', 'duration_on_bike', 'duration_in_ridehail', \n",
    "              'distance_travelling', 'duration_in_transit', 'distance_walking','distance_bike','distance_ridehail', \n",
    "              'distance_privateCar', 'distance_transit', 'mode_choice_planned_BEAM','mode_choice_actual_BEAM',\n",
    "              'vehicleIds', 'distance_mode_choice', 'replanning_status', 'reason', 'fuel_marginal','BlockGroupStart',\n",
    "              'startX', 'startY', 'bgid_start', 'tractid_start', 'juris_name_start', 'county_name_start', 'mpo_start', \n",
    "               'BlockGroupEnd', 'endX', 'endY', 'bgid_end', 'tractid_end', 'juris_name_end', 'county_name_end', 'mpo_end', \n",
    "               'emission_marginal', 'duration_door_to_door', 'waitTime_no_replanning', 'waitTime_replanning', 'actPurpose', \n",
    "               'mode_choice_actual_6', 'mode_choice_actual_5', 'mode_choice_actual_4', 'trip_mode_AS_trips', 'logsum_trip_Potential_INEXUS',\n",
    "               'age', 'income', 'hh_cars', 'TAZ_x', 'origin_x', 'destination_x', 'TAZ_y', 'home_taz', 'auto_ownership', 'home_is_urban', 'home_is_rural', 'DRIVEALONEFREE',\n",
    "               'DRIVEALONEPAY', 'SHARED2FREE', 'SHARED2PAY', 'SHARED3FREE', 'SHARED3PAY', 'WALK', 'BIKE', 'WALK_LOC', 'WALK_LRF', \n",
    "               'WALK_EXP', 'WALK_HVY', 'WALK_COM', 'DRIVE_LOC', 'DRIVE_LRF', 'DRIVE_EXP', 'DRIVE_HVY', 'DRIVE_COM', 'TAXI',\n",
    "               'TNC_SINGLE', 'TNC_SHARED', 'income_quartiles', 'income_deciles' ] # Specify the columns to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3d2ee41",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# define the filenames for each row in the DataFrame\n",
    "filenames = []\n",
    "\n",
    "for key in key_list:\n",
    "    # split the key based on the `/` separator and take the last element\n",
    "    filename = key.split('/')[-1].rstrip('.csv.gz')\n",
    "    #filename = filename.split(\"_2019\")[0]  # remove anything after the year 2019\n",
    "    filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fca42f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sfbay_5_fleets_scenario_fleet_size-1000_2019__20230226',\n",
       " 'sfbay_rh_mixedprice5_rh_fltsz-100_2019__20230226',\n",
       " 'sfbay_5_fleets_scenario_fleet_size-1000_2019__20230226',\n",
       " 'sfbay_5_fleets_scenario_fleet_size-200_2019__20230225',\n",
       " 'sfbay_5_fleets_scenario_fleet_size-400_2019__20230225',\n",
       " 'sfbay_5_fleets_scenario_fleet_size-1000_2019__20230226']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb18208",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_values = {}\n",
    "    \n",
    "for i, row in df[23:].reset_index(drop=True).iterrows():\n",
    "    # your code here.iterrows():\n",
    "    # extract the custom values for the current row\n",
    "    custom_value_1 = row['lever_position_price']\n",
    "    custom_value_2 = row['lever_position_fltsz']\n",
    "    custom_value_3 = row['lever_n_fleets']\n",
    "    custom_value_4 = row['fleetsize_uber']\n",
    "    custom_value_5 = row['fleetsize_lyft']\n",
    "    custom_value_6 = row['fleetsize_cruise']\n",
    "    custom_value_7 = row['fleetsize_flywheel']\n",
    "    custom_value_8 = row['fleetsize_waymo']\n",
    "    # convert the \"lever_position_price\" value from percent to decimal\n",
    "    custom_value_1_decimal = float(custom_value_1.strip('%')) / 100\n",
    "    custom_value_2_decimal = float(custom_value_2.strip('%')) / 100\n",
    "    \n",
    "    # get the filename for the current row\n",
    "    filename = filenames[i]  \n",
    "    \n",
    "    # check if the filename already exists in the dictionary\n",
    "    if filename not in custom_values:\n",
    "        # create a new dictionary entry for the filename if it doesn't exist\n",
    "        custom_values[filename] = []\n",
    "        \n",
    "    custom_value_dict = {'lever_position_price': custom_value_1_decimal, 'lever_position_fltsz': custom_value_2_decimal, \n",
    "                         'lever_n_fleets': custom_value_3, 'fleetsize_uber': custom_value_4, 'fleetsize_lyft': custom_value_5,\n",
    "                         'fleetsize_cruise': custom_value_6,'fleetsize_flywheel': custom_value_7,'fleetsize_waymo': custom_value_8}\n",
    "    \n",
    "    # append the custom values to the existing list for the filename\n",
    "    custom_values[filename].append(custom_value_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa94ab29",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (27,234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (27,234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (27,234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (27,234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (27,234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:5: DtypeWarning: Columns (234,235) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2h 5min 18s\n",
      "Wall time: 2h 5min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfs = []\n",
    "\n",
    "for key in key_list:\n",
    "    obj = s3.get_object(Bucket=\"beam-outputs\", Key=key)\n",
    "    sf_files = pd.read_csv(obj['Body'], compression = 'gzip', usecols = cols_to_use)\n",
    "    \n",
    "    filename = key.split('/')[-1].rstrip('.csv.gz')\n",
    "    #filename = filename.split(\"_2019\")[0]  # remove anything after the year 2019\n",
    "    \n",
    "    for custom_value_dict in custom_values[filename]:\n",
    "        sf_files = sf_files.assign(**custom_value_dict)\n",
    "    sf_files['year'] = 2018\n",
    "    #append the dataframe to the list of dataframes\n",
    "    dfs.append(sf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4f555b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all columns and rows\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0494b26f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def generate_summary_table(df):\n",
    "    df['socialCarbonCost'] = df['emission_marginal']*185\n",
    "    df['incomeInThousands'] = df['income']/1000\n",
    "    df = df[df['incomeInThousands'].notna()]\n",
    "    person_income = pd.pivot_table(df, index=['IDMerged'], aggfunc={'incomeInThousands': lambda x: ', '.join(set(x.dropna().astype(str)))}).reset_index() \n",
    "    person_income['incomeInThousands'] = person_income['incomeInThousands'].astype(float)\n",
    "   \n",
    "    # Add a column of income ranks\n",
    "    twenty_one_ranks = person_income['incomeInThousands'].quantile([0, 0.048, 0.095, 0.143, 0.191, 0.239, 0.287, 0.335, 0.383, 0.431, 0.479,\n",
    "                                                                0.527, 0.575, 0.623, 0.671, 0.719, 0.767, 0.815, 0.863, 0.911, 0.959,\n",
    "                                                                1]).tolist()\n",
    "    \n",
    "    # Add incomeInThousands twenty_one_ranks\n",
    "    conditions  = [(person_income['incomeInThousands'] >= twenty_one_ranks[0]) & (person_income['incomeInThousands'] < twenty_one_ranks[1]), \n",
    "               (person_income['incomeInThousands'] >= twenty_one_ranks[1]) & (person_income['incomeInThousands'] < twenty_one_ranks[2]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[2]) & (person_income['incomeInThousands'] < twenty_one_ranks[3]),\n",
    "               (person_income['incomeInThousands'] >= twenty_one_ranks[3]) & (person_income['incomeInThousands'] < twenty_one_ranks[4]), \n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[4]) & (person_income['incomeInThousands'] < twenty_one_ranks[5]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[5]) & (person_income['incomeInThousands'] < twenty_one_ranks[6]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[6]) & (person_income['incomeInThousands'] < twenty_one_ranks[7]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[7]) & (person_income['incomeInThousands'] < twenty_one_ranks[8]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[8]) & (person_income['incomeInThousands'] < twenty_one_ranks[9]),\n",
    "               (person_income['incomeInThousands'] >=  twenty_one_ranks[9]) & (person_income['incomeInThousands'] <= twenty_one_ranks[10]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[10]) & (person_income['incomeInThousands'] <= twenty_one_ranks[11]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[11]) & (person_income['incomeInThousands'] <= twenty_one_ranks[12]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[12]) & (person_income['incomeInThousands'] <= twenty_one_ranks[13]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[13]) & (person_income['incomeInThousands'] <= twenty_one_ranks[14]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[14]) & (person_income['incomeInThousands'] <= twenty_one_ranks[15]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[15]) & (person_income['incomeInThousands'] <= twenty_one_ranks[16]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[16]) & (person_income['incomeInThousands'] <= twenty_one_ranks[17]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[17]) & (person_income['incomeInThousands'] <= twenty_one_ranks[18]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[18]) & (person_income['incomeInThousands'] <= twenty_one_ranks[19]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[19]) & (person_income['incomeInThousands'] <= twenty_one_ranks[20]),\n",
    "              (person_income['incomeInThousands'] >=  twenty_one_ranks[20]) & (person_income['incomeInThousands'] <= twenty_one_ranks[21])]\n",
    "    choices = [0, 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "    \n",
    "    person_income['incRank'] = np.select(conditions, choices, default=None)\n",
    "    \n",
    "    df = pd.merge(left = df, right = person_income, how='left', on = ['IDMerged'], suffixes=('', '_drop'))\n",
    "    \n",
    "    df.drop([col for col in df.columns if 'drop' in col], axis=1, inplace=True)\n",
    "    \n",
    "    # Mean pivot table\n",
    "    mean_inc_table = pd.pivot_table(df,index=['incRank'], aggfunc={'incomeInThousands': np.mean,\n",
    "           'logsum_trip_Potential_INEXUS': np.mean,\n",
    "            'duration_travelling': np.mean,\n",
    "            'duration_door_to_door': np.mean,\n",
    "            'duration_walking': np.mean,\n",
    "            'duration_in_privateCar': np.mean,\n",
    "            'duration_on_bike': np.mean,\n",
    "            'duration_in_ridehail': np.mean,\n",
    "            'duration_in_transit': np.mean,\n",
    "            'waitTime_no_replanning': np.mean,\n",
    "            'waitTime_replanning': np.mean,\n",
    "            'distance_travelling': np.mean,\n",
    "            'distance_walking': np.mean,\n",
    "            'distance_bike': np.mean,\n",
    "            'distance_ridehail': np.mean,\n",
    "            'distance_privateCar': np.mean,\n",
    "            'distance_transit': np.mean,\n",
    "            'distance_mode_choice': np.mean,\n",
    "            'replanning_status': np.mean,\n",
    "            'fuel_marginal': np.mean,\n",
    "            'emission_marginal': np.mean,\n",
    "            'cost_BEAM': np.mean,\n",
    "            'socialCarbonCost':np.mean,\n",
    "           'lever_position_price':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "           'lever_position_fltsz':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "           'lever_n_fleets':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "            'fleetsize_uber':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "            'fleetsize_lyft':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "            'fleetsize_cruise':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "            'fleetsize_flywheel':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "            'fleetsize_waymo':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "           'IDMerged': 'nunique',\n",
    "           'tripIndex': 'nunique'}).reset_index()\n",
    "    # Median summary table\n",
    "    median_inc_table = pd.pivot_table(df, index=['incRank'], aggfunc={'incomeInThousands': np.median,\n",
    "           'logsum_trip_Potential_INEXUS': np.median,\n",
    "            'duration_travelling': np.median,\n",
    "            'duration_door_to_door': np.median,\n",
    "            'duration_walking':np.median,\n",
    "            'duration_in_privateCar': np.median,\n",
    "            'duration_on_bike': np.median,\n",
    "            'duration_in_ridehail': np.median,\n",
    "            'duration_in_transit': np.median,\n",
    "            'waitTime_no_replanning': np.median,\n",
    "            'waitTime_replanning': np.median,\n",
    "            'distance_travelling': np.median,\n",
    "            'distance_walking': np.median,\n",
    "            'distance_bike': np.median,\n",
    "            'distance_ridehail': np.median,\n",
    "            'distance_privateCar': np.median,\n",
    "            'distance_transit': np.median,\n",
    "            'distance_mode_choice': np.median,\n",
    "            'replanning_status': np.median,\n",
    "            'fuel_marginal': np.median,\n",
    "            'emission_marginal': np.median,\n",
    "            'cost_BEAM': np.median,\n",
    "            'socialCarbonCost':np.median\n",
    "           }).reset_index() \n",
    "   # Sum summary table\n",
    "    sum_inc_table = pd.pivot_table(df, index=['incRank'], aggfunc={'duration_travelling': np.sum,\n",
    "            'duration_door_to_door': np.sum,\n",
    "            'duration_walking': np.sum,\n",
    "            'duration_in_privateCar': np.sum,\n",
    "            'duration_on_bike': np.sum,\n",
    "            'duration_in_ridehail': np.sum,\n",
    "            'duration_in_transit': np.sum,\n",
    "            'waitTime_no_replanning': np.sum,\n",
    "            'waitTime_replanning': np.sum,\n",
    "            'distance_travelling': np.sum,\n",
    "            'distance_walking': np.sum,\n",
    "            'distance_bike': np.sum,\n",
    "            'distance_ridehail': np.sum,\n",
    "            'distance_privateCar': np.sum,\n",
    "            'distance_transit': np.sum,\n",
    "            'distance_mode_choice': np.sum,\n",
    "            'replanning_status': np.sum,\n",
    "            'fuel_marginal': np.sum,\n",
    "            'emission_marginal': np.sum,\n",
    "            'cost_BEAM': np.sum,\n",
    "            'socialCarbonCost':np.sum\n",
    "           }).reset_index() \n",
    "            \n",
    "    mode_counts = df.groupby(['incRank', 'mode_choice_actual_BEAM'])['mode_choice_actual_BEAM'].count().unstack().add_prefix('mode_').reset_index()\n",
    "    sum_inc_table = sum_inc_table.merge(mode_counts, on='incRank').assign(mode_ridehail_total = lambda x: x['mode_ride_hail'] + x['mode_ride_hail_pooled'])\n",
    "    sum_inc_table.columns = [col + '_sum' for col in sum_inc_table.columns]\n",
    "    sum_inc_table = sum_inc_table.rename(columns={'incRank_sum': 'incRank'})\n",
    "    mm_inc_table = pd.merge(mean_inc_table, median_inc_table, on='incRank', suffixes=('_mean', '_median'))\n",
    "    summary_inc_table = pd.merge(mm_inc_table, sum_inc_table, on='incRank', how='left')\n",
    "    summary_inc_table = summary_inc_table.rename(columns={'tripIndex': 'n_trips', \n",
    "                                                      'IDMerged': 'n_agents', \n",
    "                                                      'logsum_trip_Potential_INEXUS_median': 'Potential_INEXUS_median',\n",
    "                                                      'logsum_trip_Potential_INEXUS_mean': 'Potential_INEXUS_mean',\n",
    "                                                      'mode_ride_hail_sum': 'mode_ride_hail_solo_sum'})        \n",
    "    \n",
    "    summary_inc_table = summary_inc_table.iloc[:, :2].join(summary_inc_table.iloc[:, 2:].sort_index(axis=1)) \n",
    "    \n",
    "    # shift column 'person' to first position\n",
    "    third_column = summary_inc_table.pop('n_trips')\n",
    "    \n",
    "    # insert column using insert(position,column_name,first_column) function\n",
    "    summary_inc_table.insert(2, 'n_trips', third_column)\n",
    "    \n",
    "    # calculate median and mean wait times by mode and incRank\n",
    "    grouped_waitTime_no_replanning = df.groupby(['mode_choice_actual_BEAM', 'incRank'])['waitTime_no_replanning'].agg(['mean', 'median']).reset_index()\n",
    "    \n",
    "    # pivot the wait time statistics by incRank and mode\n",
    "    waittime_pivot = pd.pivot_table(grouped_waitTime_no_replanning, index=['incRank'], columns=['mode_choice_actual_BEAM'],\n",
    "    values=['median', 'mean'], aggfunc=np.sum)\n",
    "    \n",
    "    # flatten the multi-index column names and rename them with appropriate suffixes\n",
    "    waittime_pivot.columns = [f\"waitTime_no_replanning_{mode}_{agg}\" for (agg, mode) in waittime_pivot.columns.to_flat_index()]\n",
    "    \n",
    "    # merge the wait time statistics pivot table with the original pivot table\n",
    "    summary_inc_table = summary_inc_table.merge(waittime_pivot, on='incRank', how='left')\n",
    "    \n",
    "    # calculate median and mean wait times by mode and incRank\n",
    "    grouped_waitTime_replanning = df.groupby(['mode_choice_planned_BEAM', 'incRank'])['waitTime_replanning'].agg(['mean', 'median']).reset_index()\n",
    "    \n",
    "    # pivot the wait time statistics by incRank and mode\n",
    "    waittime_pivot = pd.pivot_table(grouped_waitTime_replanning, index=['incRank'], columns=['mode_choice_planned_BEAM'], values=['median', 'mean'], aggfunc=np.sum)\n",
    "    \n",
    "    # flatten the multi-index column names and rename them with appropriate suffixes\n",
    "    waittime_pivot.columns = [f\"waitTime_replanning_{mode}_{agg}\" for (agg, mode) in waittime_pivot.columns.to_flat_index()]\n",
    "    \n",
    "    # merge the wait time statistics pivot table with the original pivot table\n",
    "    summary_inc_table = summary_inc_table.merge(waittime_pivot, on='incRank', how='left')\n",
    "    return summary_inc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "537fef6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1h 28min 32s\n",
      "Wall time: 1h 28min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfs_summary = []\n",
    "for df in dfs:\n",
    "    df_summary = generate_summary_table(df)\n",
    "    dfs_summary.append(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d29ffc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 5.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sf_stacked = pd.concat(dfs_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e76e4c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_stacked.to_csv('s3://beam-core-act/deepDive/CleanData/SanFrancisco/Stacked/sf_2018_stacked_rh_fltsz_price_incRank_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de7a8b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 77.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stacked_rh_path = \"https://beam-core-act.s3.amazonaws.com/deepDive/CleanData/SanFrancisco/Stacked/\"\n",
    "sf_stacked_12 = pd.read_csv(stacked_rh_path + 'sf_2018_stacked_rh_fltsz_price_incRank_1_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3333244",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [sf_stacked_12, sf_stacked]\n",
    "sf_stacked123 = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4c16cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_stacked123.to_csv('s3://beam-core-act/deepDive/CleanData/SanFrancisco/Stacked/sf_2018_stacked_rh_fltsz_price_incRank_1_2_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55411408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

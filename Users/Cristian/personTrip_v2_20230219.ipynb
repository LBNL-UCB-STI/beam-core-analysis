{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ae3a30-fe26-4180-8a5c-f7fc85c9cbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_722606/2772208371.py:4: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.26.76)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.76 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.29.80)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.76->boto3) (1.26.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.76->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.76->boto3) (1.16.0)\n",
      "Requirement already satisfied: patool in /opt/conda/lib/python3.10/site-packages (1.12)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import h5py\n",
    "! pip install boto3\n",
    "! pip install patool\n",
    "import boto.s3\n",
    "import glob\n",
    "import boto3\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import patoolib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f23ff722-db6b-4ab8-a990-8739d48d951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addGeometryIdToDataFrame(df, gdf, xcol, ycol, idColumn=\"geometry\", df_geom='epsg:4326'):\n",
    "    gdf_data = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df[xcol], df[ycol]))\n",
    "    gdf_data.crs = {'init': df_geom}\n",
    "    joined = gpd.sjoin(gdf_data.to_crs('epsg:26910'), gdf.to_crs('epsg:26910'))\n",
    "    gdf_data = gdf_data.merge(joined['blkgrpid'], left_index=True, right_index=True, how=\"left\")\n",
    "    gdf_data.rename(columns={'blkgrpid': idColumn}, inplace=True)\n",
    "    df = pd.DataFrame(gdf_data.drop(columns='geometry'))\n",
    "    #df.drop(columns=[xcol, ycol], inplace=True)\n",
    "    return df.loc[~df.index.duplicated(keep='first'), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb9faa-7404-4888-886c-2d8c04c74d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_id pilates-outputs/sfbay-rh_wt_1.6-20220617/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\n",
      "step  2.1457672119140625e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_722606/3261198214.py:57: DtypeWarning: Columns (7,12,13,35,40,41,44,51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes, chunksize=2500000):\n",
      "/tmp/ipykernel_722606/3261198214.py:57: DtypeWarning: Columns (7,12,13,44,51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes, chunksize=2500000):\n",
      "/tmp/ipykernel_722606/3261198214.py:57: DtypeWarning: Columns (7,12,13,44,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes, chunksize=2500000):\n",
      "/tmp/ipykernel_722606/3261198214.py:57: DtypeWarning: Columns (44,51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes, chunksize=2500000):\n",
      "/tmp/ipykernel_722606/3261198214.py:57: DtypeWarning: Columns (44,51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes, chunksize=2500000):\n",
      "/tmp/ipykernel_722606/3261198214.py:57: DtypeWarning: Columns (7,12,13,44,51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes, chunksize=2500000):\n",
      "/tmp/ipykernel_722606/3261198214.py:57: DtypeWarning: Columns (7,12,13,44,51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes, chunksize=2500000):\n",
      "/tmp/ipykernel_722606/3261198214.py:57: DtypeWarning: Columns (7,12,13,44,51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes, chunksize=2500000):\n",
      "/tmp/ipykernel_722606/3261198214.py:57: DtypeWarning: Columns (7,12,13,44,51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes, chunksize=2500000):\n",
      "/tmp/ipykernel_722606/3261198214.py:57: DtypeWarning: Columns (7,12,13,44,51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes, chunksize=2500000):\n",
      "/tmp/ipykernel_722606/3261198214.py:57: DtypeWarning: Columns (7,12,13,44,51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes, chunksize=2500000):\n",
      "/tmp/ipykernel_722606/3261198214.py:57: DtypeWarning: Columns (7,12,13,44,51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes, chunksize=2500000):\n",
      "/tmp/ipykernel_722606/3261198214.py:57: DtypeWarning: Columns (7,12,13,44,51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes, chunksize=2500000):\n",
      "/tmp/ipykernel_722606/3261198214.py:57: DtypeWarning: Columns (44,51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes, chunksize=2500000):\n",
      "/tmp/ipykernel_722606/3261198214.py:57: DtypeWarning: Columns (44,51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes, chunksize=2500000):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  338.7417781352997\n"
     ]
    }
   ],
   "source": [
    "keys = [\n",
    "    # \"pilates-outputs/sfbay-rh_radius_0.2-20220610/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\",\n",
    "    #     \"pilates-outputs/sfbay-rh_radius_0.5-20220610/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\",\n",
    "        # \"pilates-outputs/sfbay-rh_radius_1.5-20220614/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\",\n",
    "        # \"pilates-outputs/sfbay-rh_radius_5.0-20220616/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\",\n",
    "        # \"pilates-outputs/sfbay-rh_repo_0.0-20220613/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\",\n",
    "        # \"pilates-outputs/sfbay-rh_repo_0.5-20220613/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\",\n",
    "        # \"pilates-outputs/sfbay-rh_repo_1.5-20220613/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\", \n",
    "        # \"pilates-outputs/sfbay-rh_repo_3.0-20220613/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\", \n",
    "        # \"pilates-outputs/sfbay-rh_wt_0.2-20220617/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\", \n",
    "        # \"pilates-outputs/sfbay-rh_wt_0.5-20220617/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\", \n",
    "        # \"pilates-outputs/sfbay-rh_wt_0.8-20220617/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\", \n",
    "        \"pilates-outputs/sfbay-rh_wt_1.6-20220617/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\", \n",
    "        \"pilates-outputs/sfbay-rh_detour_0.0-20220618/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\", \n",
    "        \"pilates-outputs/sfbay-rh_detour_0.5-20220618/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\", \n",
    "        \"pilates-outputs/sfbay-rh_detour_0.75-20220618/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\", \n",
    "        \"pilates-outputs/sfbay-rh_detour_1.5-20220618/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\",\n",
    "        # \"pilates-outputs/sfbay-rh_invrepo_dem_0.0_dist_0.0-20220627/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\", \n",
    "        # \"pilates-outputs/sfbay-rh_invrepo_dem_0.2_dist_0.9-20220627/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\", \n",
    "        # \"pilates-outputs/sfbay-rh_invrepo_dem_0.6_dist_0.9-20220627/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\", \n",
    "        # \"pilates-outputs/sfbay-rh_invrepo_dem_1.0_dist_0.9-20220627/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\", \n",
    "        # \"pilates-outputs/sfbay-rh_invrepo_dem_0.4_dist_0.3-20220628/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\", \n",
    "        # \"pilates-outputs/sfbay-rh_invrepo_dem_0.4_dist_0.6-20220628/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\", \n",
    "        # \"pilates-outputs/sfbay-rh_invrepo_dem_0.4_dist_1.0-20220628/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\", \n",
    "        # \"pilates-outputs/sfbay-rh_invrepo_dem_1.0_dist_1.0-20220628/beam/year-2018-iteration-5/ITERS/it.0/0.events.csv.gz\",\n",
    "    ] \n",
    "         \n",
    "for key_id in keys:\n",
    "    print('key_id',key_id)\n",
    "    a = time.time()\n",
    "    print('step ', time.time()-a)\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    obj = s3.get_object(Bucket=\"beam-outputs\", Key=key_id)\n",
    "    dtypes = {\n",
    "        \"time\": \"float32\",\n",
    "        \"type\": \"category\",\n",
    "        \"legMode\": \"category\",\n",
    "        \"actType\": \"category\", \n",
    "        \"primaryFuelLevel\": \"float64\",\n",
    "        \"legMode\": \"category\",\n",
    "        \"chargingPointType\":\"category\",\n",
    "        \"pricingModel\":\"category\",\n",
    "        \"parkingType\":\"category\",\n",
    "        \"mode\":\"category\",\n",
    "        \"personalVehicleAvailable\": \"category\",\n",
    "        \"person\": \"object\",\n",
    "        \"driver\": \"object\",\n",
    "        \"riders\": \"object\",\n",
    "        'primaryFuelType': \"category\",\n",
    "        'secondaryFuelType': 'category',\n",
    "        'currentTourMode': 'category',\n",
    "        'currentActivity': 'category',\n",
    "        'nextActivity': 'category'    \n",
    "    }\n",
    "    \n",
    "    eventsSF =[]\n",
    "    for chunk in pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes, chunksize=2500000):\n",
    "            eventsSF.append(chunk)\n",
    "    eventsSF = pd.concat(eventsSF)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # eventsSF = pd.read_csv(obj['Body'], compression = 'gzip', dtype = dtypes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Show all columns and rows\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.options.display.max_rows = None\n",
    "    # Showing the entire number in dataframe\n",
    "    pd.set_option('float_format', '{:f}'.format)\n",
    "    # Adding scenario info\n",
    "    eventsSF['scenario'] = \"baseline\"\n",
    "    eventsSF['scenario'] = eventsSF['scenario'].astype(\"category\")\n",
    "    eventsSF['lever'] = \"default\"\n",
    "    eventsSF['lever'] = eventsSF['lever'].astype(\"category\")\n",
    "    eventsSF['year'] = 2018\n",
    "    eventsSF['lever_position'] = 1\n",
    "    # Rename the \"mode\" column\n",
    "    eventsSF.rename(columns={\"mode\":\"modeBEAM\"}, inplace=True) \n",
    "    # Replace \"Work\" with \"work\" in the \"actType\" column\n",
    "    eventsSF[\"actType\"].replace({\"Work\": \"work\"}, inplace=True)\n",
    "    # Remove person = TransitDriver or RidehailDriver because there are no agent information in these rows\n",
    "    eventsSF = eventsSF[~eventsSF.person.str.contains(\"Agent\", na=False)].reset_index(drop=True)\n",
    "    # Remove the cases where parking events have time = 0 which are additional parking events that were thrown at the beginning \n",
    "    # of the day when vehicles are first assigned to a parking spot \n",
    "    eventsSF = eventsSF[~((eventsSF.type.str.contains(\"ParkingEvent\", na=False))&(eventsSF['time']==0))].reset_index(drop=True)\n",
    "    # shift column 'person' to first position\n",
    "    first_column = eventsSF.pop('person')\n",
    "    second_column = eventsSF.pop('driver')\n",
    "    third_column = eventsSF.pop('riders')\n",
    "    # insert column using insert(position,column_name,first_column) function\n",
    "    eventsSF.insert(0, 'person', first_column)\n",
    "    eventsSF.insert(1, 'driver', second_column)\n",
    "    eventsSF.insert(2, 'riders', third_column)\n",
    "    # Adding the IDMerged Column\n",
    "    eventsSF['UniqueID'] = eventsSF['person'] #make a copy of the person column\n",
    "    eventsSF['personID'] = np.where(eventsSF['person'].isin(eventsSF['driver']), eventsSF['person'], np.nan) \n",
    "    eventsSF['driverID'] = np.where(eventsSF['driver'].isin(eventsSF['person']), eventsSF['driver'], np.nan)\n",
    "    # Merging person and driver ids in one column\n",
    "    eventsSF['IDMerged'] = eventsSF['personID'].combine_first(eventsSF['driverID'])\n",
    "    eventsSF['IDMerged'] = eventsSF['UniqueID'].combine_first(eventsSF['IDMerged'])\n",
    "    # Dropping unused columns\n",
    "    eventsSF = eventsSF.drop(['personID','driverID','UniqueID'], axis=1) \n",
    "    # Shift column 'IDMerged' to first position\n",
    "    first_column = eventsSF.pop('IDMerged')\n",
    "    # Insert column using insert(position,column_name,first_column) function\n",
    "    eventsSF.insert(0, 'IDMerged', first_column)\n",
    "    print('step ', time.time()-a)\n",
    "    # Split the \"riders' column and replicated rows for every rider\n",
    "    eventsSF['riders'] = eventsSF['riders'].str.split(':')\n",
    "    eventsSF = eventsSF.explode('riders')\n",
    "    # Combine riderID with IDMerged\n",
    "    eventsSF['riderID'] = np.where(eventsSF['riders'].isin(eventsSF['person']), eventsSF['riders'], np.nan)\n",
    "    eventsSF['IDMerged'] = eventsSF['riderID'].combine_first(eventsSF['IDMerged'])\n",
    "    # Dropping unused columns\n",
    "    eventsSF = eventsSF.drop(['riderID'], axis=1) \n",
    "    # Remove driver = TransitDriver or RidehailDriver for IDMerged = NAN because there are no agent information in these rows \n",
    "    eventsSF = eventsSF[~((eventsSF.driver.str.contains(\"Agent\", na=False))&(eventsSF.IDMerged.isna()))].reset_index(drop=True)\n",
    "    print('step ', time.time()-a)\n",
    "    # Filling NANs in ID related to charging events\n",
    "    eventsSF[\"chargeID\"] = eventsSF.groupby('vehicle')['IDMerged'].transform(lambda x: x.ffill().bfill())\n",
    "    # Combining chargeID with IDMerged so no NANs anymore\n",
    "    eventsSF['IDMerged'] = eventsSF['chargeID'].combine_first(eventsSF['IDMerged'])\n",
    "    # Dropping unused columns\n",
    "    eventsSF = eventsSF.drop(['chargeID'], axis=1) \n",
    "    print('step ', time.time()-a)\n",
    "    # Change the IDMerged column type to numeric\n",
    "    eventsSF[\"IDMerged\"] = pd.to_numeric(eventsSF.IDMerged)\n",
    "    # Sort by IDMerged and time columns\n",
    "    eventsSF = eventsSF.sort_values(['IDMerged','time']).reset_index(drop=True)\n",
    "    # We assume that the number of passengers is 1 for ride_hail_pooled\n",
    "    eventsSF['modeBEAM_rh'] = np.where(eventsSF.driver.str.contains(\"rideHailAgent\", na=False), 'ride_hail' , eventsSF['modeBEAM'])\n",
    "    # Adding teleportation mode to the type = TeleportationEvent row \n",
    "    eventsSF[\"modeBEAM_rh\"] = np.where(eventsSF['type']=='TeleportationEvent', eventsSF.modeBEAM_rh.fillna(method='ffill'), eventsSF[\"modeBEAM_rh\"])\n",
    "    eventsSF['modeBEAM_rh_pooled'] = np.where((eventsSF['type'] == 'PersonCost') & (eventsSF['modeBEAM'] == 'ride_hail_pooled'), 'ride_hail_pooled', np.nan)\n",
    "    eventsSF['modeBEAM_rh_ride_hail_transit'] = np.where((eventsSF['type'] == 'PersonCost') & (eventsSF['modeBEAM'] == 'ride_hail_transit'), 'ride_hail_transit', np.nan)\n",
    "    eventsSF['modeBEAM_rh_pooled'] = eventsSF['modeBEAM_rh_pooled'].shift(+1)\n",
    "    eventsSF['modeBEAM_rh_ride_hail_transit'] = eventsSF['modeBEAM_rh_ride_hail_transit'].shift(+1)\n",
    "    eventsSF['modeBEAM_rh'] = np.where((eventsSF['type'] == 'PathTraversal') & (eventsSF['modeBEAM'] == 'car') & (eventsSF['driver'].str.contains(\"rideHailAgent\", na=False)) & (eventsSF['modeBEAM_rh_pooled'] != 'nan'), eventsSF['modeBEAM_rh_pooled'], eventsSF['modeBEAM_rh'])\n",
    "    eventsSF['modeBEAM_rh'] = np.where((eventsSF['type'] == 'PathTraversal') & (eventsSF['modeBEAM'] == 'car') & (eventsSF['driver'].str.contains(\"rideHailAgent\", na=False)) & (eventsSF['modeBEAM_rh_ride_hail_transit'] != 'nan'), eventsSF['modeBEAM_rh_ride_hail_transit'], eventsSF['modeBEAM_rh'])\n",
    "    eventsSF = eventsSF.drop(['modeBEAM_rh_pooled'], axis=1)\n",
    "    eventsSF = eventsSF.drop(['modeBEAM_rh_ride_hail_transit'], axis=1)\n",
    "    BGs = gpd.read_file('/vsicurl/https://github.com/LBNL-UCB-STI/beam-core-analysis/raw/main/Users/Zach/scenario/sfbay-blockgroups-2010/641aa0d4-ce5b-4a81-9c30-8790c4ab8cfb202047-1-wkkklf.j5ouj.shp')\n",
    "    print('step ', time.time()-a)\n",
    "    eventsSF = addGeometryIdToDataFrame(eventsSF, BGs, 'startX', 'startY', 'BlockGroupStart')\n",
    "    print('step ', time.time()-a)\n",
    "    eventsSF = addGeometryIdToDataFrame(eventsSF, BGs, 'endX', 'endY', 'BlockGroupEnd')\n",
    "    block_info = pd.read_csv('outputs/bg.csv')\n",
    "    block_info['bgid'] = '0' + block_info['bgid'].astype(str)\n",
    "    eventsSF = pd.merge(eventsSF, block_info,  how='left',  left_on = ['BlockGroupEnd'], right_on = 'bgid',)\n",
    "    eventsSF = pd.merge(eventsSF, block_info,  how='left',  left_on = ['BlockGroupStart'], right_on = 'bgid',)\n",
    "    eventsSF.rename(columns={\"bgid_x\":\"bgid_end\"}, inplace=True) \n",
    "    eventsSF.rename(columns={\"bgid_y\":\"bgid_start\"}, inplace=True) \n",
    "    eventsSF.rename(columns={\"tractid_x\":\"tractid_end\"}, inplace=True)\n",
    "    eventsSF.rename(columns={\"tractid_y\":\"tractid_start\"}, inplace=True) \n",
    "    eventsSF.rename(columns={\"juris_name_x\":\"juris_name_end\"}, inplace=True)\n",
    "    eventsSF.rename(columns={\"juris_name_y\":\"juris_name_start\"}, inplace=True)\n",
    "    eventsSF.rename(columns={\"county_name_x\":\"county_name_end\"}, inplace=True)\n",
    "    eventsSF.rename(columns={\"county_name_y\":\"county_name_start\"}, inplace=True)\n",
    "    eventsSF.rename(columns={\"mpo_x\":\"mpo_end\"}, inplace=True)\n",
    "    eventsSF.rename(columns={\"mpo_y\":\"mpo_start\"}, inplace=True)\n",
    "    eventsSF['actEndTime'] = np.where(eventsSF['type']=='actend'\n",
    "                         , eventsSF['time'], np.nan)\n",
    "    eventsSF['actStartTime'] = np.where(eventsSF['type']=='actstart'\n",
    "                         , eventsSF['time'], np.nan)    \n",
    "    eventsSF['duration_travelling'] = np.where((eventsSF['type']=='PathTraversal')|(eventsSF['type']=='TeleportationEvent')\n",
    "                         , eventsSF['arrivalTime'] - eventsSF['departureTime'], np.nan)\n",
    "    eventsSF['distance_travelling'] = np.where((eventsSF['type']=='PathTraversal')|((eventsSF['type']=='ModeChoice')&((eventsSF['modeBEAM']=='hov2_teleportation')|(eventsSF['modeBEAM']=='hov3_teleportation'))), eventsSF['length'], np.nan)\n",
    "    eventsSF['distance_mode_choice'] = np.where(eventsSF['type']=='ModeChoice', eventsSF['length'], np.nan)\n",
    "    eventsSF['duration_walking'] = np.where(eventsSF['modeBEAM']=='walk', eventsSF['duration_travelling'], np.nan)\n",
    "    eventsSF['distance_walking'] = np.where(eventsSF['modeBEAM']=='walk', eventsSF['distance_travelling'], np.nan)\n",
    "    eventsSF['duration_on_bike'] = np.where(eventsSF['modeBEAM']=='bike', eventsSF['duration_travelling'], np.nan)\n",
    "    eventsSF['distance_bike'] = np.where(eventsSF['modeBEAM']=='bike', eventsSF['distance_travelling'], np.nan)\n",
    "    eventsSF['duration_in_ridehail'] = np.where((eventsSF['modeBEAM_rh']=='ride_hail')|(eventsSF['modeBEAM_rh']=='ride_hail_pooled')|(eventsSF['modeBEAM_rh']=='ride_hail_transit'), \n",
    "                                                eventsSF['duration_travelling'], np.nan)\n",
    "    eventsSF['distance_ridehail'] = np.where((eventsSF['modeBEAM_rh']=='ride_hail')|(eventsSF['modeBEAM_rh']=='ride_hail_pooled')|(eventsSF['modeBEAM_rh']=='ride_hail_transit'), eventsSF['distance_travelling'], np.nan)\n",
    "    eventsSF['duration_in_privateCar'] = np.where((eventsSF['modeBEAM_rh']=='car')|(eventsSF['modeBEAM_rh']=='car_hov3')|(eventsSF['modeBEAM_rh']=='car_hov2')|\n",
    "                                                  (eventsSF['modeBEAM_rh']=='hov2_teleportation')|(eventsSF['modeBEAM_rh']=='hov3_teleportation') \n",
    "                                                  , eventsSF['duration_travelling'], np.nan)\n",
    "    eventsSF['distance_privateCar'] = np.where((eventsSF['modeBEAM_rh']=='car')|(eventsSF['modeBEAM_rh']=='car_hov3')|(eventsSF['modeBEAM_rh']=='car_hov2')|\n",
    "                                                  (eventsSF['modeBEAM_rh']=='hov2_teleportation')|(eventsSF['modeBEAM_rh']=='hov3_teleportation'), eventsSF['distance_travelling'], np.nan)\n",
    "    eventsSF['duration_in_transit'] = np.where((eventsSF['modeBEAM']=='bike_transit')|(eventsSF['modeBEAM']=='drive_transit')|\n",
    "                                               (eventsSF['modeBEAM']=='walk_transit')|(eventsSF['modeBEAM']=='bus')|\n",
    "                                               (eventsSF['modeBEAM']=='tram')|(eventsSF['modeBEAM']=='subway')|\n",
    "                                               (eventsSF['modeBEAM']=='rail')|(eventsSF['modeBEAM']=='cable_car')|\n",
    "                                               (eventsSF['modeBEAM']=='ride_hail_transit'), eventsSF['duration_travelling'], np.nan)\n",
    "    eventsSF['distance_transit'] = np.where((eventsSF['modeBEAM']=='bike_transit')|(eventsSF['modeBEAM']=='drive_transit')|\n",
    "                                            (eventsSF['modeBEAM']=='walk_transit')|(eventsSF['modeBEAM']=='bus')|\n",
    "                                            (eventsSF['modeBEAM']=='tram')|(eventsSF['modeBEAM']=='subway')|\n",
    "                                            (eventsSF['modeBEAM']=='rail')|(eventsSF['modeBEAM']=='cable_car')|\n",
    "                                            (eventsSF['modeBEAM']=='ride_hail_transit'), eventsSF['distance_travelling'], np.nan)\n",
    "    eventsSF['replanningTime'] = np.where(eventsSF['type'] == 'Replanning', eventsSF['time'], np.nan)\n",
    "    eventsSF['replanningTime'] = eventsSF['replanningTime'].shift(+1)\n",
    "    eventsSF['tourIndex_fixed'] = np.where((eventsSF['type'] == 'ModeChoice')&(eventsSF['replanningTime'].notna()), np.nan, eventsSF['tourIndex'])\n",
    "    eventsSF['fuelFood'] = np.where((eventsSF['type']=='PathTraversal')&(eventsSF['primaryFuelType']=='Food'), \n",
    "                                    eventsSF['primaryFuel'], np.nan)\n",
    "    eventsSF['emissionFood'] = eventsSF['fuelFood'] * 8.3141841e-9 * 0\n",
    "    eventsSF['fuelElectricity'] = np.where((eventsSF['type']=='PathTraversal')&(eventsSF['primaryFuelType']=='Electricity'), \n",
    "                                    eventsSF['primaryFuel'], np.nan)\n",
    "    eventsSF['emissionElectricity'] = eventsSF['fuelElectricity'] * 2.77778e-10 * 947.2 * 0.0005\n",
    "    eventsSF['fuelDiesel'] = np.where((eventsSF['type']=='PathTraversal')&(eventsSF['primaryFuelType']=='Diesel'), \n",
    "                                    eventsSF['primaryFuel'], np.nan)\n",
    "    eventsSF['emissionDiesel'] = eventsSF['fuelDiesel'] * 8.3141841e-9 * 10.180e-3\n",
    "    eventsSF['fuelBiodiesel'] = np.where((eventsSF['type']=='PathTraversal')&(eventsSF['primaryFuelType']=='Biodiesel'), \n",
    "                                    eventsSF['primaryFuel'], np.nan)\n",
    "    eventsSF['emissionBiodiesel'] = eventsSF['fuelBiodiesel'] * 8.3141841e-9 * 10.180e-3\n",
    "    eventsSF['fuel_not_Food'] = np.where((eventsSF['type']=='PathTraversal')&(eventsSF['primaryFuelType']!='Food')\n",
    "                                , eventsSF['primaryFuel']+eventsSF['secondaryFuel'], np.nan)\n",
    "    eventsSF['fuelGasoline'] = np.where((eventsSF['type']=='PathTraversal')&((eventsSF['primaryFuelType']=='Gasoline')|(eventsSF['secondaryFuelType']=='Gasoline')), \n",
    "                               eventsSF['primaryFuel']+eventsSF['secondaryFuel'], np.nan)\n",
    "    eventsSF['emissionGasoline'] = eventsSF['fuelGasoline'] * 8.3141841e-9 * 8.89e-3\n",
    "    # Marginal fuel\n",
    "    conditions  = [(eventsSF['modeBEAM_rh'] == 'ride_hail_pooled'), \n",
    "                   (eventsSF['modeBEAM_rh'] == 'walk_transit') | (eventsSF['modeBEAM_rh'] == 'drive_transit')|\n",
    "                   (eventsSF['modeBEAM_rh'] == 'ride_hail_transit')|(eventsSF['modeBEAM_rh'] == 'bus')|(eventsSF['modeBEAM_rh'] == 'subway')|\n",
    "                   (eventsSF['modeBEAM_rh'] == 'rail')|(eventsSF['modeBEAM_rh'] == 'tram')|(eventsSF['modeBEAM_rh'] == 'cable_car')|\n",
    "                   (eventsSF['modeBEAM_rh'] == 'bike_transit'),\n",
    "                   (eventsSF['modeBEAM_rh'] == 'walk')|(eventsSF['modeBEAM_rh'] == 'bike'),\n",
    "                   (eventsSF['modeBEAM_rh'] == 'ride_hail')|(eventsSF['modeBEAM_rh'] == 'car')| \n",
    "                   (eventsSF['modeBEAM_rh'] == 'car_hov2')| (eventsSF['modeBEAM_rh'] == 'car_hov3')|\n",
    "                   (eventsSF['modeBEAM_rh'] == 'hov2_teleportation')| (eventsSF['modeBEAM_rh'] == 'hov3_teleportation')]\n",
    "    choices = [eventsSF['fuel_not_Food']/eventsSF['numPassengers'], 0 , eventsSF['fuelFood'], eventsSF['fuel_not_Food']]\n",
    "    eventsSF['fuel_marginal'] = np.select(conditions, choices, default=np.nan)\n",
    "    # Marginal emission\n",
    "    conditions1  = [(eventsSF['modeBEAM_rh'] == 'ride_hail_pooled') & (eventsSF['fuelElectricity'].notna() != 0), \n",
    "                   (eventsSF['modeBEAM_rh'] == 'ride_hail_pooled') & (eventsSF['fuelGasoline'].notna() != 0),\n",
    "                   (eventsSF['modeBEAM_rh'] == 'ride_hail_pooled') & (eventsSF['fuelBiodiesel'].notna() != 0),\n",
    "                   (eventsSF['modeBEAM_rh'] == 'ride_hail_pooled') & (eventsSF['fuelDiesel'].notna() != 0),             \n",
    "                   (eventsSF['modeBEAM_rh'] == 'walk_transit') | (eventsSF['modeBEAM_rh'] == 'drive_transit')|\n",
    "                   (eventsSF['modeBEAM_rh'] == 'ride_hail_transit')|(eventsSF['modeBEAM_rh'] == 'bus')|(eventsSF['modeBEAM_rh'] == 'subway')|\n",
    "                   (eventsSF['modeBEAM_rh'] == 'rail')|(eventsSF['modeBEAM_rh'] == 'tram')|(eventsSF['modeBEAM_rh'] == 'cable_car')|\n",
    "                   (eventsSF['modeBEAM_rh'] == 'bike_transit'),\n",
    "\n",
    "                   (eventsSF['modeBEAM_rh'] == 'walk')|(eventsSF['modeBEAM_rh'] == 'bike'),\n",
    "\n",
    "                   (eventsSF['modeBEAM_rh'] == 'ride_hail')|(eventsSF['modeBEAM_rh'] == 'car')| \n",
    "                   (eventsSF['modeBEAM_rh'] == 'car_hov2')| (eventsSF['modeBEAM_rh'] == 'car_hov3')|\n",
    "                   (eventsSF['modeBEAM_rh'] == 'hov2_teleportation')| (eventsSF['modeBEAM_rh'] == 'hov3_teleportation')&\n",
    "                   (eventsSF['fuelElectricity'].notna() != 0),\n",
    "\n",
    "                   (eventsSF['modeBEAM_rh'] == 'ride_hail')|(eventsSF['modeBEAM_rh'] == 'car')| \n",
    "                   (eventsSF['modeBEAM_rh'] == 'car_hov2')| (eventsSF['modeBEAM_rh'] == 'car_hov3')|\n",
    "                   (eventsSF['modeBEAM_rh'] == 'hov2_teleportation')| (eventsSF['modeBEAM_rh'] == 'hov3_teleportation')&\n",
    "                   (eventsSF['fuelGasoline'].notna() != 0),           \n",
    "\n",
    "                   (eventsSF['modeBEAM_rh'] == 'ride_hail')|(eventsSF['modeBEAM_rh'] == 'car')| \n",
    "                   (eventsSF['modeBEAM_rh'] == 'car_hov2')| (eventsSF['modeBEAM_rh'] == 'car_hov3')|\n",
    "                   (eventsSF['modeBEAM_rh'] == 'hov2_teleportation')| (eventsSF['modeBEAM_rh'] == 'hov3_teleportation')&\n",
    "                   (eventsSF['fuelBiodiesel'].notna() != 0),   \n",
    "\n",
    "                   (eventsSF['modeBEAM_rh'] == 'ride_hail')|(eventsSF['modeBEAM_rh'] == 'car')| \n",
    "                   (eventsSF['modeBEAM_rh'] == 'car_hov2')| (eventsSF['modeBEAM_rh'] == 'car_hov3')|\n",
    "                   (eventsSF['modeBEAM_rh'] == 'hov2_teleportation')| (eventsSF['modeBEAM_rh'] == 'hov3_teleportation')&\n",
    "                   (eventsSF['fuelDiesel'].notna() != 0),\n",
    "\n",
    "                   (eventsSF['modeBEAM_rh'] == 'ride_hail')|(eventsSF['modeBEAM_rh'] == 'car')| \n",
    "                   (eventsSF['modeBEAM_rh'] == 'car_hov2')| (eventsSF['modeBEAM_rh'] == 'car_hov3')|\n",
    "                   (eventsSF['modeBEAM_rh'] == 'hov2_teleportation')| (eventsSF['modeBEAM_rh'] == 'hov3_teleportation')&\n",
    "                   (eventsSF['fuelFood'].notna() != 0)]\n",
    "\n",
    "    choices1 = [eventsSF['emissionElectricity']/eventsSF['numPassengers'],\n",
    "               eventsSF['emissionGasoline']/eventsSF['numPassengers'],\n",
    "               eventsSF['emissionBiodiesel']/eventsSF['numPassengers'],\n",
    "               eventsSF['emissionDiesel']/eventsSF['numPassengers'],           \n",
    "               0 , \n",
    "               eventsSF['emissionFood'], \n",
    "               eventsSF['emissionElectricity'],\n",
    "               eventsSF['emissionGasoline'],\n",
    "               eventsSF['emissionBiodiesel'],\n",
    "               eventsSF['emissionDiesel'],\n",
    "               eventsSF['emissionFood']]\n",
    "    eventsSF['emission_marginal'] = np.select(conditions1, choices1, default=np.nan)\n",
    "    eventsSF['actEndType'] = np.where(eventsSF['type']=='actend', eventsSF['actType'], \"\")\n",
    "    eventsSF['actStartType'] = np.where(eventsSF['type']=='actstart', eventsSF['actType'], \"\")\n",
    "    eventsSF[\"tripIndex\"] = eventsSF.tripId.fillna(method='ffill')\n",
    "    eventsSF['mode_choice_actual_BEAM'] = eventsSF.groupby(['IDMerged','tripId', 'type'])['modeBEAM'].transform('last')\n",
    "    eventsSF['mode_choice_planned_BEAM'] = eventsSF.groupby(['IDMerged','tripId', 'type'])['modeBEAM'].transform('first')\n",
    "    eventsSF['mode_choice_actual_BEAM'] = np.where(eventsSF['type'] != 'ModeChoice' , np.nan, eventsSF['mode_choice_actual_BEAM'])\n",
    "    eventsSF['mode_choice_planned_BEAM'] = np.where(eventsSF['type'] != 'ModeChoice' , np.nan, eventsSF['mode_choice_planned_BEAM'])\n",
    "    eventsSF.rename(columns={\"netCost\":\"cost_BEAM\"}, inplace=True) \n",
    "    eventsSF['replanning_status'] = np.where(eventsSF['type']=='Replanning', 1, 0)\n",
    "    eventsSF['reason'].replace('nan', np.NaN)\n",
    "    eventsSF['transit_bus'] = np.where(eventsSF['modeBEAM_rh']=='bus', 1, 0)\n",
    "    eventsSF['transit_subway'] = np.where(eventsSF['modeBEAM_rh']=='subway', 1, 0)\n",
    "    eventsSF['transit_tram'] = np.where(eventsSF['modeBEAM_rh']=='tram', 1, 0)\n",
    "    eventsSF['transit_rail'] = np.where(eventsSF['modeBEAM_rh']=='rail', 1, 0)\n",
    "    eventsSF['transit_cable_car'] = np.where(eventsSF['modeBEAM_rh']=='cable_car', 1, 0)\n",
    "    eventsSF['ride_hail_pooled'] = np.where(eventsSF['modeBEAM_rh']=='ride_hail_pooled', 1, 0)\n",
    "    print('step ', time.time()-a)\n",
    "    Person_Trip_eventsSF = pd.pivot_table(\n",
    "       eventsSF,\n",
    "       index=['IDMerged','tripIndex'],\n",
    "       aggfunc={'actStartTime': np.sum, \n",
    "                'actEndTime': np.sum, \n",
    "                'duration_travelling': np.sum, \n",
    "                'cost_BEAM': np.sum, \n",
    "                'actStartType': np.sum, \n",
    "                'actEndType': np.sum, \n",
    "                'duration_walking': np.sum, \n",
    "                'duration_in_privateCar': np.sum, \n",
    "                'duration_on_bike': np.sum, \n",
    "                'duration_in_ridehail': np.sum, \n",
    "                'distance_travelling': np.sum, \n",
    "                'duration_in_transit': np.sum, \n",
    "                'distance_walking': np.sum, \n",
    "                'distance_bike': np.sum, \n",
    "                'distance_ridehail': np.sum, \n",
    "                'distance_privateCar': np.sum, \n",
    "                'distance_transit': np.sum, \n",
    "                'legVehicleIds': np.sum, \n",
    "                'mode_choice_planned_BEAM':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "                'mode_choice_actual_BEAM':lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "                'vehicle': lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "                'numPassengers': lambda x: ', '.join(list(x.dropna().astype(str))),\n",
    "                'distance_mode_choice': np.sum,\n",
    "                'replanning_status': np.sum,\n",
    "                'reason': lambda x: ', '.join(list(x.dropna().astype(str))),\n",
    "                'parkingType': lambda x: ', '.join(list(x.dropna().astype(str))),\n",
    "                'transit_bus': np.sum, \n",
    "                'transit_subway': np.sum, \n",
    "                'transit_tram': np.sum, \n",
    "                'transit_cable_car': np.sum,\n",
    "                'ride_hail_pooled': np.sum, \n",
    "                'transit_rail': np.sum,\n",
    "                'year': lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "                'lever_position': lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "                'scenario': lambda x: ', '.join(set(x.dropna().astype(str))),\n",
    "                'fuelFood': np.sum, \n",
    "                'fuelElectricity': np.sum, \n",
    "                'fuelBiodiesel': np.sum, \n",
    "                'fuelDiesel': np.sum, \n",
    "                'fuel_not_Food': np.sum, \n",
    "                'fuelGasoline': np.sum, \n",
    "                'fuel_marginal': np.sum,\n",
    "                'emissionFood': np.sum, \n",
    "                'emissionElectricity': np.sum, \n",
    "                'emissionDiesel': np.sum, \n",
    "                'emissionGasoline': np.sum,\n",
    "                'emissionBiodiesel': np.sum, \n",
    "                'emission_marginal': np.sum,\n",
    "                'BlockGroupStart': 'first',\n",
    "                'startX': 'first',\n",
    "                'startY': 'first',\n",
    "                'bgid_start': 'first',\n",
    "                'tractid_start':'first',\n",
    "                'juris_name_start': 'first',\n",
    "                'county_name_start': 'first',\n",
    "                'mpo_start': 'first',\n",
    "                'BlockGroupEnd': 'last',\n",
    "                'endX': 'last',\n",
    "                'endY': 'last',\n",
    "                'bgid_end':'last',    \n",
    "                'tractid_end':'last',\n",
    "                'juris_name_end':'last',       \n",
    "                'county_name_end': 'last',   \n",
    "                'mpo_end': 'last',\n",
    "                'lever': lambda x: ', '.join(set(x.dropna().astype(str)))\n",
    "               }).reset_index() \n",
    "    Person_Trip_eventsSF['duration_door_to_door'] = Person_Trip_eventsSF['actStartTime'] - Person_Trip_eventsSF['actEndTime'] \n",
    "    Person_Trip_eventsSF['waitTime_no_replanning'] = np.where(Person_Trip_eventsSF['replanning_status'] == 0, Person_Trip_eventsSF['duration_door_to_door'] - Person_Trip_eventsSF['duration_travelling'], 0)\n",
    "    Person_Trip_eventsSF['waitTime_replanning'] = np.where(Person_Trip_eventsSF['replanning_status'] > 0, Person_Trip_eventsSF['duration_door_to_door'] - Person_Trip_eventsSF['duration_travelling'], 0) \n",
    "    Person_Trip_eventsSF['actPurpose'] = Person_Trip_eventsSF['actEndType'].astype(str) + \"_to_\" + Person_Trip_eventsSF['actStartType'].astype(str)\n",
    "    Person_Trip_eventsSF.rename(columns={\"legVehicleIds\":\"vehicleIds_estimate\"}, inplace=True) \n",
    "    Person_Trip_eventsSF.rename(columns={\"vehicle\":\"vehicleIds\"}, inplace=True) \n",
    "    # Column with five summarized modes\n",
    "    conditions  = [(Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'ride_hail') | (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'ride_hail_pooled'), \n",
    "                   (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'walk_transit') | (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'drive_transit')| (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'ride_hail_transit')|(Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'bike_transit'),\n",
    "                   (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'walk'), (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'bike'),\n",
    "                   (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'car') | (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'car_hov2')| (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'car_hov3')|(Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'hov2_teleportation')| (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'hov3_teleportation')]\n",
    "    choices = [ 'ride_hail', 'transit', 'walk', 'bike', 'car']\n",
    "    Person_Trip_eventsSF['mode_choice_actual_5'] = np.select(conditions, choices, default= np.nan)\n",
    "    # Column with six summarized modes\n",
    "    conditions  = [(Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'ride_hail') | (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'ride_hail_pooled'), \n",
    "                   (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'walk_transit') | (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'drive_transit')|(Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'bike_transit'),\n",
    "                   (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'walk'), (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'bike'),\n",
    "                   (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'car') | (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'car_hov2')| (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'car_hov3')|(Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'hov2_teleportation')| (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'hov3_teleportation'),\n",
    "                   (Person_Trip_eventsSF['mode_choice_actual_BEAM'] == 'ride_hail_transit')]\n",
    "    choices = [ 'ride_hail', 'transit', 'walk', 'bike', 'car', 'ride_hail_transit']\n",
    "    Person_Trip_eventsSF['mode_choice_actual_6'] = pd.Series(np.select(conditions, choices, default= np.nan)).replace({'nan':np.nan})\n",
    "    # Column with four summarized modes\n",
    "    Person_Trip_eventsSF['mode_choice_actual_4']  = np.where((Person_Trip_eventsSF['mode_choice_actual_5'] == 'walk')|(Person_Trip_eventsSF['mode_choice_actual_5'] == 'bike'),\n",
    "                                                            'walk/bike', Person_Trip_eventsSF['mode_choice_actual_5'])\n",
    "    Person_Trip_eventsSF = Person_Trip_eventsSF.drop(Person_Trip_eventsSF[Person_Trip_eventsSF.duration_door_to_door < 0].index)\n",
    "    #Person_Trip_eventsSF.to_csv('C:/Shared-Work/Data/CleanData/sf_2018_base_core_act.csv', index = False)\n",
    "    key = key_id.split('/')[0]+'/'+key_id.split('/')[1]+'/activitysim/'+key_id.split('/')[3]+'/households.csv.gz'\n",
    "    # key = \"pilates-outputs/sfbay-baseline-20220822/activitysim/year-2018-iteration-5/households.csv.gz\"  #the path should be updated\n",
    "    obj = s3.get_object(Bucket=\"beam-outputs\", Key=key)\n",
    "    households = pd.read_csv(obj['Body'], compression = 'gzip')\n",
    "    key = key_id.split('/')[0]+'/'+key_id.split('/')[1]+'/activitysim/'+key_id.split('/')[3]+'/persons.csv.gz'\n",
    "    # key = \"pilates-outputs/sfbay-baseline-20220822/activitysim/year-2018-iteration-5/persons.csv.gz\"  #the path should be updated\n",
    "    obj = s3.get_object(Bucket=\"beam-outputs\", Key=key)\n",
    "    persons = pd.read_csv(obj['Body'], compression = 'gzip')\n",
    "    key = key_id.split('/')[0]+'/'+key_id.split('/')[1]+'/activitysim/'+'final_tours.csv'\n",
    "    # key = \"pilates-outputs/sfbay-baseline-20220822/activitysim/year-2018-iteration-5/final_tours.csv.gz\"  #the path should be updated\n",
    "    obj = s3.get_object(Bucket=\"beam-outputs\", Key=key)\n",
    "    tours = pd.read_csv(obj['Body'])\n",
    "    key = key_id.split('/')[0]+'/'+key_id.split('/')[1]+'/activitysim/'+'final_trips.csv'\n",
    "    # key = \"pilates-outputs/sfbay-baseline-20220822/activitysim/year-2018-iteration-5/final_trips.csv.gz\"  #the path should be updated\n",
    "    obj = s3.get_object(Bucket=\"beam-outputs\", Key=key)\n",
    "    trips = pd.read_csv(obj['Body'])\n",
    "    # Merge BEAM households and persons \n",
    "    persons = persons.sort_values(by=['household_id']).reset_index(drop=True)\n",
    "    households = households.sort_values(by=['household_id']).reset_index(drop=True)\n",
    "    hhpersons = pd.merge(left=persons, right=households, how='left', on='household_id')\n",
    "    #hhpersons = pd.merge(left=persons, right=households, how='left', on='household_id', suffixes=('', '_drop'))\n",
    "    #hhpersons.drop([col for col in hhpersons.columns if 'drop' in col], axis=1, inplace=True)\n",
    "    # Merge tours, households and persons\n",
    "    tours = tours.sort_values(by=['person_id']).reset_index(drop=True)\n",
    "    hhpersons = hhpersons.sort_values(by=['person_id']).reset_index(drop=True)\n",
    "    hhperTours = pd.merge(left=tours, right=hhpersons, how='left', on='person_id')\n",
    "    #hhperTours = pd.merge(left=tours, right=hhpersons, how='left', on='person_id', suffixes=('', '_drop'))\n",
    "    #hhperTours.drop([col for col in hhperTours.columns if 'drop' in col], axis=1, inplace=True)\n",
    "    # Merge trips, tours, households and persons\n",
    "    trips = trips.sort_values(by=['person_id', 'tour_id']).reset_index(drop=True)\n",
    "    hhperTours = hhperTours.sort_values(by=['person_id','tour_id']).reset_index(drop=True)\n",
    "    tourTripsMerged = pd.merge(left=trips, right=hhperTours, how='left', on=['person_id','tour_id'])\n",
    "    #tourTripsMerged = pd.merge(left=trips, right=hhperTours, how='left', on=['person_id','tour_id'], suffixes=('', '_drop'))\n",
    "    #tourTripsMerged.drop([col for col in tourTripsMerged.columns if 'drop' in col], axis=1, inplace=True)\n",
    "    # Concat mode_choice_utilities files - Download File from s3 bucket \n",
    "    # key = key_id.split('/')[0]+'/'+key_id.split('/')[1]+'/activitysim/'+key_id.split('/')[3]+'/trip_mode_choice.csv.gz'\n",
    "    # key = \"pilates-outputs/sfbay-baseline-20220822/activitysim/year-2018-iteration-5/trip_mode_choice.zip\"  #the path should be updated\n",
    "    key = key_id.split('/')[0]+'/'+key_id.split('/')[1]+'/activitysim/'+key_id.split('/')[3]+'/trip_mode_choice.zip'\n",
    "    print('step ', time.time()-a)\n",
    "    s3.download_file(\n",
    "        Bucket=\"beam-outputs\", Key=key, Filename=\"trip_mode_choice.zip\")\n",
    "    # Automatically extracting ZipFile with a python command \n",
    "    with ZipFile('trip_mode_choice.zip', 'r') as zipObj:\n",
    "       # Extract all the contents of zip file in current directory\n",
    "       zipObj.extractall()\n",
    "    # Concat mode_choice_utilities files\n",
    "    path = 'trip_mode_choice/'\n",
    "    all_files = glob.glob(path + \"*utilities.csv\")\n",
    "    li_mapper = map(lambda filename: pd.read_csv(filename, index_col = None, header = 0), all_files)\n",
    "    li2 = list(li_mapper)\n",
    "    SFmode_choice_utilities = pd.concat(li2, axis = 0, ignore_index=True)\n",
    "    #just utilities\n",
    "    print('step ', time.time()-a)\n",
    "    # Merge trips, tours, households, persons, trip_mode_choice_raw, and utilities\n",
    "    tourTripsMerged = tourTripsMerged.sort_values(by=['trip_id'])\n",
    "    SFmode_choice_utilities = SFmode_choice_utilities.sort_values(by=['trip_id'])\n",
    "    SFActMerged= pd.merge(left=tourTripsMerged, right=SFmode_choice_utilities, how='left', on=['trip_id'])\n",
    "    # Both raw and utilities\n",
    "    # Merge trips, tours, households, persons, trip_mode_choice_raw, and utilities\n",
    "    #tourTripsMerged = tourTripsMerged.sort_values(by=['trip_id'])\n",
    "    #rawUtil = rawUtil.sort_values(by=['trip_id'])\n",
    "    #SFActMerged= pd.merge(left=tourTripsMerged, right=rawUtil, how='left', on=['trip_id'])\n",
    "    # Merge person_trip level BEAM with activity sim merged files\n",
    "    SFActMerged = SFActMerged.sort_values(by=['person_id', 'trip_id']).reset_index(drop=True)\n",
    "    Person_Trip_eventsSF = Person_Trip_eventsSF.sort_values(by=['IDMerged','tripIndex']).reset_index(drop=True)\n",
    "    eventsASim = pd.merge(left=Person_Trip_eventsSF, right=SFActMerged, how='left', left_on=[\"IDMerged\", 'tripIndex'], right_on=['person_id', 'trip_id'])\n",
    "    print('step ', time.time()-a)\n",
    "    #eventsASim = pd.merge(left=Person_Trip_eventsSF, right=tourTripsMerged, how='left',left_on = [\"IDMerged\", 'tripId'] , right_on=['person_id', 'trip_id'], suffixes=('', '_drop'))\n",
    "    #eventsASim.drop([col for col in eventsASim.columns if 'drop' in col], axis=1, inplace=True)\n",
    "    eventsASim.rename(columns={\"mode_choice_logsum_y\":\"logsum_tours_mode_AS_tours\"}, inplace=True)\n",
    "    eventsASim.rename(columns={\"tour_mode\":\"tour_mode_AS_tours\"}, inplace=True)\n",
    "    eventsASim.rename(columns={\"mode_choice_logsum_x\":\"logsum_trip_Potential_INEXUS\"}, inplace=True)\n",
    "    eventsASim.rename(columns={\"trip_mode\":\"trip_mode_AS_trips\"}, inplace=True)\n",
    "    # Add a column of income quartiles\n",
    "    quartiles = eventsASim['income'].quantile([0,.25, .5, .75,1]).tolist()\n",
    "    # Add income deciles\n",
    "    conditions  = [(eventsASim['income'] >= quartiles[0]) & (eventsASim['income'] < quartiles[1]), \n",
    "                   (eventsASim['income'] >= quartiles[1]) & (eventsASim['income'] < quartiles[2]),\n",
    "                   (eventsASim['income'] >=  quartiles[2]) & (eventsASim['income'] < quartiles[3]),\n",
    "                   (eventsASim['income'] >= quartiles[3]) & (eventsASim['income'] <= quartiles[4])]\n",
    "    print('step ', time.time()-a)\n",
    "    choices = [ '1stQ', '2ndQ', '3rdQ', '4thD']\n",
    "    eventsASim['income_quartiles'] = np.select(conditions, choices, default=None)\n",
    "    # Add a column of income deciles\n",
    "    deciles = eventsASim['income'].quantile([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]).tolist()\n",
    "    # Add income deciles\n",
    "    print('step ', time.time()-a)\n",
    "    conditions  = [(eventsASim['income'] >= deciles[0]) & (eventsASim['income'] < deciles[1]), \n",
    "                   (eventsASim['income'] >= deciles[1]) & (eventsASim['income'] < deciles[2]),\n",
    "                   (eventsASim['income'] >=  deciles[2]) & (eventsASim['income'] < deciles[3]),\n",
    "                   (eventsASim['income'] >= deciles[3]) & (eventsASim['income'] < deciles[4]), \n",
    "                   (eventsASim['income'] >=  deciles[4]) & (eventsASim['income'] < deciles[5]),\n",
    "                   (eventsASim['income'] >=  deciles[5]) & (eventsASim['income'] < deciles[6]),\n",
    "                   (eventsASim['income'] >=  deciles[6]) & (eventsASim['income'] < deciles[7]),\n",
    "                   (eventsASim['income'] >=  deciles[7]) & (eventsASim['income'] < deciles[8]),\n",
    "                   (eventsASim['income'] >=  deciles[8]) & (eventsASim['income'] < deciles[9]),\n",
    "                   (eventsASim['income'] >=  deciles[9]) & (eventsASim['income'] <= deciles[10])]\n",
    "\n",
    "    choices = [ '1stD', '2ndD', '3rdD', \n",
    "               '4thD', '5thD', '6thD', '7thD', '8thD', '9thD','10thD']\n",
    "    eventsASim['income_deciles'] = np.select(conditions, choices, default=None)\n",
    "    # Save the output file to S3\n",
    "    # eventsASim.to_csv('s3://beam-outputs/'+key_id[:-26]+key_id.split('/')[1]+'_inexus.csv', index=False)  #the path should be updated\n",
    "    eventsASim.to_csv('outputs/'+key_id.split('/')[1]+'_inexus.csv', index=False)  #the path should be updated\n",
    "    # Delete the utilities files downloaded and saved in the system\n",
    "    os.remove('trip_mode_choice.zip')   #the path should be updated\n",
    "    shutil.rmtree('trip_mode_choice')   #the path should be updated\n",
    "    \n",
    "    eventsASim = 0\n",
    "    Person_Trip_eventsSF = 0\n",
    "    SFActMerged = 0\n",
    "    eventsSF = 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa55683d-78a8-4b79-a900-c8cf9c754732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8bc5b9-a4e3-40f3-823d-5592c9c2a76f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f8b80-30c0-4a08-9ff4-4bfa682c8826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b62b9a-3dca-43c9-a533-723f5fb68a11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

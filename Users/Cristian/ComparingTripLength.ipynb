{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3019083/1902307361.py:9: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import time\n",
    "from itertools import groupby\n",
    "import geopandas as gpd\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import os\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import shapely\n",
    "from shapely.geometry import shape, Point, LineString, Polygon\n",
    "import warnings\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/beam_root/jupyter/jupyter_home_old\n",
      "/home/jovyan/beam_root\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Setting the current working directory. Ideally the tree should follow look the following\n",
    "BEAM\n",
    "- Data\n",
    "- Notebooks\n",
    "    - Google Cloud Scripts\n",
    "    - Local Scripts\n",
    "- Output\n",
    "    - City Name\n",
    "        - Passenger\n",
    "            - simulation_name        \n",
    "        - Freight\n",
    "            - simulation_name\n",
    "'''\n",
    "\n",
    "# This will return the folder where the script is stored\n",
    "BASE_DIR = Path.cwd()\n",
    "print(BASE_DIR)\n",
    "\n",
    "# set the project directory (two-levels up, ideally)\n",
    "project_folder = BASE_DIR.parent.parent\n",
    "print(project_folder)\n",
    "\n",
    "# General Declaration\n",
    "gc_url = f\"https://storage.googleapis.com/beam-core-outputs/\"\n",
    "iteration_no = \"0\" #change if we refer different simulation iteration\n",
    "len_id_transit = 3 # I dont know why its been used, but copied from CP script\n",
    "# conversion units\n",
    "meter_to_mile = 0.000621371\n",
    "percent_of_samples = 0.15\n",
    "mps_to_mph = 2.23694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(variable_filename):\n",
    "    simulation_data = {\n",
    "            f\"pilates-austin-baseline-calibrated-v3/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"Baseline\",\n",
    "            # Transit Capacity Scenarios\n",
    "            f\"pilates-austin-baseline-calibrated-v3-tr-capacity-025/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"TR_Cap_25pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-tr-capacity-050/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"TR_Cap_50pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-tr-capacity-150/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"TR_Cap_150pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-tr-capacity-200/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"TR_Cap_200pct\",\n",
    "            # # Transit Speed Scenarios\n",
    "            f\"pilates-austin-baseline-calibrated-v3-tr-spd-050-attmpt-4/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"TR_Spd_50pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-tr-spd-066-attmpt-4/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"TR_Spd_66pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-tr-spd-200/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"TR_Spd_200pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-tr-spd-400/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"TR_Spd_400pct\",\n",
    "            # # Transit Frequency Scenarios\n",
    "            f\"pilates-austin-baseline-calibrated-v3-tr-freq-025/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"TR_Frq_25pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-tr-freq-050/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"TR_Frq_50pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-tr-freq-150/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"TR_Frq_150pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-tr-freq-200/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"TR_Frq_200pct\",\n",
    "            # # Transit Fare Reduction Scenarios\n",
    "            f\"pilates-austin-baseline-calibrated-v3-tr-incentive-0c/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"TR_Inc_0c\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-tr-incentive-25c/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"TR_Inc_25c\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-tr-incentive-50c/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"TR_Inc_50c\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-tr-incentive-75c/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"TR_Inc_75c\",\n",
    "            # # Ridehail price Scenarios\n",
    "            f\"pilates-austin-baseline-calibrated-v3-rh-price-0125/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"RH_price_12.5pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-rh-price-050/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"RH_price_50pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-rh-price-200/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"RH_price_200pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-rh-price-500/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"RH_price_500pct\",\n",
    "            # # # Ridehail fleet size\n",
    "            f\"pilates-austin-baseline-calibrated-v3-rh-fleetsize-0125/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"RH_fleetsize_12.5pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-rh-fleetsize-050/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"RH_fleetsize_50pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-rh-fleetsize-300-attmpt-2/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"RH_fleetsize_300pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-rh-fleetsize-1000/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"RH_fleetsize_1000pct\",\n",
    "            # # # Ridehail fleet reposition\n",
    "            f\"pilates-austin-baseline-calibrated-v3-rh-reposition-0125/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"RH_reposition_12.5pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-rh-reposition-050/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"RH_reposition_50pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-rh-reposition-300/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"RH_reposition_300pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-rh-reposition-1000/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"RH_reposition_1000pct\",\n",
    "            # # # Ridehail fleet detour\n",
    "            f\"pilates-austin-baseline-calibrated-v3-rh-detour-0125/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"RH_detour_12.5pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-rh-detour-050/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"RH_detour_50pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-rh-detour-300/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"RH_detour_300pct\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-rh-detour-1000/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"RH_detour_1000pct\",\n",
    "            # # # NMT Incentives\n",
    "            f\"pilates-austin-baseline-calibrated-v3-nmt-incentive-025/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"NMT_Inc_25c\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-nmt-incentive-050/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"NMT_Inc_50c\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-nmt-incentive-100/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"NMT_Inc_100c\",\n",
    "            f\"pilates-austin-baseline-calibrated-v3-nmt-incentive-200/beam/year-2020-iteration-4/ITERS/it.{iteration_no}/{iteration_no}.{variable_filename}\": \"NMT_Inc_200c\",\n",
    "        }\n",
    "    return simulation_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path Traversal Columns\n",
    "PTsColumns = [\n",
    "    'vehicle', 'time', 'type', 'mode', 'length', 'vehicleType', 'arrivalTime', 'departureTime',\n",
    "    'capacity', 'secondaryFuel', 'primaryFuelType', 'secondaryFuelType', 'numPassengers', 'primaryFuel', 'startX',\n",
    "    'startY', 'endX', 'endY'\n",
    "]\n",
    "\n",
    "MCsColumns = ['person', 'time', 'type', 'mode', 'length', 'legModes']\n",
    "MCsColumns_no_leg_modes = ['person', 'time', 'type', 'mode', 'length']\n",
    "\n",
    "# modes associated with PT travel\n",
    "PTsModes = np.array(['walk', 'bike', 'bike_Sharing', 'bike_Sharing_empty', 'car', 'car_emer', 'car_hov2_emer',\n",
    "                     'car_hov3_emer', 'car_RideHail', 'car_RideHail_empty', 'car_RideHail_WC', 'car_RideHail_WC_empty',\n",
    "                     'car_CAV', 'car_hov2', 'car_hov3', 'bus', 'tram', 'rail', 'subway', 'cable_car', 'ferry',\n",
    "                     'bus_empty', 'tram_empty', 'rail_empty',\n",
    "                     'subway_empty', 'cable_car_empty', 'ferry_empty'])\n",
    "PTsModesNames = ['Walk', 'Bike', 'Bike Sharing', 'Empty Bike Sharing', 'Car', 'Car Emergency', 'Car HOV2 Emergency',\n",
    "                 'Car HOV3 Emergency', 'Ride Hail', 'Empty Ride Hail', 'Ride Hail WC', 'Empty Ride Hail WC', 'CAV',\n",
    "                 'Car HOV2', 'Car HOV3', 'Bus', 'Tram', 'Rail', 'Subway', 'Cable Car', 'Ferry', 'Empty Bus',\n",
    "                 'Empty Tram', 'Empty Rail', 'Empty Subway', 'Empty Cable Car', 'Empty Ferry', ]\n",
    "\n",
    "# Available Transits\n",
    "transit_modes = ['bus', 'subway', 'tram', 'rail', 'cable_car', 'ferry']\n",
    "transit_MCmodes = ['bus', 'subway', 'tram', 'rail', 'walk_transit', 'ride_hail_transit', 'drive_transit',\n",
    "                   'cable_car', 'bike_transit']\n",
    "# Mode Choices available\n",
    "MCsModes = np.array(['bus', 'subway', 'tram', 'rail', 'car', 'hov3_teleportation', 'bike', 'hov2_teleportation',\n",
    "                     'walk', 'car_hov2', 'car_hov3', 'walk_transit', 'ride_hail', 'ride_hail_transit',\n",
    "                     'ride_hail_pooled', 'drive_transit', 'cable_car', 'bike_transit'])\n",
    "MCsModesNames = ['Bus', 'Subway', 'Tram', 'Rail', 'Car', 'HOV3 Passenger', 'Bike', 'HOV2 Passenger', 'Walk',\n",
    "                 'HOV2 Driver', 'HOV3 Driver', 'Walk-Transit', 'Ride Hail', 'Ride Hail-Transit', 'Ride Hail Pooled',\n",
    "                 'Drive-Transit', 'Cable Car', 'Bike-Transit']\n",
    "\n",
    "# Fuel type available in BEAM\n",
    "primaryFuelTypes = ['Biodiesel', 'Diesel', 'Gasoline', 'Electricity', 'Food']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readEvents(directory):\n",
    "    '''\n",
    "    Reads the events file for the given simulation scenario\n",
    "    filters the events file into Path Traversal, Person Entering Vehicles, Mode Choices, Replanning and ActStart\n",
    "    returns them as lists variables PT, PEVs, PLVs, MCs, RPs, AS\n",
    "    :param directory:\n",
    "    :return PT, PEVs, PLVs, MCs, RPs, AS:\n",
    "    '''\n",
    "    # fullPath = directory + 'ITERS/it.0/0.events.csv.gz'\n",
    "    PTs = list()  # Path Traversal\n",
    "    PEVs = list()  # Person Entering Vehicles\n",
    "    PLVs = list()  # PersonLeavesVehicle\n",
    "    MCs = list()  # Mode Choices\n",
    "    RPs = list()  # Replanning\n",
    "    ASs = list()  # actstart\n",
    "\n",
    "    print('Reading ', directory)\n",
    "    readEvents_time = time.time()\n",
    "    for chunk in pd.read_csv(directory, chunksize=4000000):\n",
    "        if sum((chunk['type'] == 'PathTraversal')) > 0:\n",
    "            chunk['vehicle'] = chunk['vehicle'].astype(str)\n",
    "            # PT = Path Traversal events\n",
    "            # print(len(chunk.loc[(chunk['type'] == 'PathTraversal')]), ': len chunk PT')\n",
    "            PT = chunk.loc[(chunk['type'] == 'PathTraversal') & (chunk['length'] > 0)].dropna(how='all', axis=1)\n",
    "            PT['links'] = PT['links'].fillna('0')\n",
    "            PT['departureTime'] = PT['departureTime'].astype(int)\n",
    "            PT['arrivalTime'] = PT['arrivalTime'].astype(int)\n",
    "            PTs.append(PT[PTsColumns])\n",
    "            # print(len(PT), ': after filtering zero-length PT')\n",
    "            # PEV = Person Entering Vehicle\n",
    "            # print(len(chunk.loc[(chunk['type'] == 'PersonEntersVehicle')]), ': len chunk PEV')\n",
    "            #             PEV = chunk.loc[(chunk.type == \"PersonEntersVehicle\") &\n",
    "            #                             ~(chunk['person'].apply(str).str.contains('Agent').fillna(False)) &\n",
    "            #                             ~(chunk['vehicle'].str.contains('body').fillna(False)), :].dropna(how='all', axis=1)\n",
    "            # PEV = persons entering vehicle\n",
    "            PEV = chunk.loc[(chunk.type == \"PersonEntersVehicle\") &\n",
    "                            ~(chunk['person'].apply(str).str.contains('Agent').fillna(False))\n",
    "            , :].dropna(how='all', axis=1)\n",
    "            # print(len(PEV), ': after filtering drivers')\n",
    "\n",
    "            # PLV = Passenger Leaving Vehicle\n",
    "            if len(PEV) > 0:\n",
    "                PEV['person'] = PEV['person'].astype(int)\n",
    "                PEV['time'] = PEV['time'].astype(int)\n",
    "                PEVs.append(PEV)\n",
    "\n",
    "            # PLV\n",
    "            #             PLV = chunk.loc[(chunk.type == \"PersonLeavesVehicle\") &\n",
    "            #                             ~(chunk['person'].apply(str).str.contains('Agent').fillna(False)) &\n",
    "            #                             ~(chunk['vehicle'].str.contains('body').fillna(False)), :].dropna(how='all', axis=1)\n",
    "            # PLV = persons leaving vehicles\n",
    "            # print(len(chunk.loc[(chunk['type'] == 'PersonLeavesVehicle')]), ': len chunk PLV')\n",
    "            PLV = chunk.loc[(chunk.type == \"PersonLeavesVehicle\") &\n",
    "                            ~(chunk['person'].apply(str).str.contains('Agent').fillna(False))\n",
    "            , :].dropna(how='all', axis=1)\n",
    "            # print(len(PLV), ': after filtering drivers')\n",
    "            if len(PLV) > 0:\n",
    "                PLV['person'] = PLV['person'].astype(int)\n",
    "                PLV['time'] = PLV['time'].astype(int)\n",
    "                PLVs.append(PLV)\n",
    "        if sum((chunk['type'] == 'ModeChoice')) > 0:\n",
    "            # MC = Mode Choice\n",
    "            MC = chunk.loc[(chunk['type'] == 'ModeChoice') & (chunk['length'] > 0)].dropna(how='all', axis=1)\n",
    "            try:\n",
    "                MCs.append(MC[MCsColumns])\n",
    "            except:\n",
    "                MCs.append(MC[MCsColumns_no_leg_modes])\n",
    "                print('WARNING: probably no legModes')\n",
    "\n",
    "        if sum((chunk['type'] == 'Replanning')) > 0:\n",
    "            # RP = Replanning\n",
    "            RP = chunk.loc[(chunk['type'] == 'Replanning')].dropna(how='all', axis=1)\n",
    "            RPs.append(RP)\n",
    "\n",
    "        if sum((chunk['type'] == 'actstart')) > 0:\n",
    "            # AS = actstart\n",
    "            AS = chunk.loc[(chunk['type'] == 'actstart')].dropna(how='all', axis=1)\n",
    "            ASs.append(AS)\n",
    "\n",
    "        # print(chunk['type'].value_counts())\n",
    "    # print(len(pd.concat(PEVs)), ':len PEVs')\n",
    "    # print(len(pd.concat(PLVs)), ':len PLVs')\n",
    "\n",
    "    PEVs = pd.concat(PEVs)  # PEVs = persons entering vehicles events\n",
    "    PLVs = pd.concat(PLVs)  # PLVs = persons leaving vehicles events\n",
    "    PTs = pd.concat(PTs)  # PT = Path Traversal events\n",
    "    MCs = pd.concat(MCs)  # MC = mode choice events\n",
    "    RPs = pd.concat(RPs)  # RP = replanning events\n",
    "    ASs = pd.concat(ASs)  # AS = activity start events\n",
    "\n",
    "    # print(len(PTs), ':len PTs')\n",
    "    # print(len(MCs), ':len MCs')\n",
    "    # print(len(RPs), ':len RPs')\n",
    "    # print(len(ASs), ':len ASs')\n",
    "    print(f\"Completed reading events file in \"\n",
    "          f\"{time.strftime('%H:%M:%S'.format(str((time.time() - readEvents_time) % 1)), time.gmtime((time.time() - readEvents_time)))}\")\n",
    "    return MCs, PTs, PEVs, PLVs, RPs, ASs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixData(Mcs, PTs, PEVs, PLVs, len_id_transit):\n",
    "    '''\n",
    "    :param Mcs: list variable containing all events where events[type] = ModeChoice\n",
    "    :param PTs: list variable containing all events where events[type] = PathTraversals\n",
    "    :param PEVs: list variable containing all events where events[type] = PersonEnterVehicles\n",
    "    :param PLVs: list variable containing all events where events[type] = PersonLeavesVehicles\n",
    "    :param len_id_transit: globally declared =3\n",
    "    :return:\n",
    "    '''\n",
    "    fixdata_time = time.time()\n",
    "    PTs['duration'] = PTs['arrivalTime'] - PTs['departureTime']\n",
    "    PTs['gallonsGasoline'] = 0\n",
    "    PTs.loc[PTs['primaryFuelType'] == 'Gasoline', 'gallonsGasoline'] += (\n",
    "            PTs.loc[PTs['primaryFuelType'] == 'Gasoline', 'primaryFuel'] * 8.3141841e-9)\n",
    "    PTs.loc[PTs['secondaryFuelType'] == 'Gasoline', 'gallonsGasoline'] += (\n",
    "            PTs.loc[PTs['secondaryFuelType'] == 'Gasoline', 'secondaryFuel'] * 8.3141841e-9)\n",
    "    PTs['occupancy'] = PTs['numPassengers']\n",
    "\n",
    "    PTs['isCAV'] = PTs['vehicleType'].str.contains('L5')\n",
    "    PTs['isRH'] = PTs['vehicle'].str.contains('rideHail')\n",
    "    PTs['isBS'] = PTs['vehicle'].str.contains('bay_wheels')  # ??? needed?\n",
    "    PTs['isRH_WC'] = PTs['vehicleType'].str.contains('RH_Car-wheelchair')  # ??? needed?\n",
    "    PTs['is_empty'] = PTs['numPassengers'] == 0\n",
    "    PTs['is_RHempty'] = PTs['isRH'] & PTs['is_empty']\n",
    "    PTs['is_car_emer'] = PTs['vehicle'].str.contains('emergency')\n",
    "\n",
    "    PTs.loc[PTs['mode'] == 'car', 'capacity'] += 1\n",
    "    PTs.loc[PTs['mode'] == 'car_hov2', 'capacity'] += 1\n",
    "    PTs.loc[PTs['mode'] == 'car_hov3', 'capacity'] += 1\n",
    "    PTs.loc[PTs['mode'] == 'walk', 'capacity'] = 1\n",
    "    PTs.loc[PTs['mode'] == 'bike', 'capacity'] = 1\n",
    "\n",
    "    PTs.loc[PTs['isRH'], 'mode'] += '_RideHail'\n",
    "    PTs.loc[PTs['isBS'], 'mode'] += '_Sharing'\n",
    "    PTs.loc[PTs['isRH_WC'], 'mode'] += '_WC'\n",
    "    PTs.loc[PTs['isCAV'], 'mode'] += '_CAV'\n",
    "    PTs.loc[PTs['is_RHempty'], 'mode'] += '_empty'\n",
    "    PTs.loc[PTs['is_car_emer'], 'mode'] += '_emer'\n",
    "\n",
    "    PTs.loc[PTs['mode'] == 'car', 'occupancy'] = 1\n",
    "    PTs.loc[PTs['mode'] == 'car_emer', 'occupancy'] = 1\n",
    "    PTs.loc[PTs['mode'] == 'car_hov2', 'occupancy'] += 1\n",
    "    PTs.loc[PTs['mode'] == 'car_hov3', 'occupancy'] += 1\n",
    "    PTs.loc[PTs['mode'] == 'car_hov3_emer', 'occupancy'] = 1\n",
    "    PTs.loc[PTs['mode'] == 'walk', 'occupancy'] = 1\n",
    "    PTs.loc[PTs['mode'] == 'bike', 'occupancy'] = 1\n",
    "\n",
    "    PTs['vehicleMiles'] = PTs['length'] / 1609.34  # meters to miles\n",
    "    PTs['passengerMiles'] = (PTs['length'] * PTs['occupancy']) / 1609.34\n",
    "    PTs['totalEnergyInJoules'] = PTs['primaryFuel'] + PTs['secondaryFuel']\n",
    "\n",
    "    # print(\"PT[is_empty]\")\n",
    "    # print(PTs['is_empty'])\n",
    "    PTs['is_transit'] = 0\n",
    "    for tm in transit_modes:  # Tag all public transport events within PathTraversal events file\n",
    "        PTs['is' + tm] = PTs['mode'].str.contains(tm)\n",
    "        PTs['is_' + tm + '_empty'] = PTs['is' + tm] & PTs['is_empty']\n",
    "        PTs['is_transit'] += PTs['is' + tm]\n",
    "        PTs.loc[PTs['is_' + tm + '_empty'], 'mode'] += '_empty'\n",
    "        PTs.drop(columns=['is' + tm])\n",
    "        PTs.drop(columns=['is_' + tm + '_empty'])\n",
    "    #\n",
    "    PTs.drop(columns=['isCAV', 'is_empty', 'is_RHempty', 'isRH_WC', 'is_car_emer'])\n",
    "    #\n",
    "    vehicles_2 = list()\n",
    "    vehicles = PTs['vehicle']\n",
    "    for vehicle in vehicles:\n",
    "        vehicles_2.append(vehicle[:len_id_transit])\n",
    "    vehicles_2 = np.array(vehicles_2)\n",
    "    PTs['vehicle2'] = vehicles_2\n",
    "\n",
    "    vehicles_2 = list()\n",
    "    vehicles = PEVs['vehicle']\n",
    "    for vehicle in vehicles:\n",
    "        vehicles_2.append(vehicle[:len_id_transit])\n",
    "    PEVs['vehicle2'] = vehicles_2\n",
    "\n",
    "    vehicles_2 = list()\n",
    "    vehicles = PLVs['vehicle']\n",
    "    for vehicle in vehicles:\n",
    "        vehicles_2.append(vehicle[:len_id_transit])\n",
    "    PLVs['vehicle2'] = vehicles_2\n",
    "\n",
    "    print(f\"Fixed data in {time.strftime('%H:%M:%S'.format(str((time.time() - fixdata_time) % 1)), time.gmtime((time.time() - fixdata_time)))}\")\n",
    "    return Mcs, PTs, PEVs, PLVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation name: pilates-austin-baseline-calibrated-v3\n",
      "Reading  https://storage.googleapis.com/beam-core-outputs/pilates-austin-baseline-calibrated-v3/beam/year-2020-iteration-4/ITERS/it.0/0.events.csv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3019083/1104815210.py:19: DtypeWarning: Columns (0,3,6,7,11,12,13,19,20,22,24,27,28,29,30,37,38,41,53,55,56,60,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(directory, chunksize=4000000):\n",
      "/tmp/ipykernel_3019083/1104815210.py:19: DtypeWarning: Columns (34,41,60,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(directory, chunksize=4000000):\n",
      "/tmp/ipykernel_3019083/1104815210.py:19: DtypeWarning: Columns (10,22,24,27,28,29,30,60,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(directory, chunksize=4000000):\n",
      "/tmp/ipykernel_3019083/1104815210.py:19: DtypeWarning: Columns (10,60,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(directory, chunksize=4000000):\n",
      "/tmp/ipykernel_3019083/1104815210.py:19: DtypeWarning: Columns (10,34,41,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(directory, chunksize=4000000):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed reading events file in 00:02:57\n",
      "Fixed data in 00:00:32\n",
      "Simulation name: pilates-austin-baseline-calibrated-v3-tr-capacity-025\n",
      "Reading  https://storage.googleapis.com/beam-core-outputs/pilates-austin-baseline-calibrated-v3-tr-capacity-025/beam/year-2020-iteration-4/ITERS/it.0/0.events.csv.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:820\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1049\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1433\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1429\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[1;32m   1430\u001b[0m     )\n\u001b[0;32m-> 1433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[1;32m   1436\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m Replannings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     39\u001b[0m ActivityStarts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m---> 41\u001b[0m ModeChoices, PathTraversals, PersonEnteringVehicles, PersonLeavingVehicles, Replannings, ActivityStarts \u001b[38;5;241m=\u001b[39m \u001b[43mreadEvents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgc_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mevent_file\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# get filtered events files\u001b[39;00m\n\u001b[1;32m     42\u001b[0m ModeChoices, PathTraversals, PersonEnteringVehicles, PersonLeavingVehicles \u001b[38;5;241m=\u001b[39m fixData(ModeChoices, PathTraversals, PersonEnteringVehicles, PersonLeavingVehicles, len_id_transit)  \u001b[38;5;66;03m# fix transit modes name\u001b[39;00m\n\u001b[1;32m     44\u001b[0m ModeChoices_dict[simulation_name] \u001b[38;5;241m=\u001b[39m ModeChoices\n",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36mreadEvents\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading \u001b[39m\u001b[38;5;124m'\u001b[39m, directory)\n\u001b[1;32m     18\u001b[0m readEvents_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(directory, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4000000\u001b[39m):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m((chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPathTraversal\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m         chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvehicle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvehicle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1698\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1698\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1700\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1810\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1809\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[0;32m-> 1810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get trips by modes\n",
    "\n",
    "# get linkstats file\n",
    "linkstats_filename = \"linkstats.csv.gz\"\n",
    "variable_filename = linkstats_filename\n",
    "linkstats_simulation_data = get_filenames(variable_filename)\n",
    "linstats_data_names = linkstats_simulation_data.keys()\n",
    "\n",
    "# get events file\n",
    "events_filename = \"events.csv.gz\"\n",
    "variable_filename = events_filename\n",
    "events_simulation_data = get_filenames(variable_filename)\n",
    "events_data_names = events_simulation_data.keys()\n",
    "\n",
    "# get plans file\n",
    "plans_filename = \"plans.csv.gz\"\n",
    "variable_filename = plans_filename\n",
    "plans_simulation_data = get_filenames(variable_filename)\n",
    "plans_data_names = plans_simulation_data.keys()\n",
    "\n",
    "\n",
    "ModeChoices_dict = dict() #ModeChoice\n",
    "PathTraversals_dict = dict() #PathTraversal\n",
    "PersonEnteringVehicles_dict = dict() #PersonEnteringVehicle\n",
    "PersonLeavingVehicles_dict = dict() #PersonLeavingVehicle\n",
    "PersonToPathTraversals_dict = dict() #PersonToPathTraversals\n",
    "Replannings_dict= dict() # Replanning\n",
    "ActivityStarts_dict = dict() # Activities\n",
    "\n",
    "for event_file, plan_file in zip(events_data_names, plans_data_names):\n",
    "    simulation_name = event_file.split(\"/\")[0]\n",
    "    print(f\"Simulation name: {simulation_name}\")\n",
    "    ModeChoices = list() #ModeChoice\n",
    "    PathTraversals = list() #PathTraversal\n",
    "    PersonEnteringVehicles = list() #PersonEnteringVehicle\n",
    "    PersonLeavingVehicles = list() #PersonLeavingVehicle\n",
    "    PersonToPathTraversals = list() #PersonToPathTraversals\n",
    "    Replannings = list()\n",
    "    ActivityStarts = list()\n",
    "    \n",
    "    ModeChoices, PathTraversals, PersonEnteringVehicles, PersonLeavingVehicles, Replannings, ActivityStarts = readEvents(f\"{gc_url}{event_file}\")  # get filtered events files\n",
    "    ModeChoices, PathTraversals, PersonEnteringVehicles, PersonLeavingVehicles = fixData(ModeChoices, PathTraversals, PersonEnteringVehicles, PersonLeavingVehicles, len_id_transit)  # fix transit modes name\n",
    "    \n",
    "    ModeChoices_dict[simulation_name] = ModeChoices\n",
    "    PathTraversals_dict[simulation_name] = PathTraversals\n",
    "    PersonEnteringVehicles_dict[simulation_name] = PersonEnteringVehicles\n",
    "    PersonLeavingVehicles_dict[simulation_name] = PersonLeavingVehicles\n",
    "    Replannings_dict[simulation_name] = Replannings\n",
    "    ActivityStarts_dict[simulation_name] = ActivityStarts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listSimulationNames = linkstats_simulation_data.values()\n",
    "print(pd.DataFrame(listSimulationNames))\n",
    "# dfTe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trips counts according to modechoice events\n",
    "modechoice_counts = pd.DataFrame()\n",
    "for event_file in events_data_names:\n",
    "    simulation_name = event_file.split(\"/\")[0]\n",
    "    trip_counts = ModeChoices_dict[simulation_name][\"mode\"].value_counts()\n",
    "    print(trip_counts)\n",
    "    teleportation = trip_counts[\"hov2_teleportation\"] + trip_counts[\"hov3_teleportation\"]\n",
    "    car = trip_counts[\"car\"]\n",
    "    bus = trip_counts[\"walk_transit\"] + trip_counts[\"drive_transit\"] + trip_counts[\"bike_transit\"]\n",
    "    ridehail = trip_counts[\"ridehail\"]\n",
    "    ridehail_pool = trip_counts[\"ridehail_pooled\"]\n",
    "    bike = trip_counts[\"bike\"]    \n",
    "    break # get the baseline data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trips counts according to personenteringvehicles events\n",
    "personenteringvehicles_counts = pd.DataFrame()\n",
    "for event_file in events_data_names:\n",
    "    simulation_name = event_file.split(\"/\")[0]\n",
    "    trip_counts = PersonEnteringVehicles_dict[simulation_name][\"mode\"].value_counts()\n",
    "    print(trip_counts)\n",
    "    # teleportation = trip_counts[\"hov2_teleportation\"] + trip_counts[\"hov3_teleportation\"]\n",
    "    # car = trip_counts[\"car\"]\n",
    "    # bus = trip_counts[\"walk_transit\"] + trip_counts[\"drive_transit\"] + trip_counts[\"bike_transit\"]\n",
    "    # ridehail = trip_counts[\"ridehail\"]\n",
    "    # ridehail_pool = trip_counts[\"ridehail_pooled\"]\n",
    "    # bike = trip_counts[\"bike\"]    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModeChoice = ModeChoices_dict[\"pilates-austin-baseline-calibrated-v3\"]\n",
    "dfPathTraversal = PathTraversals_dict[\"pilates-austin-baseline-calibrated-v3\"]\n",
    "dfPEV = PersonEnteringVehicles_dict[\"pilates-austin-baseline-calibrated-v3\"]\n",
    "dfPLV = PersonLeavingVehicles_dict[\"pilates-austin-baseline-calibrated-v3\"]\n",
    "dfRP = Replannings_dict[\"pilates-austin-baseline-calibrated-v3\"]\n",
    "dfActivities = ActivityStarts_dict[\"pilates-austin-baseline-calibrated-v3\"]\n",
    "\n",
    "# dfPEV.loc[dfPEV[\"vehicle\"].str.contains(\"cap\",na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes_available = ['bus_empty', 'car_RideHail_empty', 'car', 'car_hov3', 'car_hov2',\n",
    "       'walk', 'bike', 'car_RideHail', 'bus', 'car_emer', 'car_hov2_emer',\n",
    "       'car_hov3_emer', 'tram_empty', 'tram']\n",
    "\n",
    "for mode in modes_available:\n",
    "    avg_trip_length = dfPathTraversal.loc[(dfPathTraversal[\"mode\"]==mode)][\"trip_length_in_miles\"].sum()/len(dfPathTraversal.loc[(dfPathTraversal[\"mode\"]==mode)])\n",
    "    print(f\"Average {mode} trip length: {avg_trip_length}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot average trip length by modes\n",
    "modes_available = ['bus_empty', 'car_RideHail_empty', 'car', 'car_hov3', 'car_hov2',\n",
    "       'walk', 'bike', 'car_RideHail', 'bus', 'car_emer', 'car_hov2_emer',\n",
    "       'car_hov3_emer', 'tram_empty', 'tram']\n",
    "simulation_name_list = list()\n",
    "car_excluding_hovs_list = list()\n",
    "walk_list = list()\n",
    "RH_list = list()\n",
    "\n",
    "for event_file in events_data_names:\n",
    "    simulation_name = event_file.split(\"/\")[0]\n",
    "    simulation_name_list.append(simulation_name)\n",
    "    dfPathTraversals = PathTraversals_dict[simulation_name]\n",
    "    dfPathTraversals[\"trip_length_in_miles\"] = dfPathTraversals[\"length\"]*0.00062137\n",
    "    dfPathTraversals[\"trip_length_in_miles\"] = dfPathTraversals[\"trip_length_in_miles\"].replace([np.nan, np.inf, -np.inf],0)\n",
    "    # check if the 'mode' column contains any of the modes we are interested in\n",
    "    mode = \"car\"\n",
    "    avg_car_trip_length = np.divide(dfPathTraversals.loc[(dfPathTraversals[\"mode\"]==mode)][\"trip_length_in_miles\"].sum(), len(dfPathTraversals.loc[(dfPathTraversals[\"mode\"]==mode)]))\n",
    "    car_excluding_hovs_list.append(avg_car_trip_length)\n",
    "    mode = \"walk\"\n",
    "    avg_walk_trip_length = np.divide(dfPathTraversals.loc[(dfPathTraversals[\"mode\"]==mode)][\"trip_length_in_miles\"].sum(), len(dfPathTraversals.loc[(dfPathTraversals[\"mode\"]==mode)]))\n",
    "    walk_list.append(avg_walk_trip_length)\n",
    "    mode = \"car_RideHail\"\n",
    "    avg_RH_trip_length = np.divide(dfPathTraversals.loc[(dfPathTraversals[\"mode\"]==mode)][\"trip_length_in_miles\"].sum(), len(dfPathTraversals.loc[(dfPathTraversals[\"mode\"]==mode)]))\n",
    "    RH_list.append(avg_RH_trip_length)\n",
    "    \n",
    "dictAvgTripLength = {\n",
    "    \"simulation_name\":simulation_name_list,\n",
    "    \"avg_car_trip_miles\":car_excluding_hovs_list,\n",
    "    \"avg_walk_trip_miles\":walk_list,\n",
    "    \"avg_RH_trip_miles\":RH_list,    \n",
    "}\n",
    "\n",
    "dfAvgTripLength = pd.DataFrame().from_dict(dictAvgTripLength)\n",
    "dfAvgTripLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_cartriplength = dfAvgTripLength.loc[0,\"avg_car_trip_miles\"]\n",
    "dfAvgTripLength[\"Baseline_CarTripLength\"] = baseline_cartriplength\n",
    "baseline_walktriplength = dfAvgTripLength.loc[0,\"avg_walk_trip_miles\"]\n",
    "dfAvgTripLength[\"Baseline_WalkTripLength\"] = baseline_walktriplength\n",
    "baseline_RHtriplength = dfAvgTripLength.loc[0,\"avg_RH_trip_miles\"]\n",
    "dfAvgTripLength[\"Baseline_RHTripLength\"] = baseline_RHtriplength\n",
    "\n",
    "# calculate perecentage change in vMT\n",
    "dfAvgTripLength[\"pct_change_cartrip\"] = ((dfAvgTripLength[\"avg_car_trip_miles\"] - dfAvgTripLength[\"Baseline_CarTripLength\"])*100/dfAvgTripLength[\"Baseline_CarTripLength\"])\n",
    "dfAvgTripLength[\"pct_change_walktrip\"] = ((dfAvgTripLength[\"avg_walk_trip_miles\"] - dfAvgTripLength[\"Baseline_WalkTripLength\"])*100/dfAvgTripLength[\"Baseline_WalkTripLength\"])\n",
    "dfAvgTripLength[\"pct_change_rhtrip\"] = ((dfAvgTripLength[\"avg_RH_trip_miles\"] - dfAvgTripLength[\"Baseline_RHTripLength\"])*100/dfAvgTripLength[\"Baseline_RHTripLength\"])\n",
    "# dfAvgTripLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAvgTripLength.to_csv(project_folder.joinpath(\"BEAM-Analysis\", \"Output\", \"austin\", \"Passenger\", \"Austin_SummaryTables\", \"AvgTripLength.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CarTripLength Comparision #\n",
    "\n",
    "# plot figure size (Width x Height (in inches))\n",
    "plt.figure(figsize=(8,7)) # default plotsize = width of 6.4 inches and a height of 4.8 inches\n",
    "\n",
    "# mark color red, if drop in % wrt Baseline other green\n",
    "colormat=np.where(dfAvgTripLength[\"pct_change_cartrip\"]>0, 'g','r')\n",
    "\n",
    "# Horizontal barplot for VMT\n",
    "sns.barplot(y = dfAvgTripLength[\"simulation_name\"], x = dfAvgTripLength[\"pct_change_cartrip\"], palette=colormat)\n",
    "\n",
    "# change specifically label font sizes using plt.xlabel()\n",
    "sns.set(font_scale=0.45)\n",
    "\n",
    "#add plot title\n",
    "plt.title('PCT Change in Car Trip length (miles) w.r.t baseline', fontsize=16)\n",
    "\n",
    "#add axis labels\n",
    "plt.xlabel('% change', fontsize=12)\n",
    "plt.ylabel('Sensitivity Scenarios', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the plot as JPG file\n",
    "plt.savefig(\"Change_in_CarTripLength.jpg\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WalkTripLength Comparision #\n",
    "\n",
    "# plot figure size (Width x Height (in inches))\n",
    "plt.figure(figsize=(8,7)) # default plotsize = width of 6.4 inches and a height of 4.8 inches\n",
    "\n",
    "# mark color red, if drop in % wrt Baseline other green\n",
    "colormat=np.where(dfAvgTripLength[\"pct_change_walktrip\"]>0, 'g','r')\n",
    "\n",
    "# Horizontal barplot for VMT\n",
    "sns.barplot(y = dfAvgTripLength[\"simulation_name\"], x = dfAvgTripLength[\"pct_change_walktrip\"], palette=colormat)\n",
    "# change specifically label font sizes using plt.xlabel()\n",
    "sns.set(font_scale=0.45)\n",
    "\n",
    "#add plot title\n",
    "plt.title('PCT Change in Walk Trip length (miles) w.r.t baseline', fontsize=16)\n",
    "\n",
    "#add axis labels\n",
    "plt.xlabel('% change', fontsize=12)\n",
    "plt.ylabel('Sensitivity Scenarios', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the plot as JPG file\n",
    "plt.savefig(\"Change_in_WalkTripLength.jpg\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RHTripLength Comparision #\n",
    "\n",
    "# plot figure size (Width x Height (in inches))\n",
    "plt.figure(figsize=(8,7)) # default plotsize = width of 6.4 inches and a height of 4.8 inches\n",
    "\n",
    "# mark color red, if drop in % wrt Baseline other green\n",
    "colormat=np.where(dfAvgTripLength[\"pct_change_rhtrip\"]>0, 'g','r')\n",
    "\n",
    "# Horizontal barplot for VMT\n",
    "sns.barplot(y = dfAvgTripLength[\"simulation_name\"], x = dfAvgTripLength[\"pct_change_rhtrip\"], palette=colormat)\n",
    "# change specifically label font sizes using plt.xlabel()\n",
    "sns.set(font_scale=0.45)\n",
    "\n",
    "#add plot title\n",
    "plt.title('PCT Change in RH Trip length (miles) w.r.t baseline', fontsize=16)\n",
    "\n",
    "#add axis labels\n",
    "plt.xlabel('% change', fontsize=12)\n",
    "plt.ylabel('Sensitivity Scenarios', fontsize=12)\n",
    "\n",
    "# plt.xlabel(\"sepal_length\", fontsize=40)\n",
    "# plt.ylabel(\"petal_length\", fontsize=40)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the plot as JPG file\n",
    "plt.savefig(\"Change_in_RHTripLength.jpg\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RH trips empty vs pooled vs single occupancy\n",
    "\n",
    "# get pooled vs non-pooled trips\n",
    "rhwaittime_filename = \"rideHailIndividualWaitingTimes.csv\"\n",
    "variable_filename = rhwaittime_filename\n",
    "rhwaittime_data = get_filenames(variable_filename) \n",
    "\n",
    "# trips counts according to personenteringvehicles events\n",
    "sensitivity_name_list = list()\n",
    "Total_RH_Trips_list = list()\n",
    "Nonempty_RHTrips_list = list()\n",
    "Empty_RHTrips_list = list()\n",
    "Nonpooled_RHTrips_list = list()\n",
    "Pooled_RHTrips_list = list()\n",
    "avg_RHTrips_waitTimeInSeconds_list = list()\n",
    "avg_RHTrips_pooled_waitTimeInSeconds_list = list()\n",
    "avg_distance_RHTrips_list = list()\n",
    "\n",
    "for event_file, rhwaittime in zip(events_data_names, rhwaittime_data):\n",
    "    simulation_name = event_file.split(\"/\")[0]\n",
    "    sensitivity_name_list.append(simulation_name)\n",
    "    dfRH = PathTraversals_dict[simulation_name]\n",
    "    \n",
    "    Total_RH_Trips_list.append(len(dfRH.loc[(dfRH[\"isRH\"]==True)]))    \n",
    "    nonempty_RHTrips = len(dfRH.loc[(dfRH[\"isRH\"]==True)]) - len(dfRH.loc[(dfRH[\"isRH\"]==True) & (dfRH[\"mode\"].str.contains(\"empty\",na=True))])\n",
    "    Nonempty_RHTrips_list.append(nonempty_RHTrips)    \n",
    "    Empty_RHTrips_list.append(len(dfRH.loc[(dfRH[\"isRH\"]==True) & (dfRH[\"mode\"].str.contains(\"empty\",na=True))]))\n",
    "    \n",
    "    # Pooled vs non-pooled\n",
    "    dfRHWaitTime = pd.read_csv(f\"{gc_url}{rhwaittime}\")\n",
    "    Pooled_RHTrips_list.append(len(dfRHWaitTime.loc[(dfRHWaitTime[\"modeChoice\"]==\"ride_hail_pooled\") & (dfRHWaitTime[\"waitingTimeInSeconds\"]>0)]))\n",
    "    Nonpooled_RHTrips_list.append(len(dfRHWaitTime.loc[(dfRHWaitTime[\"modeChoice\"]!=\"ride_hail_pooled\") & (dfRHWaitTime[\"waitingTimeInSeconds\"]>0)]))\n",
    "    \n",
    "    # Average WaitingTimeInSeconds\n",
    "    avg_RHTrips_waitTimes = dfRHWaitTime.loc[(dfRHWaitTime[\"modeChoice\"]!=\"ride_hail_pooled\")&(dfRHWaitTime[\"waitingTimeInSeconds\"]>0)][\"waitingTimeInSeconds\"].sum()/len(dfRHWaitTime.loc[(dfRHWaitTime[\"modeChoice\"]!=\"ride_hail_pooled\") & (dfRHWaitTime[\"waitingTimeInSeconds\"]>0)])\n",
    "    avg_RHTrips_waitTimeInSeconds_list.append(avg_RHTrips_waitTimes)\n",
    "    \n",
    "    avg_RHTrips_waitTimes = dfRHWaitTime.loc[(dfRHWaitTime[\"modeChoice\"]==\"ride_hail_pooled\")&(dfRHWaitTime[\"waitingTimeInSeconds\"]>0)][\"waitingTimeInSeconds\"].sum()/len(dfRHWaitTime.loc[(dfRHWaitTime[\"modeChoice\"]==\"ride_hail_pooled\") & (dfRHWaitTime[\"waitingTimeInSeconds\"]>0)])\n",
    "    avg_RHTrips_pooled_waitTimeInSeconds_list.append(avg_RHTrips_waitTimes)\n",
    "    \n",
    "    # Average Distance RHTrip makes \n",
    "    avg_distance_RHTrips = dfRH.loc[(dfRH[\"isRH\"]==True) & (~dfRH[\"mode\"].str.contains(\"empty\",na=True))][\"length\"].sum()/len(dfRH.loc[(dfRH[\"isRH\"]==True) & (~dfRH[\"mode\"].str.contains(\"empty\",na=True))])\n",
    "    avg_distance_RHTrips *= 0.000621371\n",
    "    avg_distance_RHTrips_list.append(avg_distance_RHTrips)\n",
    "    \n",
    "dictRHTable = {\n",
    "                \"simulation_name\":sensitivity_name_list,\n",
    "                \"Total_RHTrips\": Total_RH_Trips_list,\n",
    "                \"Empty_RHTrips\": Empty_RHTrips_list,\n",
    "                \"NonEmpty_RHTrips\": Nonempty_RHTrips_list,\n",
    "                \"Nonpooled_RHTrips\": Nonpooled_RHTrips_list,\n",
    "                \"pooled_RHTrips\": Pooled_RHTrips_list,\n",
    "                \"nonpooled_avg_waitTimeInSec\": avg_RHTrips_waitTimeInSeconds_list,\n",
    "                \"pooled_avg_waitTimeInSec\": avg_RHTrips_pooled_waitTimeInSeconds_list,\n",
    "                \"avg_miles_nonempty_RHTrips\": avg_distance_RHTrips_list\n",
    "                }\n",
    "\n",
    "dfRHTable = pd.DataFrame.from_dict(dictRHTable)\n",
    "dfRHTable.to_csv(project_folder.joinpath(\"BEAM-Analysis\", \"Output\", \"austin\", \"Passenger\", \"Austin_SummaryTables\", \"RidehailrelatedStats.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfComparisionTable = pd.read_csv(project_folder.joinpath(\"BEAM-Analysis\", \"Output\", \"austin\", \"Passenger\", \"Austin_SummaryTables\", \"VMT_VHT_Transit.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total RH Trips\n",
    "\n",
    "# plot figure size (Width x Height (in inches))\n",
    "plt.figure(figsize=(8,7)) # default plotsize = width of 6.4 inches and a height of 4.8 inches\n",
    "\n",
    "sns.barplot(y = dfRHTable[\"simulation_name\"], x = dfRHTable[\"Total_RHTrips\"]/1000)\n",
    "# change specifically label font sizes using plt.xlabel()\n",
    "sns.set(font_scale=0.55)\n",
    "\n",
    "#add plot title\n",
    "plt.title('Total RH Trips', fontsize=16)\n",
    "\n",
    "#add axis labels\n",
    "plt.xlabel('Number of Trips (in thousands)', fontsize=12)\n",
    "plt.ylabel('Sensitivity Scenarios', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the plot as JPG file\n",
    "plt.savefig(\"RH_Trips.jpg\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty RH Trips\n",
    "\n",
    "# plot figure size (Width x Height (in inches))\n",
    "plt.figure(figsize=(8,7)) # default plotsize = width of 6.4 inches and a height of 4.8 inches\n",
    "\n",
    "sns.barplot(y = dfRHTable[\"simulation_name\"], x = dfRHTable[\"NonEmpty_RHTrips\"]/1000)\n",
    "# change specifically label font sizes using plt.xlabel()\n",
    "sns.set(font_scale=0.55)\n",
    "\n",
    "#add plot title\n",
    "plt.title('Non Empty RH Trips', fontsize=16)\n",
    "\n",
    "#add axis labels\n",
    "plt.xlabel('Number of Trips (in thousands)', fontsize=12)\n",
    "plt.ylabel('Sensitivity Scenarios', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the plot as JPG file\n",
    "plt.savefig(\"Non Empty_RH_Trips.jpg\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooled RH Trips\n",
    "\n",
    "# plot figure size (Width x Height (in inches))\n",
    "plt.figure(figsize=(8,7)) # default plotsize = width of 6.4 inches and a height of 4.8 inches\n",
    "\n",
    "sns.barplot(y = dfRHTable[\"simulation_name\"], x = dfRHTable[\"pooled_RHTrips\"])\n",
    "# change specifically label font sizes using plt.xlabel()\n",
    "sns.set(font_scale=0.55)\n",
    "\n",
    "#add plot title\n",
    "plt.title('Pooled RH Trips', fontsize=16)\n",
    "\n",
    "#add axis labels\n",
    "plt.xlabel('Number of Trips', fontsize=12)\n",
    "plt.ylabel('Sensitivity Scenarios', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the plot as JPG file\n",
    "plt.savefig(\"Pooled RH Trips.jpg\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg RH Trip length\n",
    "\n",
    "# plot figure size (Width x Height (in inches))\n",
    "plt.figure(figsize=(8,7)) # default plotsize = width of 6.4 inches and a height of 4.8 inches\n",
    "\n",
    "sns.barplot(y = dfRHTable[\"simulation_name\"], x = dfRHTable[\"avg_miles_nonempty_RHTrips\"])\n",
    "# change specifically label font sizes using plt.xlabel()\n",
    "sns.set(font_scale=0.55)\n",
    "\n",
    "#add plot title\n",
    "plt.title('Avg non-empty RH Trips length (in miles)', fontsize=16)\n",
    "\n",
    "#add axis labels\n",
    "plt.xlabel('Trip length (in miles)', fontsize=12)\n",
    "plt.ylabel('Sensitivity Scenarios', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the plot as JPG file\n",
    "plt.savefig(\"avg_miles_nonempty_RHTrips.jpg\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NonEmpty vs Empty RH Trips\n",
    "\n",
    "# plot figure size (Width x Height (in inches))\n",
    "plt.figure(figsize=(8,7)) # default plotsize = width of 6.4 inches and a height of 4.8 inches\n",
    "\n",
    "sns.barplot(y = dfRHTable[\"simulation_name\"], x = dfRHTable[\"NonEmpty_RHTrips\"]*100/dfRHTable[\"Total_RHTrips\"])\n",
    "# change specifically label font sizes using plt.xlabel()\n",
    "sns.set(font_scale=0.55)\n",
    "\n",
    "#add plot title\n",
    "plt.title('PCT of non RH Trips of Total', fontsize=16)\n",
    "\n",
    "#add axis labels\n",
    "plt.xlabel('% of non-empty RH Trips compared to total RH trips', fontsize=12)\n",
    "plt.ylabel('Sensitivity Scenarios', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the plot as JPG file\n",
    "plt.savefig(\"pct_nonempty_RH_Trips.jpg\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfRHTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfRHWaitTime = pd.read_csv(\"https://storage.googleapis.com/beam-core-outputs/pilates-austin-baseline-calibrated-v3/beam/year-2020-iteration-4/ITERS/it.0/0.rideHailIndividualWaitingTimes.csv\")\n",
    "# dfRHWaitTime.loc[(dfRHWaitTime[\"modeChoice\"]!=\"ride_hail_pooled\") & (dfRHWaitTime[\"waitingTimeInSeconds\"]>0)][\"waitingTimeInSeconds\"].sum()/len(dfRHWaitTime.loc[(dfRHWaitTime[\"modeChoice\"]!=\"ride_hail_pooled\") & (dfRHWaitTime[\"waitingTimeInSeconds\"]>0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfRHWaitTime[\"modeChoice\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13100387-c7ec-4f15-b3a8-26c60cded3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in /opt/conda/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from geopandas) (1.5.3)\n",
      "Requirement already satisfied: pyproj>=2.6.1.post1 in /opt/conda/lib/python3.10/site-packages (from geopandas) (3.4.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from geopandas) (23.0)\n",
      "Requirement already satisfied: fiona>=1.8 in /opt/conda/lib/python3.10/site-packages (from geopandas) (1.8.22)\n",
      "Requirement already satisfied: shapely>=1.7 in /opt/conda/lib/python3.10/site-packages (from geopandas) (2.0.0)\n",
      "Requirement already satisfied: attrs>=17 in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8->geopandas) (22.2.0)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
      "Requirement already satisfied: six>=1.7 in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8->geopandas) (1.16.0)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8->geopandas) (8.1.3)\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8->geopandas) (2022.12.7)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8->geopandas) (66.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.0->geopandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.0->geopandas) (1.23.5)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: pygeos in /opt/conda/lib/python3.10/site-packages (0.14)\n",
      "Requirement already satisfied: numpy>=1.13 in /opt/conda/lib/python3.10/site-packages (from pygeos) (1.23.5)\n",
      "Requirement already satisfied: boto in /opt/conda/lib/python3.10/site-packages (2.49.0)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.10/site-packages (2023.1.0)\n",
      "Requirement already satisfied: aiobotocore~=2.4.2 in /opt/conda/lib/python3.10/site-packages (from s3fs) (2.4.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from s3fs) (3.8.3)\n",
      "Requirement already satisfied: fsspec==2023.1.0 in /opt/conda/lib/python3.10/site-packages (from s3fs) (2023.1.0)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /opt/conda/lib/python3.10/site-packages (from aiobotocore~=2.4.2->s3fs) (1.14.1)\n",
      "Requirement already satisfied: botocore<1.27.60,>=1.27.59 in /opt/conda/lib/python3.10/site-packages (from aiobotocore~=2.4.2->s3fs) (1.27.59)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from aiobotocore~=2.4.2->s3fs) (0.11.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (22.2.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.2->s3fs) (2.8.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.2->s3fs) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.2->s3fs) (1.26.14)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.2->s3fs) (1.16.0)\n",
      "Requirement already satisfied: shapely in /opt/conda/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely) (1.23.5)\n",
      "Requirement already satisfied: gcsfs in /opt/conda/lib/python3.10/site-packages (2023.1.0)\n",
      "Requirement already satisfied: decorator>4.1.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs) (5.1.1)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.10/site-packages (from gcsfs) (2.7.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from gcsfs) (2.28.2)\n",
      "Requirement already satisfied: fsspec==2023.1.0 in /opt/conda/lib/python3.10/site-packages (from gcsfs) (2023.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.10/site-packages (from gcsfs) (0.8.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from gcsfs) (3.8.3)\n",
      "Requirement already satisfied: google-auth>=1.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs) (2.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs) (5.3.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs) (1.16.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs) (2.11.0)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs) (2.4.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs) (2.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->gcsfs) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->gcsfs) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->gcsfs) (1.26.14)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (4.21.12)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.58.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage->gcsfs) (1.5.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_93706/1029240406.py:14: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "! pip install geopandas\n",
    "! pip install pandas\n",
    "! pip install pygeos\n",
    "! pip install boto\n",
    "! pip install s3fs\n",
    "! pip install shapely\n",
    "! pip install gcsfs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import time\n",
    "from itertools import groupby\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac3d799-1024-4505-a458-4a0657cdfc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPDATE COLUMNS ON THE TABLES\n",
    "scenarios = ['Aug2021','Aug2020','Baseline2019','Jan2022','May2020','May2022']\n",
    "for scenario in scenarios:\n",
    "    print('Read {}'.format(scenario))\n",
    "    nuovo  = pd.read_csv('outputs/MergedPTs_NYC_{}.csv.gz'.format(scenario), index_col=0)\n",
    "    nuovo2 =  pd.read_csv('outputs/MergedPtoPTs_NYC_{}.csv.gz'.format(scenario), index_col=0)\n",
    "    nuovo = nuovo[[ 'vehicle', 'mode', 'startX', 'startY', 'endX', 'endY',\n",
    "       'vehicleType', 'arrivalTime', 'departureTime', 'primaryFuel',\n",
    "       'primaryFuelType', 'secondaryFuel', 'secondaryFuelType', 'capacity',\n",
    "       'riders', 'duration', 'occupancy', 'vehicleMiles',\n",
    "       'passengerMiles', 'totalEnergyInJoules', 'gallonsGasoline']]\n",
    "    nuovo2 = nuovo2[['vehicleID', 'pathTraversalID', 'personID', 'planIndex', 'agency']]\n",
    "    new_list_riders = []\n",
    "    for list_riders in nuovo['riders']:\n",
    "        new_list = ''\n",
    "        for rider in list_riders.split(','):\n",
    "            new_list+=rider+':'\n",
    "        new_list_riders.append(new_list[1:-2])\n",
    "    nuovo['riders'] = new_list_riders\n",
    "    nuovo['riders'].replace('', None)\n",
    "    nuovo.to_csv('outputs/MergedPTs_NYC_{}_2.csv.gz'.format(scenario))\n",
    "    nuovo2.to_csv('outputs/MergedPtoPTs_NYC_{}_2.csv.gz'.format(scenario))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d9353-d784-4768-acca-d624b92830c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPDATE COLUMNS ON THE TABLES 2\n",
    "scenarios = ['Aug2021','Aug2020','Baseline2019','Jan2022','May2020','May2022']\n",
    "for scenario in scenarios:\n",
    "    print('Read {}'.format(scenario))\n",
    "    nuovo  = pd.read_csv('outputs/MergedPTs_NYC_{}_2.csv.gz'.format(scenario), index_col=0)\n",
    "    nuovo2 =  pd.read_csv('outputs/MergedPtoPTs_NYC_{}_2.csv.gz'.format(scenario), index_col=0)\n",
    "    nuovo['driver'] = False\n",
    "    nuovo['mode_extended'] = False\n",
    "\n",
    "    nuovo.to_csv('outputs/MergedPTs_NYC_{}_3.csv.gz'.format(scenario))\n",
    "    nuovo2.to_csv('outputs/MergedPtoPTs_NYC_{}_3.csv.gz'.format(scenario))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29845590-cbe5-4cf1-a846-80b55d81ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MERGE POPULATION AND HOUSEHOLDS\n",
    "\n",
    "fp = \"s3://beam-outputs/output/newyork/\"\n",
    "output_nm = 'Baseline'\n",
    "fp_res = 'outputs/'\n",
    "\n",
    "data = {\n",
    "    'new-york-baseline-0-of-10__2023-01-03_19-59-12_nwn/ITERS/it.5/5.events.csv.gz':'baseline-0',\n",
    "    'new-york-baseline-1-of-10__2023-01-03_19-59-13_jgp/ITERS/it.5/5.events.csv.gz':'baseline-1',\n",
    "    'new-york-baseline-2-of-10__2023-01-03_19-59-06_brm/ITERS/it.5/5.events.csv.gz':'baseline-2',\n",
    "    'new-york-baseline-3-of-10__2023-01-03_19-59-07_kqr/ITERS/it.5/5.events.csv.gz':'baseline-3',\n",
    "    'new-york-baseline-4-of-10__2023-01-03_19-59-09_fbb/ITERS/it.5/5.events.csv.gz':'baseline-4',\n",
    "    'new-york-baseline-5-of-10__2023-01-03_19-59-11_tjh/ITERS/it.5/5.events.csv.gz':'baseline-5',\n",
    "    'new-york-baseline-6-of-10__2023-01-03_19-59-12_zwm/ITERS/it.5/5.events.csv.gz':'baseline-6',\n",
    "    'new-york-baseline-7-of-10__2023-01-03_19-59-19_bcr/ITERS/it.5/5.events.csv.gz':'baseline-7',\n",
    "    'new-york-baseline-8-of-10__2023-01-03_19-59-14_olx/ITERS/it.5/5.events.csv.gz':'baseline-8',\n",
    "    'new-york-baseline-9-of-10__2023-01-03_19-59-07_xcc/ITERS/it.5/5.events.csv.gz':'baseline-9',\n",
    "}\n",
    "data_names = data.keys()\n",
    "pop_names = []\n",
    "hs_names = []\n",
    "for data_name in data_names:\n",
    "    pop_names.append(data_name[:-26]+'population.csv.gz')\n",
    "    hs_names.append(data_name[:-26]+'households.csv.gz')\n",
    "names = data.values()\n",
    "\n",
    "Merged_Pop = []\n",
    "Merged_Hs = []\n",
    "for name, pop_name, hs_name in zip(names, pop_names, hs_names):\n",
    "    Merged_Pop.append(pd.read_csv(fp+pop_name)) \n",
    "    Merged_Hs.append(pd.read_csv(fp+hs_name))\n",
    "Merged_Pop = pd.concat(Merged_Pop)\n",
    "Merged_Hs = pd.concat(Merged_Hs)\n",
    "Merged_Pop.to_csv(fp_res+'Merged_Pop_{}.csv.gz'.format(output_nm))\n",
    "Merged_Hs.to_csv(fp_res+'Merged_Hs_{}.csv.gz'.format(output_nm))\n",
    "print(len(Merged_Pop))\n",
    "print(len(Merged_Hs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a0d30-339c-466f-b356-6a17cb5d639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"s3://beam-outputs/output/newyork/\"\n",
    "output_nm = 'Jan2022'\n",
    "fp_res = 'outputs/'\n",
    "\n",
    "data = {\n",
    "    'new-york-jan2022-0-of-10__2023-01-04_00-19-31_leg/ITERS/it.5/5.events.csv.gz':'jan2022-0',\n",
    "    'new-york-jan2022-1-of-10__2023-01-14_04-04-57_ztd/ITERS/it.5/5.events.csv.gz':'jan2022-1',\n",
    "    'new-york-jan2022-2-of-10__2023-01-04_00-19-30_bqw/ITERS/it.5/5.events.csv.gz':'jan2022-2',\n",
    "    'new-york-jan2022-3-of-10__2023-01-04_00-19-36_noe/ITERS/it.5/5.events.csv.gz':'jan2022-3',\n",
    "    'new-york-jan2022-4-of-10__2023-01-04_00-19-33_nop/ITERS/it.5/5.events.csv.gz':'jan2022-4',\n",
    "    'new-york-jan2022-5-of-10__2023-01-04_00-19-37_pnp/ITERS/it.5/5.events.csv.gz':'jan2022-5',\n",
    "    'new-york-jan2022-6-of-10__2023-01-04_00-19-29_kny/ITERS/it.5/5.events.csv.gz':'jan2022-6',\n",
    "    'new-york-jan2022-7-of-10__2023-01-04_00-19-39_qwt/ITERS/it.5/5.events.csv.gz':'jan2022-7',\n",
    "    'new-york-jan2022-8-of-10__2023-01-04_00-19-35_fzq/ITERS/it.5/5.events.csv.gz':'jan2022-8',\n",
    "    'new-york-jan2022-9-of-10__2023-01-04_00-19-31_dgg/ITERS/it.5/5.events.csv.gz':'jan2022-9',\n",
    "}\n",
    "data_names = data.keys()\n",
    "pop_names = []\n",
    "hs_names = []\n",
    "for data_name in data_names:\n",
    "    pop_names.append(data_name[:-26]+'population.csv.gz')\n",
    "    hs_names.append(data_name[:-26]+'households.csv.gz')\n",
    "names = data.values()\n",
    "\n",
    "Merged_Pop = []\n",
    "Merged_Hs = []\n",
    "for name, pop_name, hs_name in zip(names, pop_names, hs_names):\n",
    "    Merged_Pop.append(pd.read_csv(fp+pop_name)) \n",
    "    Merged_Hs.append(pd.read_csv(fp+hs_name))\n",
    "Merged_Pop = pd.concat(Merged_Pop)\n",
    "Merged_Hs = pd.concat(Merged_Hs)\n",
    "Merged_Pop.to_csv(fp_res+'Merged_Pop_{}.csv.gz'.format(output_nm))\n",
    "Merged_Hs.to_csv(fp_res+'Merged_Hs_{}.csv.gz'.format(output_nm))\n",
    "print(len(Merged_Pop))\n",
    "print(len(Merged_Hs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73a4e3a-bcd4-4fd9-a255-e3b3d98cf668",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"s3://beam-outputs/output/newyork/\"\n",
    "output_nm = 'May2020'\n",
    "fp_res = 'outputs/'\n",
    "\n",
    "data = {\n",
    "    'new-york-may2020-0-of-10__2023-01-12_18-51-34_sju/ITERS/it.5/5.events.csv.gz':'may2020-0',\n",
    "    'new-york-may2020-1-of-10__2023-01-12_18-51-33_nyd/ITERS/it.5/5.events.csv.gz':'may2020-1',\n",
    "    'new-york-may2020-2-of-10__2023-01-12_18-51-29_yow/ITERS/it.5/5.events.csv.gz':'may2020-2',\n",
    "    'new-york-may2020-3-of-10__2023-01-12_18-51-36_inx/ITERS/it.5/5.events.csv.gz':'may2020-3',\n",
    "    'new-york-may2020-4-of-10__2023-01-12_18-51-31_iwm/ITERS/it.5/5.events.csv.gz':'may2020-4',\n",
    "    'new-york-may2020-5-of-10__2023-01-12_18-51-34_jui/ITERS/it.5/5.events.csv.gz':'may2020-5',\n",
    "    'new-york-may2020-6-of-10__2023-01-12_18-51-34_gqr/ITERS/it.5/5.events.csv.gz':'may2020-6',\n",
    "    'new-york-may2020-7-of-10__2023-01-12_18-51-28_juu/ITERS/it.5/5.events.csv.gz':'may2020-7',\n",
    "    'new-york-may2020-8-of-10__2023-01-12_18-51-36_ehv/ITERS/it.5/5.events.csv.gz':'may2020-8',\n",
    "    'new-york-may2020-9-of-10__2023-01-12_18-51-54_bas/ITERS/it.5/5.events.csv.gz':'may2020-9',\n",
    "}\n",
    "data_names = data.keys()\n",
    "pop_names = []\n",
    "hs_names = []\n",
    "for data_name in data_names:\n",
    "    pop_names.append(data_name[:-26]+'population.csv.gz')\n",
    "    hs_names.append(data_name[:-26]+'households.csv.gz')\n",
    "names = data.values()\n",
    "\n",
    "Merged_Pop = []\n",
    "Merged_Hs = []\n",
    "for name, pop_name, hs_name in zip(names, pop_names, hs_names):\n",
    "    Merged_Pop.append(pd.read_csv(fp+pop_name)) \n",
    "    Merged_Hs.append(pd.read_csv(fp+hs_name))\n",
    "Merged_Pop = pd.concat(Merged_Pop)\n",
    "Merged_Hs = pd.concat(Merged_Hs)\n",
    "Merged_Pop.to_csv(fp_res+'Merged_Pop_{}.csv.gz'.format(output_nm))\n",
    "Merged_Hs.to_csv(fp_res+'Merged_Hs_{}.csv.gz'.format(output_nm))\n",
    "print(len(Merged_Pop))\n",
    "print(len(Merged_Hs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d842d0-e97f-4b92-80fb-ff3df8fe7232",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"s3://beam-outputs/output/newyork/\"\n",
    "output_nm = 'May2022'\n",
    "fp_res = 'outputs/'\n",
    "\n",
    "data = {\n",
    "    'new-york-june2022-0-of-10__2023-01-10_16-53-18_iir/ITERS/it.5/5.events.csv.gz':'may2022-0',\n",
    "    'new-york-june2022-1-of-10__2023-01-10_16-53-23_mrs/ITERS/it.0/0.events.csv.gz':'may2022-1',\n",
    "    'new-york-june2022-2-of-10__2023-01-10_16-53-36_log/ITERS/it.5/5.events.csv.gz':'may2022-2',\n",
    "    'new-york-june2022-3-of-10__2023-01-10_16-53-36_lvd/ITERS/it.0/0.events.csv.gz':'may2022-3',\n",
    "    'new-york-june2022-4-of-10__2023-01-10_16-53-44_afe/ITERS/it.0/0.events.csv.gz':'may2022-4',\n",
    "    'new-york-june2022-5-of-10__2023-01-10_16-53-58_gry/ITERS/it.5/5.events.csv.gz':'may2022-5',\n",
    "    'new-york-june2022-6-of-10__2023-01-10_16-53-59_qdi/ITERS/it.5/5.events.csv.gz':'may2022-6',\n",
    "    'new-york-june2022-7-of-10__2023-01-10_16-54-13_fwj/ITERS/it.0/0.events.csv.gz':'may2022-7',\n",
    "    'new-york-june2022-8-of-10__2023-01-10_16-54-18_okl/ITERS/it.5/5.events.csv.gz':'may2022-8',\n",
    "    'new-york-june2022-9-of-10__2023-01-10_16-54-38_jvb/ITERS/it.5/5.events.csv.gz':'may2022-9',\n",
    "}\n",
    "data_names = data.keys()\n",
    "pop_names = []\n",
    "hs_names = []\n",
    "for data_name in data_names:\n",
    "    pop_names.append(data_name[:-26]+'population.csv.gz')\n",
    "    hs_names.append(data_name[:-26]+'households.csv.gz')\n",
    "names = data.values()\n",
    "\n",
    "Merged_Pop = []\n",
    "Merged_Hs = []\n",
    "for name, pop_name, hs_name in zip(names, pop_names, hs_names):\n",
    "    Merged_Pop.append(pd.read_csv(fp+pop_name)) \n",
    "    Merged_Hs.append(pd.read_csv(fp+hs_name))\n",
    "Merged_Pop = pd.concat(Merged_Pop)\n",
    "Merged_Hs = pd.concat(Merged_Hs)\n",
    "Merged_Pop.to_csv(fp_res+'Merged_Pop_{}.csv.gz'.format(output_nm))\n",
    "Merged_Hs.to_csv(fp_res+'Merged_Hs_{}.csv.gz'.format(output_nm))\n",
    "print(len(Merged_Pop))\n",
    "print(len(Merged_Hs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afce561-ed74-47b9-b503-0b4e307cc00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"s3://beam-outputs/output/newyork/\"\n",
    "output_nm = 'Aug2021'\n",
    "fp_res = 'outputs/'\n",
    "\n",
    "data = {\n",
    "    'new-york-august2021-0-of-10__2023-01-12_18-51-43_rxk/ITERS/it.5/5.events.csv.gz':'aug2021-0',\n",
    "    'new-york-august2021-1-of-10__2023-01-12_18-51-29_aza/ITERS/it.5/5.events.csv.gz':'aug2021-1',\n",
    "    'new-york-august2021-2-of-10__2023-01-12_18-51-33_xqs/ITERS/it.5/5.events.csv.gz':'aug2021-2',\n",
    "    'new-york-august2021-3-of-10__2023-01-12_18-51-29_abg/ITERS/it.5/5.events.csv.gz':'aug2021-3',\n",
    "    'new-york-august2021-4-of-10__2023-01-12_18-51-34_lwg/ITERS/it.5/5.events.csv.gz':'aug2021-4',\n",
    "    'new-york-august2021-5-of-10__2023-01-12_18-51-34_air/ITERS/it.5/5.events.csv.gz':'aug2021-5',\n",
    "    'new-york-august2021-6-of-10__2023-01-12_18-51-28_vbj/ITERS/it.5/5.events.csv.gz':'aug2021-6',\n",
    "    'new-york-august2021-7-of-10__2023-01-12_18-51-29_dnu/ITERS/it.5/5.events.csv.gz':'aug2021-7',\n",
    "    'new-york-august2021-8-of-10__2023-01-12_18-51-29_uzv/ITERS/it.5/5.events.csv.gz':'aug2021-8',\n",
    "    'new-york-august2021-9-of-10__2023-01-19_00-58-43_fwc/ITERS/it.5/5.events.csv.gz':'aug2021-9',\n",
    "}\n",
    "data_names = data.keys()\n",
    "pop_names = []\n",
    "hs_names = []\n",
    "for data_name in data_names:\n",
    "    pop_names.append(data_name[:-26]+'population.csv.gz')\n",
    "    hs_names.append(data_name[:-26]+'households.csv.gz')\n",
    "names = data.values()\n",
    "\n",
    "Merged_Pop = []\n",
    "Merged_Hs = []\n",
    "for name, pop_name, hs_name in zip(names, pop_names, hs_names):\n",
    "    Merged_Pop.append(pd.read_csv(fp+pop_name)) \n",
    "    Merged_Hs.append(pd.read_csv(fp+hs_name))\n",
    "Merged_Pop = pd.concat(Merged_Pop)\n",
    "Merged_Hs = pd.concat(Merged_Hs)\n",
    "Merged_Pop.to_csv(fp_res+'Merged_Pop_{}.csv.gz'.format(output_nm))\n",
    "Merged_Hs.to_csv(fp_res+'Merged_Hs_{}.csv.gz'.format(output_nm))\n",
    "print(len(Merged_Pop))\n",
    "print(len(Merged_Hs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e3888-04d6-4649-b15f-fef06a1d5bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fp = \"s3://beam-outputs/output/newyork/\"\n",
    "output_nm = 'Aug2020'\n",
    "fp_res = 'outputs/'\n",
    "\n",
    "data = {\n",
    "    'new-york-august2020-0-of-10__2023-01-10_16-52-48_lgr/ITERS/it.5/5.events.csv.gz':'aug2020-0',\n",
    "    'new-york-august2020-1-of-10__2023-01-10_16-52-46_gib/ITERS/it.5/5.events.csv.gz':'aug2020-1',\n",
    "    'new-york-august2020-2-of-10__2023-01-14_21-19-35_flk/ITERS/it.5/5.events.csv.gz':'aug2020-2',\n",
    "    'new-york-august2020-3-of-10__2023-01-15_01-54-44_txq/ITERS/it.5/5.events.csv.gz':'aug2020-3',\n",
    "    'new-york-august2020-4-of-10__2023-01-10_16-52-43_rio/ITERS/it.5/5.events.csv.gz':'aug2020-4',\n",
    "    'new-york-august2020-5-of-10__2023-01-10_16-53-00_vjr/ITERS/it.5/5.events.csv.gz':'aug2020-5',\n",
    "    'new-york-august2020-6-of-10__2023-01-10_16-52-53_bhh/ITERS/it.5/5.events.csv.gz':'aug2020-6',\n",
    "    'new-york-august2020-7-of-10__2023-01-10_16-53-01_drt/ITERS/it.5/5.events.csv.gz':'aug2020-7',\n",
    "    'new-york-august2020-8-of-10__2023-01-10_16-53-15_qgy/ITERS/it.5/5.events.csv.gz':'aug2020-8',\n",
    "    'new-york-august2020-9-of-10__2023-01-10_16-53-14_pdt/ITERS/it.5/5.events.csv.gz':'aug2020-9',\n",
    "}\n",
    "data_names = data.keys()\n",
    "pop_names = []\n",
    "hs_names = []\n",
    "for data_name in data_names:\n",
    "    pop_names.append(data_name[:-26]+'population.csv.gz')\n",
    "    hs_names.append(data_name[:-26]+'households.csv.gz')\n",
    "names = data.values()\n",
    "\n",
    "Merged_Pop = []\n",
    "Merged_Hs = []\n",
    "for name, pop_name, hs_name in zip(names, pop_names, hs_names):\n",
    "    Merged_Pop.append(pd.read_csv(fp+pop_name)) \n",
    "    Merged_Hs.append(pd.read_csv(fp+hs_name))\n",
    "Merged_Pop = pd.concat(Merged_Pop)\n",
    "Merged_Hs = pd.concat(Merged_Hs)\n",
    "Merged_Pop.to_csv(fp_res+'Merged_Pop_{}.csv.gz'.format(output_nm))\n",
    "Merged_Hs.to_csv(fp_res+'Merged_Hs_{}.csv.gz'.format(output_nm))\n",
    "print(len(Merged_Pop))\n",
    "print(len(Merged_Hs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b4759-9dbb-44d4-9a1e-166bb450d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE A TABLE WITH VEHICLE CAPACITIES FOR EACH SCENARIO\n",
    "\n",
    "scenarios = ['new-york-baseline-0-of-10__2023-01-03_19-59-12_nwn/ITERS/it.5/5.events.csv.gz',\n",
    "            'new-york-jan2022-0-of-10__2023-01-04_00-19-31_leg/ITERS/it.5/5.events.csv.gz',\n",
    "            'new-york-may2020-0-of-10__2023-01-12_18-51-34_sju/ITERS/it.5/5.events.csv.gz',\n",
    "            'new-york-june2022-0-of-10__2023-01-10_16-53-18_iir/ITERS/it.5/5.events.csv.gz',\n",
    "            'new-york-august2021-0-of-10__2023-01-12_18-51-43_rxk/ITERS/it.5/5.events.csv.gz',\n",
    "            'new-york-august2020-0-of-10__2023-01-10_16-52-48_lgr/ITERS/it.5/5.events.csv.gz',\n",
    "            ]\n",
    "names = ['Baseline2019','Jan2022','May2020','May2022', 'Aug2021','Aug2020']\n",
    "\n",
    "for scenario, name in zip(scenarios, names):\n",
    "    print('Read {}'.format(scenario))\n",
    "    Pts_scenario  = pd.read_csv('s3://beam-outputs/output/newyork/{}'.format(scenario), nrows = None)\n",
    "    Pts_scenario = Pts_scenario[(Pts_scenario['type']=='PathTraversal')]\n",
    "    Pts_scenario = Pts_scenario[(Pts_scenario['mode']=='bus')|(Pts_scenario['mode']=='subway')|(Pts_scenario['mode']=='rail')|(Pts_scenario['mode']=='tram')|(Pts_scenario['mode']=='ferry')]\n",
    "    vehicles = Pts_scenario.groupby(['vehicle','capacity']).apply(lambda x: int(np.mean(list(x.capacity))*10))\n",
    "    \n",
    "    vehicles.to_csv('outputs/vehicles_{}.csv'.format(name))\n",
    "    # nuovo.to_csv('outputs/MergedPTs_NYC_{}_3.csv.gz'.format(scenario))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d412f-93bd-4222-bd92-3d7464c8217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUNG PEOPLE baseline\n",
    "\n",
    "PTs = pd.read_csv('outputs/MergedPTs_NYC_Baseline2019_3.csv.gz', index_col=0)\n",
    "PtoPts = pd.read_csv('outputs/MergedPtoPTs_NYC_Baseline2019_3.csv.gz', index_col=0)\n",
    "persons = pd.read_csv('outputs/Merged_Pop_Baseline.csv.gz', index_col=0)\n",
    "households = pd.read_csv('outputs/Merged_Hs_Baseline.csv.gz', index_col=0)\n",
    "# persons_ip = pd.read_csv('outputs/persons.csv.gz', index_col=0)\n",
    "# households_ip = pd.read_csv('outputs/households.csv.gz', index_col=0)\n",
    "# persons_ip_allages = pd.read_csv('persons.csv.gz', index_col=0)\n",
    "young_file = pd.read_csv('inputs/nyc-kids-base-fixed.csv', index_col=0)\n",
    "vehicles = pd.read_csv('outputs/vehicles_Baseline2019.csv', index_col=0)\n",
    "vehicles = dict(zip(vehicles.index, vehicles.capacity*10))\n",
    "\n",
    "PTs['mode'] = PTs['mode'].replace('bus_empty', 'bus')\n",
    "PTs['mode'] = PTs['mode'].replace('subway_empty', 'subway')\n",
    "PTs['mode'] = PTs['mode'].replace('rail_empty', 'rail')\n",
    "PTs['mode'] = PTs['mode'].replace('tram_empty', 'tram')\n",
    "PTs['mode'] = PTs['mode'].replace('ferry_empty', 'ferry')\n",
    "\n",
    "i =0\n",
    "PtoPts_to_add = pd.DataFrame()\n",
    "for adult_to_duplicate, young_personid, hsID, age, is_fem, ind in zip(young_file['adultPersonIdToCopy'],young_file.index, young_file['household_id'],young_file['age'],young_file['sex'],young_file['industry'],):\n",
    "    if i%100 ==0:\n",
    "        young_file['adultPersonIdToCopy'].to_csv('outputs/{}.csv'.format(i))\n",
    "    PtoPts_to_duplicate = PtoPts[PtoPts['personID']==adult_to_duplicate]\n",
    "    PtoPts_to_duplicate.loc[:,'personID'] = young_personid\n",
    "    PtoPts_to_add = pd.concat([PtoPts_to_add, PtoPts_to_duplicate])\n",
    "    persons.loc[99999999999+i,['personId','age','isFemale','householdId','householdRank','excludedModes','valueOfTime']] = [int(young_personid), int(age), True if is_fem==2 else False, hsID, 0, np.nan, 7.25]\n",
    "    if str(hsID) not in list(households['householdId']):\n",
    "        print('Warning, {} new household'.format(hsID))\n",
    "        young_file['adultPersonIdToCopy'].to_csv('outputs/{}.csv'.format(hsID))\n",
    "    i+=1\n",
    "    for PT_id, person_id in zip(PtoPts_to_duplicate['pathTraversalID'],PtoPts_to_duplicate['personID']):\n",
    "        PTs.loc[PT_id,'occupancy'] +=1\n",
    "        PTs.loc[PT_id,'riders'] += ':{}'.format(person_id)\n",
    "        PTs.loc[PT_id,'passengerMiles'] += PTs.loc[PT_id,'vehicleMiles']\n",
    "\n",
    "PtoPts = pd.concat([PtoPts, PtoPts_to_add])\n",
    "persons.index = range(len(persons))\n",
    "PtoPts.index = range(len(PtoPts))\n",
    "\n",
    "\n",
    "persons['personId'] = persons.personId.astype(int)\n",
    "persons['householdRank'] = persons.householdRank.astype(int)\n",
    "\n",
    "capacities = []\n",
    "for vehicle in PTs['vehicle']:\n",
    "    capacities.append(vehicles[vehicle])\n",
    "PTs['capacity'] = capacities\n",
    "\n",
    "PtoPts.to_csv('outputs/MergedPtoPTs_NYC_Baseline2019_4.csv.gz')\n",
    "persons.to_csv('outputs/Merged_Pop_Baseline_4.csv.gz')\n",
    "PTs.to_csv('outputs/MergedPTs_NYC_Baseline2019_4.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbea7e9-e965-4a99-8d55-03a09df79fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUNG PEOPLE aug2020\n",
    "\n",
    "PTs = pd.read_csv('outputs/MergedPTs_NYC_Aug2020_3.csv.gz', index_col=0)\n",
    "PtoPts = pd.read_csv('outputs/MergedPtoPTs_NYC_Aug2020_3.csv.gz', index_col=0)\n",
    "persons = pd.read_csv('outputs/Merged_Pop_Aug2020.csv.gz', index_col=0)\n",
    "households = pd.read_csv('outputs/Merged_Hs_Aug2020.csv.gz', index_col=0)\n",
    "# persons_ip = pd.read_csv('outputs/persons.csv.gz', index_col=0)\n",
    "# households_ip = pd.read_csv('outputs/households.csv.gz', index_col=0)\n",
    "# persons_ip_allages = pd.read_csv('persons.csv.gz', index_col=0)\n",
    "young_file = pd.read_csv('inputs/nyc-kids-august2020.csv', index_col=0)\n",
    "vehicles = pd.read_csv('outputs/vehicles_Aug2020.csv', index_col=0)\n",
    "vehicles = dict(zip(vehicles.index, vehicles.capacity*10))\n",
    "\n",
    "PTs['mode'] = PTs['mode'].replace('bus_empty', 'bus')\n",
    "PTs['mode'] = PTs['mode'].replace('subway_empty', 'subway')\n",
    "PTs['mode'] = PTs['mode'].replace('rail_empty', 'rail')\n",
    "PTs['mode'] = PTs['mode'].replace('tram_empty', 'tram')\n",
    "PTs['mode'] = PTs['mode'].replace('ferry_empty', 'ferry')\n",
    "\n",
    "i =0\n",
    "PtoPts_to_add = pd.DataFrame()\n",
    "for adult_to_duplicate, young_personid, hsID, age, is_fem, ind in zip(young_file['adultPersonIdToCopy'],young_file.index, young_file['household_id'],young_file['age'],young_file['sex'],young_file['industry'],):\n",
    "    if i%100 ==0:\n",
    "        young_file['adultPersonIdToCopy'].to_csv('outputs/{}.csv'.format(i))\n",
    "    PtoPts_to_duplicate = PtoPts[PtoPts['personID']==adult_to_duplicate]\n",
    "    PtoPts_to_duplicate.loc[:,'personID'] = young_personid\n",
    "    PtoPts_to_add = pd.concat([PtoPts_to_add, PtoPts_to_duplicate])\n",
    "    persons.loc[99999999999+i,['personId','age','isFemale','householdId','householdRank','excludedModes','valueOfTime']] = [int(young_personid), int(age), True if is_fem==2 else False, hsID, 0, np.nan, 7.25]\n",
    "    if str(hsID) not in list(households['householdId']):\n",
    "        print('Warning, {} new household'.format(hsID))\n",
    "        young_file['adultPersonIdToCopy'].to_csv('outputs/{}.csv'.format(hsID))\n",
    "    i+=1\n",
    "    for PT_id, person_id in zip(PtoPts_to_duplicate['pathTraversalID'],PtoPts_to_duplicate['personID']):\n",
    "        PTs.loc[PT_id,'occupancy'] +=1\n",
    "        PTs.loc[PT_id,'riders'] += ':{}'.format(person_id)\n",
    "        PTs.loc[PT_id,'passengerMiles'] += PTs.loc[PT_id,'vehicleMiles']\n",
    "\n",
    "PtoPts = pd.concat([PtoPts, PtoPts_to_add])\n",
    "persons.index = range(len(persons))\n",
    "PtoPts.index = range(len(PtoPts))\n",
    "\n",
    "\n",
    "persons['personId'] = persons.personId.astype(int)\n",
    "persons['householdRank'] = persons.householdRank.astype(int)\n",
    "\n",
    "capacities = []\n",
    "for vehicle in PTs['vehicle']:\n",
    "    capacities.append(vehicles[vehicle])\n",
    "PTs['capacity'] = capacities\n",
    "\n",
    "PtoPts.to_csv('outputs/MergedPtoPTs_NYC_Aug2020_4.csv.gz')\n",
    "persons.to_csv('outputs/Merged_Pop_Aug2020_4.csv.gz')\n",
    "PTs.to_csv('outputs/MergedPTs_NYC_Aug2020_4.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3ada09c-0dff-42cf-ab5f-493e7ff61904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE PERSON TABLE\n",
    "Pts = pd.read_csv('outputs/MergedPTs_NYC_Aug2020_4.csv.gz', index_col=0)\n",
    "PtoPts = pd.read_csv('outputs/MergedPtoPTs_NYC_Aug2020_4.csv.gz', index_col=0)\n",
    "Persons = pd.read_csv('outputs/Merged_Pop_Aug2020_4.csv.gz', index_col=0)\n",
    "Households = pd.read_csv('outputs/Merged_Hs_Aug2020.csv.gz', index_col=0)\n",
    "pop = pd.read_csv('outputs/persons_baseline.csv.gz', index_col=0)\n",
    "hs = pd.read_csv('outputs/households_baseline.csv.gz', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25164d8d-84a9-4310-b2e8-48b05327ea2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('s')\n",
    "# plans_baseline1 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-0-of-10__2023-01-03_19-59-12_nwn/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "# print('s')\n",
    "\n",
    "# plans_baseline2 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-1-of-10__2023-01-03_19-59-13_jgp/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "\n",
    "# print('s')\n",
    "# plans_baseline3 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-2-of-10__2023-01-03_19-59-06_brm/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "# print('s')\n",
    "# plans_baseline4 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-3-of-10__2023-01-03_19-59-07_kqr/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "\n",
    "# print('s')\n",
    "# plans_baseline5 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-4-of-10__2023-01-03_19-59-09_fbb/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "\n",
    "# print('s')\n",
    "# plans_baseline6 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-5-of-10__2023-01-03_19-59-11_tjh/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "\n",
    "# print('s')\n",
    "# plans_baseline7 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-6-of-10__2023-01-03_19-59-12_zwm/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "\n",
    "# print('s')\n",
    "# plans_baseline8 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-7-of-10__2023-01-03_19-59-19_bcr/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "\n",
    "# print('s')\n",
    "# plans_baseline9 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-8-of-10__2023-01-03_19-59-14_olx/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "\n",
    "# print('s')\n",
    "# plans_baseline0 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-9-of-10__2023-01-03_19-59-07_xcc/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "# print('s')\n",
    "\n",
    "\n",
    "plans_baseline0 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-august2020-0-of-10__2023-01-10_16-52-48_lgr/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "plans_baseline1 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-august2020-1-of-10__2023-01-10_16-52-46_gib/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "plans_baseline2 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-august2020-2-of-10__2023-01-14_21-19-35_flk/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "plans_baseline3 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-august2020-3-of-10__2023-01-15_01-54-44_txq/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "plans_baseline4 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-august2020-4-of-10__2023-01-10_16-52-43_rio/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "plans_baseline5 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-august2020-5-of-10__2023-01-10_16-53-00_vjr/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "plans_baseline6 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-august2020-6-of-10__2023-01-10_16-52-53_bhh/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "plans_baseline7 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-august2020-7-of-10__2023-01-10_16-53-01_drt/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "plans_baseline8 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-august2020-8-of-10__2023-01-10_16-53-15_qgy/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n",
    "plans_baseline9 =     pd.read_csv('s3://beam-outputs/output/newyork/new-york-august2020-9-of-10__2023-01-10_16-53-14_pdt/ITERS/it.5/5.plans.csv.gz', usecols = ['personId','planSelected','activityType', 'activityLocationX', 'activityLocationY', 'activityStartTime', 'activityEndTime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797a021-18e8-413f-b13a-8a40d7e44c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plans_baseline = pd.concat([plans_baseline0,plans_baseline1,plans_baseline2,plans_baseline3,plans_baseline4,\n",
    "                            plans_baseline5,plans_baseline6,plans_baseline7,plans_baseline8,plans_baseline9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b139692-2383-4850-be5a-f3392045fcbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "durations = []\n",
    "modes = []\n",
    "print(len(PtoPts))\n",
    "for ptid in PtoPts['pathTraversalID']:\n",
    "    durations.append(Pts.loc[ptid, 'duration'])\n",
    "    modes.append(Pts.loc[ptid, 'mode'])\n",
    "    i+=1\n",
    "    if i%100000==0:\n",
    "        print(i)\n",
    "PtoPts['duration']=durations\n",
    "PtoPts['mode']=modes\n",
    "\n",
    "agents = list(np.unique(list(PtoPts['personID'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045da1c9-279e-4a21-9ad2-b1edd7ce5fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PtoPts_gropued = PtoPts.groupby(['personID','mode']).apply(lambda x: [sum(x.duration),len(x), len(np.unique(list(x.vehicleID))),list(np.unique(list(x.planIndex)))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01602f2f-04a9-4a7f-b68c-0c4d30a20d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "PtoPts['mode'] = PtoPts['mode'].replace('bus_empty', 'bus')\n",
    "PtoPts['mode'] = PtoPts['mode'].replace('subway_empty', 'subway')\n",
    "PtoPts['mode'] = PtoPts['mode'].replace('rail_empty', 'rail')\n",
    "PtoPts['mode'] = PtoPts['mode'].replace('tram_empty', 'tram')\n",
    "PtoPts['mode'] = PtoPts['mode'].replace('ferry_empty', 'ferry')\n",
    "\n",
    "Persons['personId'] = Persons.personId.astype(int)\n",
    "Persons['householdRank'] = Persons.householdRank.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e19704-df26-4a50-a8bc-f5e921ea7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_database = pd.DataFrame()\n",
    "\n",
    "person_database['PersonID'] = agents\n",
    "\n",
    "\n",
    "\n",
    "person_to_ages = dict(zip(pop.index, pop.age))\n",
    "person_to_gender = dict(zip(pop.index, pop.sex))\n",
    "person_to_industry = dict(zip(pop.index, pop.industry))\n",
    "\n",
    "print('s')\n",
    "\n",
    "plans_baseline_grouped = plans_baseline.groupby(['personId','planSelected','activityType']).apply(lambda x: [list(x.activityLocationX)[0],list( x.activityLocationY)[0], list(x.activityStartTime)[0], list(x.activityEndTime)[0]])\n",
    "\n",
    "print('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f85e17-7120-49bb-a4e5-012dd9a9a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = []\n",
    "is_female = []\n",
    "industry = []\n",
    "residency_x = []\n",
    "residency_y = []\n",
    "work_x = []\n",
    "work_y = []\n",
    "work_start = []\n",
    "work_end = []\n",
    "i=0\n",
    "print(len(agents))\n",
    "wrong_home=0\n",
    "wrong_work=0\n",
    "for agent in person_database['PersonID']:\n",
    "    if i%100000==0:\n",
    "        print(i)\n",
    "    i+=1\n",
    "    ages.append(person_to_ages[agent])\n",
    "    is_female.append(person_to_gender[agent])\n",
    "    industry.append(person_to_industry[agent])\n",
    "    try:\n",
    "        residency_x.append(plans_baseline_grouped[(agent,True,'Home')][0])\n",
    "        residency_y.append(plans_baseline_grouped[(agent,True,'Home')][1])\n",
    "    except:\n",
    "        residency_x.append(0)\n",
    "        residency_y.append(0)\n",
    "        wrong_home+=1\n",
    "    try:\n",
    "        work_x.append(plans_baseline_grouped[(agent,True,'Work')][0])\n",
    "        work_y.append(plans_baseline_grouped[(agent,True,'Work')][1])\n",
    "        work_start.append(plans_baseline_grouped[(agent,True,'Work')][2])\n",
    "        work_end.append(plans_baseline_grouped[(agent,True,'Work')][3])\n",
    "    except:\n",
    "        work_x.append(0)\n",
    "        work_y.append(0)\n",
    "        work_start.append(0)\n",
    "        work_end.append(0)\n",
    "        wrong_work+=1\n",
    "print('wrong_home',wrong_home)\n",
    "print('wrong_work',wrong_work)\n",
    "        \n",
    "print('s')\n",
    "\n",
    "person_database['Age'] = ages\n",
    "person_database['Gender'] = is_female\n",
    "person_database['Industry'] = industry\n",
    "person_database['X_Home'] = residency_x\n",
    "person_database['Y_Home'] = residency_y\n",
    "person_database['X_Work'] = work_x\n",
    "person_database['Y_Work'] = work_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22146394-6025-4efe-90e0-2a1c28abb0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "time_spent_on_bus = []\n",
    "time_spent_on_sub = []\n",
    "time_spent_on_rail = []\n",
    "time_spent_on_tram =[]\n",
    "time_spent_on_ferry = []\n",
    "\n",
    "total_time_on_transit = []\n",
    "\n",
    "legs_on_bus = []\n",
    "legs_on_sub = []\n",
    "legs_on_rail = []\n",
    "legs_on_tram = []\n",
    "legs_on_ferry = []\n",
    "\n",
    "total_legs_on_transit = []\n",
    "\n",
    "bus_used_in_a_day =[]\n",
    "sub_used_in_a_day =[]\n",
    "rail_used_in_a_day =[]\n",
    "tram_used_in_a_day = []\n",
    "ferry_used_in_a_day = []\n",
    "\n",
    "total_transit_vehicles_used = []\n",
    "\n",
    "n_transfer = []\n",
    "\n",
    "\n",
    "    \n",
    "i=0\n",
    "for agent in agents:\n",
    "    \n",
    "    i+=1\n",
    "    if i%100000==0:\n",
    "        print(i)\n",
    "        \n",
    "    try:\n",
    "        time_spent_on_bus.append(PtoPts_gropued[agent,'bus'][0])\n",
    "    except:\n",
    "        time_spent_on_bus.append(0)\n",
    "    try:\n",
    "        time_spent_on_sub.append(PtoPts_gropued[agent,'subway'][0])\n",
    "    except:\n",
    "        time_spent_on_sub.append(0)\n",
    "    try:\n",
    "        time_spent_on_rail.append(PtoPts_gropued[agent,'rail'][0])\n",
    "    except:\n",
    "        time_spent_on_rail.append(0)\n",
    "    try:\n",
    "        time_spent_on_tram.append(PtoPts_gropued[agent,'tram'][0])\n",
    "    except:\n",
    "        time_spent_on_tram.append(0)\n",
    "    try:\n",
    "        time_spent_on_ferry.append(PtoPts_gropued[agent,'ferry'][0])\n",
    "    except:\n",
    "        time_spent_on_ferry.append(0)\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        legs_on_bus.append(PtoPts_gropued[agent,'bus'][1])\n",
    "    except:\n",
    "        legs_on_bus.append(0)\n",
    "    try:\n",
    "        legs_on_sub.append(PtoPts_gropued[agent,'subway'][1])\n",
    "    except:\n",
    "        legs_on_sub.append(0)\n",
    "    try:\n",
    "        legs_on_rail.append(PtoPts_gropued[agent,'rail'][1])\n",
    "    except:\n",
    "        legs_on_rail.append(0)\n",
    "    try:\n",
    "        legs_on_tram.append(PtoPts_gropued[agent,'tram'][1])\n",
    "    except:\n",
    "        legs_on_tram.append(0)\n",
    "    try:\n",
    "        legs_on_ferry.append(PtoPts_gropued[agent,'ferry'][1])\n",
    "    except:\n",
    "        legs_on_ferry.append(0)\n",
    "        \n",
    "    try:\n",
    "        bus_used_in_a_day.append(PtoPts_gropued[agent,'bus'][2])\n",
    "    except:\n",
    "        bus_used_in_a_day.append(0)\n",
    "    try:\n",
    "        sub_used_in_a_day.append(PtoPts_gropued[agent,'subway'][2])\n",
    "    except:\n",
    "        sub_used_in_a_day.append(0)\n",
    "    try:\n",
    "        rail_used_in_a_day.append(PtoPts_gropued[agent,'rail'][2])\n",
    "    except:\n",
    "        rail_used_in_a_day.append(0)\n",
    "    try:\n",
    "        tram_used_in_a_day.append(PtoPts_gropued[agent,'tram'][2])\n",
    "    except:\n",
    "        tram_used_in_a_day.append(0)\n",
    "    try:\n",
    "        ferry_used_in_a_day.append(PtoPts_gropued[agent,'ferry'][2])\n",
    "    except:\n",
    "        ferry_used_in_a_day.append(0)\n",
    "        \n",
    "        \n",
    "    total_time_on_transit.append(time_spent_on_bus[-1]+time_spent_on_sub[-1]+time_spent_on_rail[-1]+time_spent_on_tram[-1]+ time_spent_on_ferry[-1])\n",
    "    \n",
    "    total_legs_on_transit.append(legs_on_bus[-1]+legs_on_sub[-1]+legs_on_rail[-1]+legs_on_tram[-1]+ legs_on_ferry[-1] )\n",
    "    \n",
    "    total_transit_vehicles_used.append(bus_used_in_a_day[-1]+sub_used_in_a_day[-1]+rail_used_in_a_day[-1]+tram_used_in_a_day[-1]+ ferry_used_in_a_day[-1]  )\n",
    "    \n",
    "    n_transfer.append(total_transit_vehicles_used[-1] - len(np.unique([item for sublist in [PtoPts_gropued[agent,mode_][3]  for mode_ in PtoPts_gropued[agent].keys() ] for item in sublist])))\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "person_database['Time spent on bus'] = time_spent_on_bus\n",
    "person_database['Time spent on subway'] = time_spent_on_sub\n",
    "person_database['Time spent on rail'] = time_spent_on_rail\n",
    "person_database['Time spent on tram'] = time_spent_on_tram\n",
    "person_database['Time spent on ferry'] = time_spent_on_ferry\n",
    "        \n",
    "        \n",
    "person_database['Number of bus stops traveled'] = legs_on_bus\n",
    "person_database['Number of subway stops traveled'] = legs_on_sub\n",
    "person_database['Number of rail stops traveled'] = legs_on_rail\n",
    "person_database['Number of tram stops traveled'] = legs_on_tram\n",
    "person_database['Number of ferry stops traveled'] = legs_on_ferry\n",
    "        \n",
    "person_database['Bus vehicles used'] = bus_used_in_a_day\n",
    "person_database['Subway vehicles used'] = sub_used_in_a_day\n",
    "person_database['Rail vehicles used'] = rail_used_in_a_day\n",
    "person_database['Tram vehicles used'] = tram_used_in_a_day\n",
    "person_database['Ferry vehicles used'] = ferry_used_in_a_day\n",
    "        \n",
    "        \n",
    "person_database['Time spent on transit'] = total_time_on_transit\n",
    "    \n",
    "person_database['Number of transit stops traveled'] = total_legs_on_transit\n",
    "    \n",
    "person_database['Transit vehicles used'] = total_transit_vehicles_used\n",
    "    \n",
    "person_database['Number of transfers'] = n_transfer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f59f16-881d-4e20-93e6-6c0ffdc23f36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "person_database.to_csv('outputs/agents_experience_Aug2020.cvs')\n",
    "# person_database['Number of transfers'] = n_transfer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b290f-3718-4bf7-a220-9f1d5a39d78a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97a8fd-6ee8-4cc1-821e-710abfebd2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a28cea-d6f3-4519-8f38-4329702b4be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcba084-5043-45c7-a099-413c91a002ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "person_database[person_database['PersonID']==2188823]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c33ce1-be9f-491a-864c-b6c1d87a98d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT AGENT IN A MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c624635-c5a1-457c-86c2-baa358065589",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pts = pd.read_csv('outputs/MergedPTs_NYC_Baseline2019_4.csv.gz', index_col=0)\n",
    "PtoPts = pd.read_csv('outputs/MergedPtoPTs_NYC_Baseline2019_4.csv.gz', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05ba2a-68e1-4d9a-b4d4-a9f16790b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Persons = pd.read_csv('outputs/Merged_Pop_Baseline_4.csv.gz', index_col=0)\n",
    "Households = pd.read_csv('outputs/Merged_Hs_Baseline.csv.gz', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d45b224-5b11-4c12-8a7a-3af142c0d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plans = pd.concat(\n",
    "    [pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-0-of-10__2023-01-03_19-59-12_nwn/ITERS/it.5/5.plans.csv.gz'),\n",
    "    pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-1-of-10__2023-01-03_19-59-13_jgp/ITERS/it.5/5.plans.csv.gz'),\n",
    "    pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-2-of-10__2023-01-03_19-59-06_brm/ITERS/it.5/5.plans.csv.gz'),\n",
    "    pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-3-of-10__2023-01-03_19-59-07_kqr/ITERS/it.5/5.plans.csv.gz'),\n",
    "    pd.read_persons.csv.gz's3://beam-outputs/output/newyork/new-york-baseline-4-of-10__2023-01-03_19-59-09_fbb/ITERS/it.5/5.plans.csv.gz'),])\n",
    "    # pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-5-of-10__2023-01-03_19-59-11_tjh/ITERS/it.5/5.plans.csv.gz'),\n",
    "    # pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-6-of-10__2023-01-03_19-59-12_zwm/ITERS/it.5/5.plans.csv.gz'),\n",
    "    # pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-7-of-10__2023-01-03_19-59-19_bcr/ITERS/it.5/5.plans.csv.gz'),\n",
    "    # pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-8-of-10__2023-01-03_19-59-14_olx/ITERS/it.5/5.plans.csv.gz'),\n",
    "    # pd.read_csv('s3://beam-outputs/output/newyork/new-york-baseline-9-of-10__2023-01-03_19-59-07_xcc/ITERS/it.5/5.plans.csv.gz')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe6c557-e092-49e2-bbec-124adcbecdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "PtoPts_user_NYC_Subway =  PtoPts[PtoPts['personID']==  PtoPts['personID'][PtoPts['agency'] == 'NYC_Subway'].value_counts().keys()[183]]\n",
    "# user_NJ_Transit_Bus = PtoPts['personID'][PtoPts['agency'] == 'NJ_Transit_Bus'].value_counts().keys()[200]\n",
    "# user_MTA_Brookl = PtoPts['personID'][PtoPts['agency'] == 'MTA_Brookl'].value_counts().keys()[200]\n",
    "# user_NYC_Bus_Co = PtoPts['personID'][PtoPts['agency'] == 'NYC_Bus_Co'].value_counts().keys()[200]\n",
    "# user_NICE_117_2 = PtoPts['personID'][PtoPts['agency'] == 'NICE_117_2'].value_counts().keys()[200]\n",
    "# user_MTA_Manhat = PtoPts['personID'][PtoPts['agency'] == 'MTA_Manhat'].value_counts().keys()[200]\n",
    "# user_MTA_Queens = PtoPts['personID'][PtoPts['agency'] == 'MTA_Queens'].value_counts().keys()[200]\n",
    "# user_MTA_Bronx_ = PtoPts['personID'][PtoPts['agency'] == 'MTA_Bronx_'].value_counts().keys()[200]\n",
    "# user_MTA_Staten = PtoPts['personID'][PtoPts['agency'] == 'MTA_Staten'].value_counts().keys()[200]\n",
    "# user_NJ_Transit_Rail = PtoPts['personID'][PtoPts['agency'] == 'NJ_Transit_Rail'].value_counts().keys()[200]\n",
    "# user_Long_Islan = PtoPts['personID'][PtoPts['agency'] == 'Long_Islan'].value_counts().keys()[200]\n",
    "# user_PATH = PtoPts['personID'][PtoPts['agency'] == '151_631:t_'].value_counts().keys()[200]\n",
    "# user_WCDOT_2020 = PtoPts['personID'][PtoPts['agency'] == 'WCDOT_2020'].value_counts().keys()[200]\n",
    "# user_Metro_Nort = PtoPts['personID'][PtoPts['agency'] == 'etro-Nort'].value_counts().keys()[200]\n",
    "# user_NYC_NYC_Ferry_ = PtoPts['personID'][PtoPts['agency'] == 'NYC_Ferry_'].value_counts().keys()[200]\n",
    "# user_NYC_Staten_Isl = PtoPts['personID'][PtoPts['agency'] == 'Staten_Isl'].value_counts().keys()[200]\n",
    "# user_NYC_JFK_Airtra = PtoPts['personID'][PtoPts['agency'] == 'JFK_Airtra'].value_counts().keys()[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a2c6f-5a37-4f36-8375-3ccfefb76d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f307fea-5c4e-46af-af89-a94ed13e930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PtoPts_user_NYC_Subway.to_csv('outputs/example_trip.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa529cd6-4a74-4760-b97a-71b99b67ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plans_person = Plans[(Plans['personId']==4611317)&(Plans['planSelected'] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87484cb9-ebf6-403a-95c3-b4c1e254b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plans_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2385685f-9e9a-4ad4-be21-2d929b590fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9909833-90df-4307-8c7d-cf988183dc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4110f8-2773-4a66-8cc2-9e6aa4d60ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_agencies = ['NJ_Transit_Rail', \n",
    "                    'NJ_Transit_Bus', \n",
    "                    '151_631:t_',\n",
    "                    'Long_Islan',\n",
    "                    'Metro-Nort',\n",
    "                    'MTA_Bronx_', \n",
    "                    'MTA_Brookl', \n",
    "                    'MTA_Manhat', \n",
    "                    'MTA_Queens', \n",
    "                    'MTA_Staten',\n",
    "                    'NYC_Bus_Co', \n",
    "                    'NYC_Ferry_', \n",
    "                    'NYC_Subway', \n",
    "                    'Staten_Island_Ferry',\n",
    "                    #'Suffolk_Co', \n",
    "                    'WCDOT_2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169784e-fe52-4ae0-b9c4-57162b0faace",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_agents = pd.DataFrame()\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "\n",
    "agencies =[]\n",
    "times = []\n",
    "vehicles = []\n",
    "\n",
    "for ptid, ag in zip(PtoPts_user_NYC_Subway['pathTraversalID'], PtoPts_user_NYC_Subway['agency']):\n",
    "    xs.append(Pts.loc[ptid, 'startX'])\n",
    "    ys.append(Pts.loc[ptid, 'startY'])\n",
    "    xs.append(Pts.loc[ptid, 'endX'])\n",
    "    ys.append(Pts.loc[ptid, 'endY'])\n",
    "    agencies.append(ag)\n",
    "    agencies.append(ag)\n",
    "    times.append(Pts.loc[ptid, 'departureTime'])\n",
    "    times.append(Pts.loc[ptid, 'departureTime'])\n",
    "    vehicles.append(Pts.loc[ptid, 'vehicle'])\n",
    "    vehicles.append(Pts.loc[ptid, 'vehicle'])\n",
    "map_agents['X'] = xs\n",
    "map_agents['Y'] = ys\n",
    "map_agents['Agency'] = agencies\n",
    "map_agents['Time'] = times\n",
    "map_agents['Vehicles'] = vehicles\n",
    "\n",
    "map_agents.to_csv('outputs/NYC_agents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45214b71-dedc-4c1d-8878-dd78435556fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plans_person.loc[[3557470,3557472],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccede84d-4bae-4b06-9d31-d20c9294740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_agents = pd.DataFrame()\n",
    "Plans_person = Plans_person.loc[[3557470,3557472],:]\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "atypes =[]\n",
    "for x, y, atype in zip(Plans_person['activityLocationX'], Plans_person['activityLocationY'], Plans_person['activityType']):\n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "    atypes.append(atype)\n",
    "plan_agents['X'] = xs\n",
    "plan_agents['Y'] = ys\n",
    "plan_agents['Atype'] = atypes\n",
    "\n",
    "plan_agents.to_csv('outputs/NYC_activities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d6d30-b609-414c-8f27-65e12ef27100",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd8252-6228-4836-b654-e8cd2f6610ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20082762-1245-440a-8989-939e57b98930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cba073d-84ea-4a85-bc6a-649f90b2f94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a583665-ef96-49f9-a4d1-843e3ba5b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.read_csv('s3://beam-outputs/pilates-outputs/sfbay_5fleets_100price_100fleet_20230209/beam/year-2020-iteration-3/ITERS/it.0/0.events.csv.gz', nrows = None)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069b798-bc5d-411e-a814-e2d49d9443fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "events['vehicle'].to_csv('outputs/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e698ac66-01a0-4520-bd38-f698f844d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# events=events.fillna(0)\n",
    "events = events[events['type']=='PathTraversal']\n",
    "eventsRH = events[events['vehicle'].str.contains('ride')]\n",
    "agencies = []\n",
    "for vehicle in eventsRH['vehicle']:\n",
    "    agencies.append(vehicle.split('@')[1])\n",
    "eventsRH['agency'] = agencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4cdd66-0d0d-43e0-9e52-748a60d042b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventsRH['agency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393a98f4-b1fc-4fd4-bc2a-3a9c82a238b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventsRH['is_pool']=0\n",
    "eventsRH['is_pool'][eventsRH['numPassengers']>0]+=1\n",
    "eventsRH['is_pool'][eventsRH['numPassengers']>1]+=1\n",
    "eventsRH['is_pool'] = eventsRH['is_pool'].replace(0, 'empty')\n",
    "eventsRH['is_pool'] = eventsRH['is_pool'].replace(1, 'solo')\n",
    "eventsRH['is_pool'] = eventsRH['is_pool'].replace(2, 'pooled')\n",
    "\n",
    "eventsRH.groupby(['agency','is_pool']).apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395d3237-5336-4a11-8da1-4d350ba43dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventsRH['is_pool'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb5938d-8b4c-483a-86ce-cc5c5b136d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventsRH['numPassengers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2154f86-4e3a-4a02-9200-13c078758725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd64df4-5030-4e7d-b4f6-8708be41504c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b64feb-1052-4089-a6e6-3dd8fb0d6438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87510447-0988-4fa2-a1d5-029fd4df2ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdffd1c2-ad72-4b6e-8067-a951a6780791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c02507-02c5-4ee0-812b-2c6e56b02ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b5cee9-5dde-4131-b7bf-2c04010da248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3de97b-b6ec-4ba7-b0f3-1effaa42b3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eea8d15-3434-4cc5-b7be-a6c312fd51a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d67b8e-bdc1-4126-b033-cf3b3d63487d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5145c3-3aac-407e-b439-be1523504db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
